# Vietnamese Legal LLM - Finetune & Serving System

Há»‡ thá»‘ng hoÃ n chá»‰nh Ä‘á»ƒ finetune vÃ  serving model Llama-3.1-8B cho tÆ° váº¥n phÃ¡p luáº­t Viá»‡t Nam trÃªn GPU droplet Digital Ocean.

## ğŸ¯ Má»¥c tiÃªu

- **Model**: Finetune Llama-3.1-8B-Instruct vá»›i dá»¯ liá»‡u phÃ¡p luáº­t Viá»‡t Nam (~100k examples)
- **Ká»¹ thuáº­t**: LoRA (Low-Rank Adaptation) vá»›i Unsloth Ä‘á»ƒ tá»‘i Æ°u hÃ³a
- **Deployment**: Serving trÃªn Digital Ocean GPU droplet H200
- **API**: Compatible vá»›i OpenAI API format

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
llm_finetuning_serving/
â”œâ”€â”€ data_processing/           # Xá»­ lÃ½ dá»¯ liá»‡u
â”‚   â”œâ”€â”€ analyze_data.py       # PhÃ¢n tÃ­ch cáº¥u trÃºc dá»¯ liá»‡u
â”‚   â”œâ”€â”€ process_llama_data.py # Chuyá»ƒn Ä‘á»•i sang format Llama
â”‚   â””â”€â”€ split_data.py         # Chia train/val/test vá»›i batching
â”œâ”€â”€ finetune/                 # Training vá»›i Unsloth LoRA
â”‚   â””â”€â”€ train_llama.py        # Script training chÃ­nh
â”œâ”€â”€ evaluation/               # ÄÃ¡nh giÃ¡ model
â”‚   â””â”€â”€ evaluate_model.py     # ROUGE, BLEU, Perplexity, LLM-eval
â”œâ”€â”€ serving/                  # FastAPI serving system
â”‚   â””â”€â”€ serve_model.py        # API server vá»›i streaming
â”œâ”€â”€ docker/                   # Docker deployment
â”‚   â”œâ”€â”€ Dockerfile           # CUDA + Python environment
â”‚   â””â”€â”€ docker-compose.yml   # Production deployment
â”œâ”€â”€ requirements.txt          # Python dependencies
â”œâ”€â”€ .env.template            # Environment variables template
â””â”€â”€ run_pipeline.sh          # Automation script
```

## ğŸš€ Quick Start

### Workflow tá»•ng quan:
1. **Local**: Xá»­ lÃ½ dá»¯ liá»‡u â†’ Upload lÃªn Digital Ocean Spaces
2. **Training GPU Droplet**: Download dá»¯ liá»‡u â†’ Train â†’ Upload model
3. **Serving GPU Droplet**: Download model â†’ Serve API

### 1. Chuáº©n bá»‹ dá»¯ liá»‡u (Local)

```bash
# Clone vÃ  di chuyá»ƒn vÃ o thÆ° má»¥c
cd llm_finetuning_serving

# Setup environment vÃ  dependencies
./run_pipeline.sh setup

# Copy vÃ  chá»‰nh sá»­a environment variables
cp .env.template .env
# Chá»‰nh sá»­a .env vá»›i Digital Ocean Spaces credentials

# Chuáº©n bá»‹ vÃ  upload dá»¯ liá»‡u
./prepare_data.sh
```

### 2. Training trÃªn GPU Droplet

```bash
# TrÃªn Digital Ocean GPU droplet (H200)
git clone <repo>
cd llm_finetuning_serving

# Setup environment
./run_pipeline.sh setup
cp .env.template .env  # Edit vá»›i credentials

# Download dá»¯ liá»‡u vÃ  train
./run_pipeline.sh train
# Model sáº½ tá»± Ä‘á»™ng Ä‘Æ°á»£c upload lÃªn Spaces sau khi train xong
```

### 3. Serving trÃªn GPU Droplet khÃ¡c

```bash
# TrÃªn Digital Ocean GPU droplet khÃ¡c
git clone <repo>
cd llm_finetuning_serving

# Setup environment
./run_pipeline.sh setup
cp .env.template .env  # Edit vá»›i credentials vÃ  MODEL_NAME

# Download model vÃ  serve
./run_pipeline.sh serve
# API sáº½ cháº¡y táº¡i http://your-droplet-ip:8000
```

## ğŸ“Š Data & Model Management

**Dá»¯ liá»‡u Ä‘Æ°á»£c lÆ°u trÃªn Digital Ocean Spaces:**
- Bucket: `legal-datalake`
- Raw data: `process_data/finetune_data/`
- Processed data: `process_data/processed/`
- Models: `models/`

**Workflow:**
1. **Local**: Xá»­ lÃ½ dá»¯ liá»‡u tá»« JSONL â†’ Llama format â†’ Upload lÃªn Spaces
2. **Training Droplet**: Auto download dá»¯ liá»‡u â†’ Train â†’ Upload model lÃªn Spaces  
3. **Serving Droplet**: Auto download model â†’ Serve API

## ğŸ“Š Xá»­ lÃ½ dá»¯ liá»‡u

### Format Ä‘áº§u vÃ o (JSONL)
```json
{
  "instruction": "Tráº£ lá»i cÃ¢u há»i phÃ¡p luáº­t sau:",
  "input": "Trong Bá»™ luáº­t HÃ¬nh sá»± thÃ¬ bao nhiÃªu tuá»•i Ä‘Æ°á»£c xem lÃ  ngÆ°á»i giÃ ?",
  "output": "NgÆ°á»i cao tuá»•i Ä‘Æ°á»£c quy Ä‘á»‹nh táº¡i Äiá»u 2 Luáº­t NgÆ°á»i cao tuá»•i 2009..."
}
```

### Format Llama-3.1 Chat (sau khi xá»­ lÃ½)
```
<|start_header_id|>system<|end_header_id|>

Báº¡n lÃ  má»™t chuyÃªn gia tÆ° váº¥n phÃ¡p luáº­t Viá»‡t Nam...<|eot_id|>
<|start_header_id|>user<|end_header_id|>

HÃ£y tráº£ lá»i chi tiáº¿t vá» quy Ä‘á»‹nh phÃ¡p lÃ½:
Trong Bá»™ luáº­t HÃ¬nh sá»± thÃ¬ bao nhiÃªu tuá»•i Ä‘Æ°á»£c xem lÃ  ngÆ°á»i giÃ ?<|eot_id|>
<|start_header_id|>assistant<|end_header_id|>

NgÆ°á»i cao tuá»•i Ä‘Æ°á»£c quy Ä‘á»‹nh táº¡i Äiá»u 2 Luáº­t NgÆ°á»i cao tuá»•i 2009...<|eot_id|>
```

### Cáº£i tiáº¿n Ä‘Ã£ thá»±c hiá»‡n

1. **ThÃªm EOS tokens**: `<|eot_id|>` cho Llama-3.1
2. **Cáº£i thiá»‡n instructions**: PhÃ¢n loáº¡i vÃ  lÃ m rÃµ cÃ¢u há»i
3. **System prompt**: ChuyÃªn gia phÃ¡p luáº­t Viá»‡t Nam
4. **Batching strategy**: Padding theo Ä‘á»™ dÃ i sequence
5. **Stratified split**: Chia dá»¯ liá»‡u cÃ¢n báº±ng theo Ä‘á»™ dÃ i

## ğŸ”§ Finetune Configuration

### LoRA Parameters (tá»‘i Æ°u cho 8B model)
```python
lora_r=16              # Rank
lora_alpha=32          # 2 * lora_r 
lora_dropout=0.05      # Dropout
target_modules=[       # Target attention layers
    "q_proj", "k_proj", "v_proj", "o_proj",
    "gate_proj", "up_proj", "down_proj"
]
```

### Training Hyperparameters
```python
learning_rate=2e-4
num_epochs=3
batch_size=2
gradient_accumulation_steps=4  # Effective batch size = 8
warmup_steps=10
max_seq_length=2048
```

### GPU Memory Optimization
- **4-bit quantization**: Load_in_4bit=True
- **Gradient checkpointing**: Unsloth optimization
- **Mixed precision**: BF16 on supported hardware

## ğŸ“ˆ Evaluation Metrics

### Automatic Metrics
- **ROUGE**: ROUGE-1, ROUGE-2, ROUGE-L
- **BLEU**: Sentence-level BLEU vá»›i smoothing
- **Perplexity**: Model confidence measure

### LLM-based Evaluation (GPT-4)
```json
{
  "accuracy": 8.5,      # Äá»™ chÃ­nh xÃ¡c (0-10)
  "completeness": 7.8,  # Äá»™ Ä‘áº§y Ä‘á»§ (0-10)
  "clarity": 9.2,       # Äá»™ rÃµ rÃ ng (0-10)
  "practicality": 8.0,  # TÃ­nh thá»±c tiá»…n (0-10)
  "overall": 8.4        # Äiá»ƒm tá»•ng thá»ƒ (0-10)
}
```

## ğŸŒ API Serving

### Endpoints

#### Chat Completions (OpenAI Compatible)
```bash
POST /v1/chat/completions
Content-Type: application/json

{
  "messages": [
    {"role": "user", "content": "Quy Ä‘á»‹nh vá» thá»i hiá»‡u khá»Ÿi kiá»‡n lÃ  gÃ¬?"}
  ],
  "temperature": 0.7,
  "max_tokens": 512
}
```

#### Streaming Response
```bash
POST /v1/chat/completions/stream
```

#### Health Check
```bash
GET /health
```

### Response Format
```json
{
  "id": "chatcmpl-123",
  "object": "chat.completion",
  "created": 1694268190,
  "model": "vietnamese-legal-llama",
  "choices": [
    {
      "index": 0,
      "message": {
        "role": "assistant",
        "content": "Thá»i hiá»‡u khá»Ÿi kiá»‡n Ä‘Æ°á»£c quy Ä‘á»‹nh..."
      },
      "finish_reason": "stop"
    }
  ],
  "usage": {
    "prompt_tokens": 20,
    "completion_tokens": 150,
    "total_tokens": 170
  }
}
```

## ğŸ‹ Docker Deployment

### Local Development
```bash
# Build image
./run_pipeline.sh build-docker

# Deploy vá»›i Docker Compose
./run_pipeline.sh deploy

# Check logs
docker-compose -f docker/docker-compose.yml logs -f
```

### Production on Digital Ocean

1. **Setup GPU Droplet H200**
```bash
# SSH vÃ o droplet
ssh root@your-droplet-ip

# Install Docker + NVIDIA Container Toolkit
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# Install NVIDIA Docker support
distribution=$(. /etc/os-release;echo $ID$VERSION_ID)
curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add -
curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | tee /etc/apt/sources.list.d/nvidia-docker.list
apt-get update && apt-get install -y nvidia-docker2
systemctl restart docker
```

2. **Deploy Application**
```bash
# Clone repository
git clone <your-repo>
cd llm_finetuning_serving

# Setup environment
cp .env.template .env
# Edit .env vá»›i production values

# Deploy
docker-compose -f docker/docker-compose.yml up -d
```

3. **Monitor & Scale**
```bash
# Check GPU utilization
nvidia-smi

# Monitor containers
docker stats

# Scale replicas (náº¿u cÃ³ multiple GPUs)
docker-compose -f docker/docker-compose.yml up -d --scale vietnamese-legal-llm=2
```

## ğŸ”‘ Environment Variables

### Required
```bash
MODEL_PATH=/app/model          # Path to finetuned model
CUDA_VISIBLE_DEVICES=0         # GPU device ID
```

### Optional (for full features)
```bash
# Training monitoring
WANDB_API_KEY=your_key

# LLM evaluation
OPENAI_API_KEY=your_key

# Model downloads
HF_TOKEN=your_token

# Data storage
DO_SPACES_KEY=your_key
DO_SPACES_SECRET=your_secret
```

## ğŸ“‹ Performance Benchmarks

### Training Time (H200 GPU)
- **Data processing**: ~10 minutes (100k examples)
- **Training**: ~4-6 hours (3 epochs)
- **Evaluation**: ~30 minutes

### Inference Performance
- **Latency**: ~200-500ms per response
- **Throughput**: ~20-50 requests/second
- **Memory**: ~12-16GB VRAM (4-bit quantization)

### Model Quality
- **ROUGE-L**: ~0.45-0.55
- **BLEU**: ~0.25-0.35
- **LLM Eval**: ~7.5-8.5/10 overall

## ğŸ› ï¸ Development

### Custom Data Format
```python
# data_processing/custom_processor.py
class CustomDataProcessor:
    def process_custom_format(self, data):
        # Implement your custom processing
        pass
```

### Custom Evaluation Metrics
```python
# evaluation/custom_metrics.py
def compute_legal_accuracy(predictions, references):
    # Implement domain-specific metrics
    pass
```

### API Extensions
```python
# serving/extensions.py
@app.post("/v1/legal/analyze")
async def analyze_legal_document(document: str):
    # Add specialized endpoints
    pass
```

## ğŸ” Troubleshooting

### Common Issues

1. **GPU Memory Error**
```bash
# Reduce batch size in config
per_device_train_batch_size=1
gradient_accumulation_steps=8
```

2. **CUDA Out of Memory**
```bash
# Use smaller model or more quantization
load_in_8bit=True
max_seq_length=1024
```

3. **Slow Training**
```bash
# Enable optimizations
use_flash_attention=True
dataloader_num_workers=4
```

### Debug Commands
```bash
# Check GPU
nvidia-smi

# Monitor training
tail -f finetune/outputs/logs/training.log

# Test API
curl -X POST http://localhost:8000/v1/chat/completions \
  -H "Content-Type: application/json" \
  -d '{"messages":[{"role":"user","content":"Test"}]}'
```

## ğŸ“š Resources

- [Unsloth Documentation](https://github.com/unslothai/unsloth)
- [Llama-3.1 Model Card](https://huggingface.co/meta-llama/Llama-3.1-8B-Instruct)
- [Digital Ocean GPU Droplets](https://www.digitalocean.com/products/gpu-droplets)
- [Vietnamese Legal Dataset](https://huggingface.co/datasets/your-legal-dataset)

## ğŸ¤ Contributing

1. Fork repository
2. Create feature branch
3. Add tests
4. Submit pull request

## ğŸ“„ License

MIT License - see LICENSE file

---

## ğŸ“ Support

Náº¿u cÃ³ váº¥n Ä‘á» gÃ¬, vui lÃ²ng táº¡o issue hoáº·c liÃªn há»‡:
- GitHub Issues: [Create Issue](https://github.com/your-repo/issues)
- Email: your-email@domain.com

**ChÃºc báº¡n thÃ nh cÃ´ng vá»›i viá»‡c finetune Vietnamese Legal LLM! ğŸš€**