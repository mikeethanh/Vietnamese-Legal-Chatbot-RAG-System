# Vietnamese Legal Chatbot RAG System ğŸ›ï¸

A comprehensive Vietnamese legal consultation chatbot system built with RAG (Retrieval-Augmented Generation) technology, modern microservices architecture, and advanced AI technologies.

## ğŸ¯ Overview

This system provides intelligent legal consultation services for Vietnamese users by combining:

- **Large Language Models (LLM)** for natural language understanding and generation
- **Vector Database** for semantic search in legal corpus
- **Legal Document Processing Pipeline** for data preparation
- **Web Chat Interface** for user interaction
- **Scalable Backend Architecture** with microservices

## âœ¨ Key Features

### ğŸ¤– AI Capabilities
- **Vietnamese Legal Consultation**: Answer questions based on Vietnamese legal documents
- **Semantic Search**: Smart search through 1.9M+ legal document corpus
- **Context-Aware Responses**: Provide accurate answers with source references
- **Real-time Chat Interface**: User-friendly web interface for legal consultation
- **Multi-format Support**: Process PDF, CSV, JSON, JSONL formats

### ğŸ”§ Technical Features
- **RAG Architecture**: Combines retrieval and generation for accurate responses
- **Vector Search**: ChromaDB for efficient semantic search
- **LLM Integration**: Support multiple models (OpenAI GPT, LLaMA, Vietnamese models)
- **Scalable Processing**: Apache Spark for large-scale document processing
- **Microservices**: Dockerized components with Redis queue management
- **Data Pipeline**: Automated ETL for legal document collection and processing

## ğŸ—ï¸ System Architecture

![System Architecture](asset%20/architecture_template.drawio.svg)

## ğŸ“ Project Structure

```
Vietnamese-Legal-Chatbot-RAG-System/
â”‚
â”œâ”€â”€ ğŸ–¥ï¸ backend/                    # Backend API service (FastAPI)
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ app.py                # Main FastAPI application
â”‚   â”‚   â”œâ”€â”€ agent.py              # AI agent with tool calling
â”‚   â”‚   â”œâ”€â”€ brain.py              # LLM integration and chat logic
â”‚   â”‚   â”œâ”€â”€ cache.py              # Redis caching utilities
â”‚   â”‚   â”œâ”€â”€ configs.py            # Configuration management
â”‚   â”‚   â”œâ”€â”€ custom_embedding.py   # Custom embedding models
â”‚   â”‚   â”œâ”€â”€ database.py           # Database connections
â”‚   â”‚   â”œâ”€â”€ import_data.py        # Data import utilities
â”‚   â”‚   â”œâ”€â”€ legal_tools.py        # Legal-specific tools
â”‚   â”‚   â”œâ”€â”€ models.py             # Pydantic data models
â”‚   â”‚   â”œâ”€â”€ query_rewriter.py     # Query optimization
â”‚   â”‚   â”œâ”€â”€ rerank.py             # Result re-ranking
â”‚   â”‚   â”œâ”€â”€ search.py             # Search functionality
â”‚   â”‚   â”œâ”€â”€ splitter.py           # Document text splitting
â”‚   â”‚   â”œâ”€â”€ summarizer.py         # Text summarization
â”‚   â”‚   â”œâ”€â”€ tasks.py              # Celery background tasks
â”‚   â”‚   â”œâ”€â”€ tavily_tool.py        # Web search integration
â”‚   â”‚   â”œâ”€â”€ utils.py              # Utility functions
â”‚   â”‚   â””â”€â”€ vectorize.py          # Vector database operations
â”‚   â”œâ”€â”€ data/
â”‚   â”‚   â”œâ”€â”€ train.jsonl          # Raw training data
â”‚   â”‚   â””â”€â”€ train_qa_format.jsonl # Formatted training data
â”‚   â”œâ”€â”€ docker-compose.yml       # Backend services
â”‚   â”œâ”€â”€ Dockerfile               # Container configuration
â”‚   â”œâ”€â”€ entrypoint.sh           # Container startup script
â”‚   â”œâ”€â”€ import_data.sh          # Data import script
â”‚   â”œâ”€â”€ migration_title_to_question.sql # Database migration
â”‚   â”œâ”€â”€ requirements.txt          # Python dependencies
â”‚   â”œâ”€â”€ ğŸ“š AGENT_TOOLS_GUIDE.md  # Agent tools documentation
â”‚   â”œâ”€â”€ ğŸ“š MIGRATION_GUIDE.md    # Migration guide
â”‚   â””â”€â”€ ğŸ“š README.md            # Backend documentation
â”‚
â”œâ”€â”€ ğŸŒ frontend/                   # Web interface (Streamlit)
â”‚   â”œâ”€â”€ chat_interface.py        # Main chat application
â”‚   â”œâ”€â”€ config.toml             # Streamlit configuration
â”‚   â”œâ”€â”€ docker-compose.yml      # Frontend services
â”‚   â”œâ”€â”€ Dockerfile              # Container configuration
â”‚   â”œâ”€â”€ entrypoint.sh           # Container startup script
â”‚   â””â”€â”€ requirements.txt        # Python dependencies
â”‚
â”œâ”€â”€ ğŸ”„ data_pipeline/             # Data processing pipeline
â”‚   â”œâ”€â”€ utils/                  # Processing utilities
â”‚   â”‚   â”œâ”€â”€ download_embed_data.ipynb      # Download legal corpus
â”‚   â”‚   â”œâ”€â”€ merge_instruction_data.py      # Merge instruction datasets
â”‚   â”‚   â”œâ”€â”€ process_finetune_data.ipynb    # Process training data
â”‚   â”‚   â”œâ”€â”€ process_finetune_data_2.ipynb  # ViLQA dataset processing
â”‚   â”‚   â””â”€â”€ process_finetune_data_3.ipynb  # Extended dataset processing
â”‚   â”œâ”€â”€ data/                   # Raw and processed data
â”‚   â”‚   â”œâ”€â”€ embed/              # Embedding data
â”‚   â”‚   â”œâ”€â”€ finetune_data/      # Fine-tuning datasets
â”‚   â”‚   â”œâ”€â”€ finetune_data2/     # ViLQA dataset
â”‚   â”‚   â”œâ”€â”€ finetune_data3/     # Extended fine-tuning data
â”‚   â”‚   â”œâ”€â”€ finetune_llm/       # LLM fine-tuning data
â”‚   â”‚   â””â”€â”€ rag/                # RAG-specific data
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â””â”€â”€ ğŸ“š README.md           # Pipeline documentation
â”‚
â”œâ”€â”€ ğŸ¤– llm_finetuning_serving/    # LLM fine-tuning and serving
â”‚   â”œâ”€â”€ data_processing/        # Data processing for LLM
â”‚   â”‚   â”œâ”€â”€ splits/             # Data splits
â”‚   â”‚   â”œâ”€â”€ analyze_data.py     # Data analysis
â”‚   â”‚   â”œâ”€â”€ data_analysis.json  # Analysis results
â”‚   â”‚   â”œâ”€â”€ download_data.py    # Download datasets
â”‚   â”‚   â”œâ”€â”€ process_llama_data.py # Process LLaMA data
â”‚   â”‚   â”œâ”€â”€ processed_llama_data.jsonl # Processed data
â”‚   â”‚   â”œâ”€â”€ sample_processed_data.json # Sample data
â”‚   â”‚   â””â”€â”€ split_data.py       # Split datasets
â”‚   â”œâ”€â”€ docker/                 # Docker configurations
â”‚   â”‚   â””â”€â”€ docker-compose.yml  # LLM serving containers
â”‚   â”œâ”€â”€ evaluation/             # Model evaluation
â”‚   â”‚   â””â”€â”€ evaluate_model.py   # Evaluation scripts
â”‚   â”œâ”€â”€ finetune/               # Fine-tuning scripts
â”‚   â”‚   â””â”€â”€ train_llama.py      # LLaMA training
â”‚   â”œâ”€â”€ serving/                # Model serving
â”‚   â”‚   â””â”€â”€ serve_model.py      # Model serving script
â”‚   â”œâ”€â”€ do_spaces_manager.py    # DigitalOcean Spaces manager
â”‚   â”œâ”€â”€ prepare_data.sh         # Data preparation script
â”‚   â”œâ”€â”€ requirements.txt        # Python dependencies
â”‚   â”œâ”€â”€ run_pipeline.sh         # Pipeline runner
â”‚   â”œâ”€â”€ test_api.py            # API testing
â”‚   â”œâ”€â”€ ğŸ“š DEPLOYMENT_GUIDE.md  # Deployment guide
â”‚   â”œâ”€â”€ ğŸ“š README.md           # LLM documentation
â”‚   â””â”€â”€ ğŸ“š SYSTEM_OVERVIEW.md  # System overview
â”‚
â”œâ”€â”€ ğŸ—„ï¸ database/                  # Database setup
â”‚   â”œâ”€â”€ docker-compose.yml     # Database services
â”‚   â”œâ”€â”€ init.sql               # Initial database schema
â”‚   â””â”€â”€ ğŸ“š README.md           # Database documentation
â”‚
â”œâ”€â”€ ğŸš€ embed_serving/             # Model serving and deployment
â”‚   â”œâ”€â”€ docker-compose.serving.yml  # Production deployment
â”‚   â”œâ”€â”€ Dockerfile.cpu-serving      # CPU serving container
â”‚   â”œâ”€â”€ GPU_CPU_DEPLOYMENT_GUIDE.md # Deployment guide
â”‚   â”œâ”€â”€ requirements_serving.txt   # Serving dependencies
â”‚   â””â”€â”€ scripts/                   # Serving scripts
â”‚       â”œâ”€â”€ download_model_from_spaces.py  # Model download utility
â”‚       â””â”€â”€ serve_model.py         # Model serving script
â”‚   â”œâ”€â”€ ğŸ“š API_USAGE.md           # API usage guide
â”‚   â””â”€â”€ ğŸ“š GPU_CPU_DEPLOYMENT_GUIDE.md  # Deployment guide
â”‚
â”œâ”€â”€ ğŸ¤– models/                    # AI models and weights
â”‚   â””â”€â”€ bge-m3/                  # BGE-M3 embedding model
â”‚       â”œâ”€â”€ config.json
â”‚       â”œâ”€â”€ pytorch_model.bin
â”‚       â”œâ”€â”€ tokenizer.json
â”‚       â””â”€â”€ ...
â”‚
â”œâ”€â”€ ğŸ§ª tests/                     # Test suite
â”‚   â”œâ”€â”€ __init__.py             # Test package init
â”‚   â”œâ”€â”€ conftest.py             # Pytest configuration
â”‚   â”œâ”€â”€ test_api_simple.py      # Simple API tests
â”‚   â”œâ”€â”€ test_backend_utils.py   # Backend utility tests
â”‚   â”œâ”€â”€ test_basic.py           # Basic functionality tests
â”‚   â”œâ”€â”€ test_brain.py           # Brain module tests
â”‚   â”œâ”€â”€ test_utils.py           # Utility function tests
â”‚   â””â”€â”€ ğŸ“š TESTING_SUMMARY.md   # Testing documentation
â”‚
â”œâ”€â”€ ğŸ“ docs/                      # Documentation
â”‚   â””â”€â”€ architecture_drawio_template.md # Architecture template
â”‚   â””â”€â”€ architecture_template.drawio    # Draw.io architecture file
â”‚   â””â”€â”€ ğŸ“š TESTING.md           # Testing documentation
â”‚
â”œâ”€â”€ ğŸ—‚ï¸ scripts/                   # Build and utility scripts
â”‚   â””â”€â”€ run_working_tests.sh    # Test runner script
â”‚
â”œâ”€â”€ ğŸ¨ asset/                     # Assets and diagrams
â”‚   â””â”€â”€ architecture_template.drawio.svg # System architecture diagram
â”‚
â”œâ”€â”€ .github/                     # GitHub workflows and templates
â”œâ”€â”€ .mypy_cache/                 # MyPy cache files
â”œâ”€â”€ .pytest_cache/               # Pytest cache files
â”œâ”€â”€ coverage.xml                 # Coverage report
â”œâ”€â”€ LICENSE                      # Project license
â”œâ”€â”€ Makefile                     # Build automation
â”œâ”€â”€ mypy.ini                     # MyPy configuration
â”œâ”€â”€ .pre-commit-config.yaml      # Pre-commit hooks
â”œâ”€â”€ pyproject.toml               # Python project configuration
â”œâ”€â”€ pytest.ini                  # Pytest configuration
â”œâ”€â”€ requirements_dev.txt         # Development dependencies
â”œâ”€â”€ setup.cfg                    # Setup configuration
â””â”€â”€ ğŸ“š README.md                # This documentation
```

## ğŸ› ï¸ Technology Stack

### ğŸ–¥ï¸ Backend
- **FastAPI**: High-performance API framework with async support
- **Celery**: Distributed queue for background task processing
- **Redis**: Message broker and caching layer
- **MySQL/PostgreSQL**: Metadata and conversation history storage
- **QdrantDB**: Vector database for document embeddings
- **Pydantic**: Data validation and serialization
- **SQLAlchemy**: Database ORM
- **Databases**: Async database support

### ğŸ”„ Data Processing
- **LlamaIndex**: Document indexing and retrieval framework
- **Pandas**: Data manipulation and analysis
- **Sentence Transformers**: Specialized embedding models
- **Custom Embedding Models**: Vietnamese-optimized embeddings

### ğŸ¤– AI/ML
- **OpenAI API**: GPT-3.5/4 for production
- **LLaMA**: Open-source alternative for fine-tuning
- **BGE-M3**: Multilingual embedding model
- **Cohere**: Additional AI model support
- **LangChain**: LLM application framework
- **BM25**: Traditional text retrieval

### ğŸŒ Frontend
- **Streamlit**: Interactive web application framework
- **Python**: Core programming language
- **Tenacity**: Retry mechanism for API calls

### ğŸ—ï¸ Infrastructure
- **Docker**: Containerization
- **Docker Compose**: Multi-container orchestration
- **DigitalOcean**: Cloud hosting platform
- **DigitalOcean Spaces**: Object storage

### ğŸ” Search & Integration
- **Tavily**: Web search integration
- **Google API**: Additional search capabilities
- **Custom Search**: Legal document specific search

### ğŸ› ï¸ Development Tools
- **MyPy**: Static type checking
- **Pytest**: Testing framework
- **Pre-commit**: Code quality hooks
- **Black/isort**: Code formatting
- **Coverage**: Test coverage analysis
- **Makefile**: Build automation

## ğŸš€ Quick Start Guide

### âš¡ Prerequisites

```bash
# System requirements
- Docker and Docker Compose v2.0+
- Python 3.8+ (for development)
- 8GB+ RAM (recommended 16GB)
- GPU with 8GB+ VRAM (optional, for faster inference)
- Ubuntu 20.04+ or macOS 10.15+

# Check requirements
docker --version
docker-compose --version
python3 --version
```

### ğŸ“¦ 1. Clone and Setup

```bash
# Clone repository
git clone https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System.git
cd Vietnamese-Legal-Chatbot-RAG-System

# Create Docker network
docker network create legal-chatbot-network

# Copy environment templates
cp backend/.env.example backend/.env
cp frontend/.env.example frontend/.env
cp database/.env.example database/.env
```

### âš™ï¸ 2. Environment Configuration

#### Backend Configuration
```bash
# Edit backend/.env
nano backend/.env

# Important variables:
OPENAI_API_KEY=your_openai_api_key_here
TAVILY_API_KEY=your_tavily_api_key_here
DATABASE_URL=postgresql://legal_user:legal_pass@postgres:5432/legal_chatbot
REDIS_URL=redis://redis:6379/0
CHROMA_HOST=chromadb
CHROMA_PORT=8000
```

#### Database Configuration
```bash
# Edit database/.env
nano database/.env

# Database configuration:
POSTGRES_DB=legal_chatbot
POSTGRES_USER=legal_user
POSTGRES_PASSWORD=legal_pass
REDIS_PASSWORD=redis_secret_password
```

### ğŸ³ 3. Docker Deployment

#### Option A: Full Deployment (Recommended)
```bash
# Start all services with one command
./scripts/start_all.sh

# Or manually:
# 1. Start infrastructure services
cd database && docker-compose up -d
cd ../backend && docker-compose up -d
cd ../frontend && docker-compose up -d

# Verify all services
docker ps
```

#### Option B: Step-by-step Deployment
```bash
# 1. Database services first
cd database
docker-compose up -d
sleep 30  # Wait for database startup

# 2. Backend services
cd ../backend
docker-compose up -d
sleep 60  # Wait for backend startup and migration

# 3. Frontend service
cd ../frontend
docker-compose up -d

# 4. Import initial data
cd ../backend
./import_data.sh
```

### ğŸ“Š 4. Data Import

#### Automatic sample data import
```bash
cd backend

# Import training data
docker exec -it chatbot-api python /usr/src/app/src/import_data.py \
  --data-file /usr/src/app/data/train_qa_format.jsonl \
  --collection llm \
  --batch-size 100

# Check import success
curl -X POST http://localhost:8000/data/status
```

#### Custom data import
```bash
# Copy your data to backend/data/
cp your_data.jsonl backend/data/

# Import via API
curl -X POST http://localhost:8000/data/import \
  -H "Content-Type: application/json" \
  -d '{"file_path": "data/your_data.jsonl", "collection": "llm"}'
```

### ğŸŒ 5. Access Applications

- **ğŸ–¥ï¸ Web Chat Interface**: http://localhost:8501
- **ğŸ“š API Documentation**: http://localhost:8000/docs
- **ğŸ“Š API Health Check**: http://localhost:8000/health
- **ğŸ—„ï¸ ChromaDB Dashboard**: http://localhost:8001
- **ğŸ’¾ Database Admin** (pgAdmin): http://localhost:5050

### âœ… 6. System Verification

```bash
# Health checks
curl http://localhost:8000/health
curl http://localhost:8501/health

# Test chat API
curl -X POST http://localhost:8000/chat \
  -H "Content-Type: application/json" \
  -d '{
    "message": "What does Vietnamese civil law say about property rights?",
    "conversation_id": "test-conversation-123"
  }'

# Check logs if there are errors
docker logs chatbot-api
docker logs chatbot-worker
docker logs streamlit-app
```

## ğŸ”§ Advanced Configuration

### ğŸ¤– AI Model Configuration

#### OpenAI Models
```bash
# In backend/.env
OPENAI_API_KEY=sk-your-key-here
DEFAULT_MODEL=gpt-3.5-turbo
FALLBACK_MODEL=gpt-4
MAX_TOKENS=4000
TEMPERATURE=0.1
```

#### Self-hosted Models
```bash
# Configuration for local LLM
LOCAL_MODEL_URL=http://localhost:11434
LOCAL_MODEL_NAME=llama2:13b-chat
USE_LOCAL_MODEL=true
```

### ğŸ” Vector Search Configuration
```bash
# Embedding model configuration
EMBEDDING_MODEL=sentence-transformers/all-MiniLM-L6-v2
EMBEDDING_DIMENSION=384
SIMILARITY_THRESHOLD=0.7
MAX_SEARCH_RESULTS=10

# ChromaDB configuration
CHROMA_COLLECTION=legal_documents
CHROMA_DISTANCE_FUNCTION=cosine
```

### ğŸ’¾ Database Configuration
```bash
# PostgreSQL optimization
POSTGRES_SHARED_BUFFERS=256MB
POSTGRES_EFFECTIVE_CACHE_SIZE=1GB
POSTGRES_MAX_CONNECTIONS=100

# Redis configuration
REDIS_MAXMEMORY=512mb
REDIS_MAXMEMORY_POLICY=allkeys-lru
```

## ğŸ“Š Data Pipeline - Detailed Guide

### ğŸ”„ RAG Data Processing

Detailed data processing is described in:
ğŸ“– **[Data Pipeline Documentation](data_pipeline/README.md)**

#### Quick Start Data Processing
```bash
cd data_pipeline

# 1. Download Vietnamese legal corpus
jupyter notebook utils/download_embed_data.ipynb

# 2. Process fine-tuning data
jupyter notebook utils/process_finetune_data.ipynb

# 3. Validate data quality
python scripts/validate_data.py
```

### ğŸ“ˆ Current Data Statistics
- **ğŸ“š Legal Corpus**: 1.9M+ Vietnamese legal documents
- **ğŸ’¬ Training Data**: 225K+ high-quality Q&A pairs
- **ğŸ¯ Fine-tuning Sets**: 3 specialized datasets
- **ğŸ“Š Coverage**: Complete coverage of major legal domains

## ğŸ§ª Testing and Quality Assurance

### ğŸ” Automated Testing
```bash
# Run full test suite
./scripts/run_tests.sh

# Unit tests
cd backend && python -m pytest tests/ -v
cd frontend && python -m pytest tests/ -v

# Integration tests
python test/test_smoke.py

# Load testing
cd backend && python test/load_test.py
```

### ğŸ“Š Performance Benchmarks
```bash
# API response time
curl -w "@curl-format.txt" -X POST http://localhost:8000/chat

# Vector search performance
python test/benchmark_search.py

# Memory usage monitoring
docker stats --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}"
```

## ğŸ“ˆ Performance and Scaling

### âš¡ Expected Performance
- **ğŸ’¬ Chat Response**: 2-5 seconds for complete RAG pipeline
- **ğŸ” Vector Search**: <1 second for semantic queries
- **ğŸ“Š Document Processing**: ~1.9M documents in ~30 minutes
- **ğŸ‘¥ Concurrent Users**: 50+ simultaneous chat sessions
- **ğŸš€ Throughput**: 100+ requests/minute

### ğŸ”„ Scaling Options

#### Horizontal Scaling
```bash
# Scale backend replicas
docker-compose up -d --scale chatbot-api=3

# Load balancer configuration
cd nginx && docker-compose up -d
```

#### Database Scaling
```bash
# PostgreSQL read replicas
cd database && docker-compose -f docker-compose.replica.yml up -d

# Redis clustering
cd redis-cluster && docker-compose up -d
```

#### Vector Database Scaling
```bash
# ChromaDB cluster mode
CHROMA_CLUSTER_MODE=true
CHROMA_NODES=3

# Alternative: Pinecone cloud
USE_PINECONE=true
PINECONE_API_KEY=your-pinecone-key
```

## ğŸš€ Production Deployment

### â˜ï¸ Digital Ocean Deployment
```bash
# Detailed deployment guide
cd embed_serving
cat GPU_CPU_DEPLOYMENT_GUIDE.md

# Quick production deploy
docker-compose -f docker-compose.serving.yml up -d
```

### ğŸ” Security Checklist
- [ ] API rate limiting enabled
- [ ] Database credentials encrypted
- [ ] HTTPS/SSL certificates setup
- [ ] Firewall rules configured
- [ ] Regular security updates scheduled
- [ ] Backup strategy established

### ğŸ“Š Monitoring Setup
```bash
# Prometheus + Grafana
cd monitoring && docker-compose up -d

# Alerts configuration
cp alerts.yml.example alerts.yml
```

## ğŸ› ï¸ Development Guide

### ğŸ—ï¸ Local Development
```bash
# Setup virtual environment
python -m venv venv
source venv/bin/activate

# Install development dependencies
pip install -r requirements_dev.txt

# Start services for development
cd backend && python src/app.py
cd frontend && streamlit run chat_interface.py
```

### ğŸ”„ Code Style and Quality
```bash
# Code formatting
black backend/src/ frontend/
isort backend/src/ frontend/

# Linting
flake8 backend/src/
pylint frontend/

# Type checking
mypy backend/src/
```

### ğŸ“š API Development
```bash
# Generate API documentation
cd backend && python scripts/generate_docs.py

# Test API endpoints
cd backend && python scripts/test_endpoints.py

# Update OpenAPI spec
cd backend && python scripts/update_openapi.py
```

## ğŸ¤ Contributing

### ğŸ“‹ Contribution Process
1. **Fork repository** and create feature branch
2. **Implement changes** with tests and documentation
3. **Run quality checks**: `./scripts/pre-commit.sh`
4. **Submit Pull Request** with detailed description
5. **Code review** and merge approval

### ğŸ› Bug Reports
- Use GitHub Issues with provided template
- Include logs, screenshots, reproduction steps
- Label appropriately (bug, enhancement, question)

### ğŸ’¡ Feature Requests
- Describe use case and expected behavior
- Consider backward compatibility
- Provide implementation suggestions if possible

## ğŸ†˜ Troubleshooting

### âŒ Common Issues

#### Connection Errors
```bash
# Database connection failed
docker exec -it postgres psql -U legal_user -d legal_chatbot

# Redis connection failed
docker exec -it redis redis-cli ping

# ChromaDB not accessible
curl http://localhost:8001/api/v1/heartbeat
```

#### Memory Issues
```bash
# Check memory usage
docker stats

# Increase memory limits
# In docker-compose.yml
services:
  chatbot-api:
    mem_limit: 4g
    memswap_limit: 4g
```

#### Performance Issues
```bash
# Profile application
cd backend && python -m cProfile -o profile.stats src/app.py

# Database query optimization
cd backend && python scripts/analyze_queries.py

# Vector search optimization
cd backend && python scripts/optimize_embeddings.py
```

### ğŸ”§ Debug Mode
```bash
# Enable debug logging
export DEBUG=true
export LOG_LEVEL=DEBUG

# Run with detailed logging
docker-compose -f docker-compose.debug.yml up
```

## ğŸ“„ Documentation

### ğŸ“š Additional Resources
- **[Backend API Documentation](backend/README.md)** - Detailed backend implementation
- **[Data Pipeline Guide](data_pipeline/README.md)** - Comprehensive data processing
- **[Database Schema](database/README.md)** - Database design and migrations
- **[Deployment Guide](embed_serving/GPU_CPU_DEPLOYMENT_GUIDE.md)** - Production deployment
- **[API Usage Examples](embed_serving/API_USAGE.md)** - API usage patterns

### ğŸ“ Tutorials
- **Setup Development Environment**: `docs/tutorials/dev-setup.md`
- **Custom Model Integration**: `docs/tutorials/custom-models.md`
- **Data Processing Workflow**: `docs/tutorials/data-processing.md`
- **Production Deployment**: `docs/tutorials/production-deploy.md`

## ğŸ“Š Metrics and Analytics

### ğŸ“ˆ Key Performance Indicators (KPIs)
- **User Engagement**: Session duration, questions per session
- **System Performance**: Response time, error rates
- **Content Quality**: User satisfaction ratings
- **Technical Metrics**: Uptime, throughput, resource utilization

### ğŸ“Š Dashboard Access
- **System Metrics**: http://localhost:3000 (Grafana)
- **Application Logs**: http://localhost:5601 (Kibana)
- **Database Monitoring**: http://localhost:5050 (pgAdmin)

## ğŸ™ Acknowledgments

### ğŸ“ Research & Data Sources
- **Vietnamese Legal Corpus**: Zalo AI Challenge dataset
- **ViLQA Dataset**: Vietnamese Legal Question Answering
- **Legal Document Sources**: Ministry of Justice, Vietnamese National Assembly
- **Academic Research**: VinAI, FPT AI, University partners

### ğŸ› ï¸ Technology Partners
- **OpenAI**: GPT models and embedding APIs
- **Hugging Face**: Model hosting and transformers library
- **ChromaDB**: Vector database technology
- **Streamlit**: Web interface framework
- **FastAPI**: High-performance API framework

### ğŸ‘¥ Contributors
- **Core Development Team**: [@mikeethanh](https://github.com/mikeethanh)
- **Data Science Team**: Vietnamese legal experts
- **Beta Testers**: Legal practitioners and students
- **Open Source Community**: Contributors and issue reporters

## ğŸ“ Support & Contact

### ğŸ†˜ Getting Help
- **ğŸ“š Documentation**: [Project Wiki](https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System/wiki)
- **ğŸ’¬ Community Discussions**: [GitHub Discussions](https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System/discussions)
- **ğŸ› Bug Reports**: [GitHub Issues](https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System/issues)
- **ğŸ“§ Direct Contact**: mikeethanh@example.com

### ğŸŒŸ Community
- **Discord Server**: [Join our community](https://discord.gg/legal-chatbot)
- **Telegram Group**: [Vietnamese Legal AI](https://t.me/vietnamese_legal_ai)
- **LinkedIn**: [Project Updates](https://linkedin.com/company/vietnamese-legal-ai)

## ğŸ“„ License

This project is distributed under **MIT License** - see the [LICENSE](LICENSE) file for details.

### ğŸ”’ Usage Terms
- âœ… Commercial use allowed
- âœ… Modification and distribution allowed
- âœ… Private use encouraged
- â— No warranty provided
- â— Legal advice disclaimer applies

---

## âš ï¸ Disclaimer

**Important Statement**: This system is designed for **research, educational, and reference support purposes**.

ğŸš¨ **Legal Notice**:
- AI results cannot replace consultation from qualified lawyers
- Always verify information with legal professionals before making decisions
- The system may have errors and does not guarantee 100% accuracy
- Do not use for important legal matters without professional consultation

---

<div align="center">

**â­ If this project is helpful, please star the repository to support the development team! â­**

Made with â¤ï¸ for Vietnamese Legal Community

[ğŸŒŸ Star](https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System/stargazers) â€¢ [ğŸ´ Fork](https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System/fork) â€¢ [ğŸ“š Docs](https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System/wiki) â€¢ [ğŸ’¬ Discord](https://discord.gg/legal-chatbot)

</div>
