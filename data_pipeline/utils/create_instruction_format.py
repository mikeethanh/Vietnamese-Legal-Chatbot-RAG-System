#!/usr/bin/env python3
"""
Script to generate answers for fine-tuning data using a pre-trained model
This helps create question-context-answer triples for better fine-tuning
"""
import json
import argparse
from pathlib import Path

def create_instruction_format(input_file, output_file, format_type="alpaca"):
    """
    Convert question-context pairs to instruction-following format
    
    Args:
        input_file: Input JSONL file with question-context pairs
        output_file: Output JSONL file with instruction format
        format_type: "alpaca" or "llama2" format
    """
    print(f"üîÑ Converting {input_file} to {format_type} instruction format...")
    
    count = 0
    with open(input_file, 'r', encoding='utf-8') as infile, \
         open(output_file, 'w', encoding='utf-8') as outfile:
        
        for line in infile:
            data = json.loads(line.strip())
            
            if format_type == "alpaca":
                # Alpaca format
                instruction_data = {
                    "instruction": "D·ª±a v√†o vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p, h√£y tr·∫£ l·ªùi c√¢u h·ªèi m·ªôt c√°ch ch√≠nh x√°c v√† chi ti·∫øt.",
                    "input": f"C√¢u h·ªèi: {data['question']}\n\nVƒÉn b·∫£n tham kh·∫£o: {data['context']}",
                    "output": "[C√¢u tr·∫£ l·ªùi s·∫Ω ƒë∆∞·ª£c t·∫°o d·ª±a tr√™n context]",
                    "source": data.get('source', ''),
                    "id": data.get('id', '')
                }
            
            elif format_type == "llama2":
                # LLaMA-2 chat format
                instruction_data = {
                    "messages": [
                        {
                            "role": "system",
                            "content": "B·∫°n l√† m·ªôt tr·ª£ l√Ω AI chuy√™n v·ªÅ ph√°p lu·∫≠t Vi·ªát Nam. H√£y tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n vƒÉn b·∫£n ph√°p lu·∫≠t ƒë∆∞·ª£c cung c·∫•p."
                        },
                        {
                            "role": "user", 
                            "content": f"D·ª±a v√†o vƒÉn b·∫£n sau:\n\n{data['context']}\n\nH√£y tr·∫£ l·ªùi c√¢u h·ªèi: {data['question']}"
                        },
                        {
                            "role": "assistant",
                            "content": "[C√¢u tr·∫£ l·ªùi s·∫Ω ƒë∆∞·ª£c t·∫°o d·ª±a tr√™n context]"
                        }
                    ],
                    "source": data.get('source', ''),
                    "id": data.get('id', '')
                }
            
            json.dump(instruction_data, outfile, ensure_ascii=False)
            outfile.write('\n')
            count += 1
            
            if count % 10000 == 0:
                print(f"   üìä Processed {count:,} entries...")
    
    print(f"‚úÖ Converted {count:,} entries to {format_type} format")
    print(f"üíæ Output saved to: {output_file}")

def main():
    parser = argparse.ArgumentParser(description="Convert QA data to instruction format")
    parser.add_argument("input_file", help="Input JSONL file with question-context pairs")
    parser.add_argument("output_file", help="Output JSONL file with instruction format")
    parser.add_argument("--format", choices=["alpaca", "llama2"], default="alpaca",
                       help="Instruction format type")
    
    args = parser.parse_args()
    
    if not Path(args.input_file).exists():
        print(f"‚ùå Input file not found: {args.input_file}")
        return
    
    create_instruction_format(args.input_file, args.output_file, args.format)

if __name__ == "__main__":
    main()