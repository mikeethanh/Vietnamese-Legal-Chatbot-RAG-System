{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "88525e18",
   "metadata": {},
   "source": [
    "# So s√°nh m√¥ h√¨nh Embedding: BGE-M3 vs ViEmbedding-base\n",
    "\n",
    "Notebook n√†y so s√°nh hi·ªáu su·∫•t c·ªßa hai m√¥ h√¨nh:\n",
    "- BAAI/bge-m3\n",
    "- anti-ai/ViEmbedding-base\n",
    "\n",
    "Tr√™n dataset: anti-ai/ViNLI-Zalo-supervised\n",
    "S·ª≠ d·ª•ng: TripletLoss v√† TripletEvaluator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9d0f5aa",
   "metadata": {},
   "source": [
    "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec60c87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentence-transformers datasets transformers torch accelerate huggingface_hub tqdm pandas numpy scikit-learn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d01d213d",
   "metadata": {},
   "source": [
    "## 2. Import th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a080fe2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer, InputExample, losses, evaluation\n",
    "from torch.utils.data import DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from datetime import datetime\n",
    "import os\n",
    "\n",
    "# Ki·ªÉm tra GPU\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "if device == \"cuda\":\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3bed23b",
   "metadata": {},
   "source": [
    "## 3. T·∫£i dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbd0858",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫£i dataset t·ª´ HuggingFace\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"anti-ai/ViNLI-Zalo-supervised\")\n",
    "\n",
    "print(f\"Dataset structure: {dataset}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(dataset['train'][0])\n",
    "\n",
    "# Ki·ªÉm tra s·ªë l∆∞·ª£ng samples\n",
    "if 'train' in dataset:\n",
    "    print(f\"\\nTrain samples: {len(dataset['train'])}\")\n",
    "if 'validation' in dataset:\n",
    "    print(f\"Validation samples: {len(dataset['validation'])}\")\n",
    "if 'test' in dataset:\n",
    "    print(f\"Test samples: {len(dataset['test'])}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c14fcaf",
   "metadata": {},
   "source": [
    "## 4. Chu·∫©n b·ªã d·ªØ li·ªáu cho Triplet Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f03b5252",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_triplet_data(dataset_split):\n",
    "    \"\"\"\n",
    "    Chuy·ªÉn ƒë·ªïi dataset th√†nh format InputExample cho TripletLoss\n",
    "    Format: (anchor, positive, negative)\n",
    "    \"\"\"\n",
    "    triplet_examples = []\n",
    "    \n",
    "    for sample in tqdm(dataset_split, desc=\"Preparing triplet data\"):\n",
    "        # L·∫•y query (anchor), positive, v√† hard_negative\n",
    "        query = sample['query']\n",
    "        positive = sample['positive']\n",
    "        negative = sample['hard_neg']\n",
    "        \n",
    "        # T·∫°o InputExample v·ªõi texts ch·ª©a [anchor, positive, negative]\n",
    "        example = InputExample(texts=[query, positive, negative])\n",
    "        triplet_examples.append(example)\n",
    "    \n",
    "    return triplet_examples\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu\n",
    "print(\"\\nPreparing training data...\")\n",
    "train_examples = prepare_triplet_data(dataset['train'])\n",
    "\n",
    "# N·∫øu c√≥ validation set, chu·∫©n b·ªã cho evaluation\n",
    "if 'validation' in dataset:\n",
    "    print(\"Preparing validation data...\")\n",
    "    val_examples = prepare_triplet_data(dataset['validation'])\n",
    "elif 'test' in dataset:\n",
    "    print(\"Preparing test data...\")\n",
    "    val_examples = prepare_triplet_data(dataset['test'])\n",
    "else:\n",
    "    # N·∫øu kh√¥ng c√≥ validation/test, chia train th√†nh 90-10\n",
    "    print(\"Splitting train data into train/val (90-10)...\")\n",
    "    split_idx = int(len(train_examples) * 0.9)\n",
    "    val_examples = train_examples[split_idx:]\n",
    "    train_examples = train_examples[:split_idx]\n",
    "\n",
    "print(f\"\\nTrain examples: {len(train_examples)}\")\n",
    "print(f\"Validation examples: {len(val_examples)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84436be7",
   "metadata": {},
   "source": [
    "## 5. T·∫°o TripletEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44fb127e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_triplet_evaluator(examples, name=\"validation\"):\n",
    "    \"\"\"\n",
    "    T·∫°o TripletEvaluator t·ª´ list InputExample\n",
    "    \"\"\"\n",
    "    anchors = []\n",
    "    positives = []\n",
    "    negatives = []\n",
    "    \n",
    "    for example in examples:\n",
    "        anchors.append(example.texts[0])\n",
    "        positives.append(example.texts[1])\n",
    "        negatives.append(example.texts[2])\n",
    "    \n",
    "    evaluator = evaluation.TripletEvaluator(\n",
    "        anchors=anchors,\n",
    "        positives=positives,\n",
    "        negatives=negatives,\n",
    "        name=name\n",
    "    )\n",
    "    \n",
    "    return evaluator\n",
    "\n",
    "# T·∫°o evaluator\n",
    "print(\"Creating evaluator...\")\n",
    "evaluator = create_triplet_evaluator(val_examples, name=\"vinli-zalo-eval\")\n",
    "print(\"Evaluator created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a5536e7",
   "metadata": {},
   "source": [
    "## 6. H√†m ƒë√°nh gi√° m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70694f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model_name, evaluator, device='cuda'):\n",
    "    \"\"\"\n",
    "    ƒê√°nh gi√° m·ªôt m√¥ h√¨nh tr√™n evaluator\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Evaluating model: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    try:\n",
    "        # Load model\n",
    "        print(f\"Loading model from {model_name}...\")\n",
    "        model = SentenceTransformer(model_name, device=device)\n",
    "        \n",
    "        # Evaluate\n",
    "        print(\"Running evaluation...\")\n",
    "        score = evaluator(model)\n",
    "        \n",
    "        # X·ª≠ l√Ω tr∆∞·ªùng h·ª£p score l√† dictionary ho·∫∑c s·ªë\n",
    "        if isinstance(score, dict):\n",
    "            accuracy = score.get('accuracy', score.get('cosine_accuracy', 0))\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Full results: {score}\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        else:\n",
    "            accuracy = score\n",
    "            print(f\"\\nResults for {model_name}:\")\n",
    "            print(f\"Accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': accuracy,\n",
    "            'full_results': score if isinstance(score, dict) else {'accuracy': score},\n",
    "            'status': 'success'\n",
    "        }\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error evaluating {model_name}: {str(e)}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        return {\n",
    "            'model_name': model_name,\n",
    "            'accuracy': None,\n",
    "            'status': 'failed',\n",
    "            'error': str(e)\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a793b452",
   "metadata": {},
   "source": [
    "## 7. ƒê√°nh gi√° c·∫£ hai m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90787e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Danh s√°ch m√¥ h√¨nh c·∫ßn ƒë√°nh gi√°\n",
    "models_to_evaluate = [\n",
    "    \"BAAI/bge-m3\",\n",
    "    \"anti-ai/ViEmbedding-base\"\n",
    "]\n",
    "\n",
    "# ƒê√°nh gi√° t·ª´ng m√¥ h√¨nh\n",
    "results = []\n",
    "\n",
    "for model_name in models_to_evaluate:\n",
    "    result = evaluate_model(model_name, evaluator, device=device)\n",
    "    results.append(result)\n",
    "    \n",
    "    # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "    if device == \"cuda\":\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Evaluation completed!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5b964a2",
   "metadata": {},
   "source": [
    "## 8. Hi·ªÉn th·ªã k·∫øt qu·∫£ so s√°nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c512df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o DataFrame ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£\n",
    "df_results = pd.DataFrame(results)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON RESULTS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string(index=False))\n",
    "\n",
    "# T√¨m m√¥ h√¨nh t·ªët nh·∫•t\n",
    "successful_results = [r for r in results if r['status'] == 'success']\n",
    "if successful_results:\n",
    "    best_model = max(successful_results, key=lambda x: x['accuracy'])\n",
    "    print(f\"\\nüèÜ Best Model: {best_model['model_name']}\")\n",
    "    print(f\"   Accuracy: {best_model['accuracy']:.4f}\")\n",
    "    \n",
    "    # T√≠nh ƒë·ªô ch√™nh l·ªách\n",
    "    if len(successful_results) == 2:\n",
    "        diff = abs(successful_results[0]['accuracy'] - successful_results[1]['accuracy'])\n",
    "        print(f\"\\nüìä Accuracy difference: {diff:.4f} ({diff*100:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fe676af",
   "metadata": {},
   "source": [
    "## 9. Inference chi ti·∫øt tr√™n m·ªôt s·ªë m·∫´u"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d209595a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detailed_inference(model_name, examples, num_samples=5, device='cuda'):\n",
    "    \"\"\"\n",
    "    Th·ª±c hi·ªán inference chi ti·∫øt tr√™n m·ªôt s·ªë m·∫´u\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"Detailed Inference - Model: {model_name}\")\n",
    "    print(f\"{'='*80}\")\n",
    "    \n",
    "    # Load model\n",
    "    model = SentenceTransformer(model_name, device=device)\n",
    "    \n",
    "    # Ch·ªçn ng·∫´u nhi√™n m·ªôt s·ªë samples\n",
    "    sample_indices = np.random.choice(len(examples), min(num_samples, len(examples)), replace=False)\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for idx in sample_indices:\n",
    "        example = examples[idx]\n",
    "        query = example.texts[0]\n",
    "        positive = example.texts[1]\n",
    "        negative = example.texts[2]\n",
    "        \n",
    "        # Encode\n",
    "        query_emb = model.encode(query, convert_to_tensor=True)\n",
    "        pos_emb = model.encode(positive, convert_to_tensor=True)\n",
    "        neg_emb = model.encode(negative, convert_to_tensor=True)\n",
    "        \n",
    "        # T√≠nh similarity\n",
    "        from sentence_transformers import util\n",
    "        pos_sim = util.cos_sim(query_emb, pos_emb).item()\n",
    "        neg_sim = util.cos_sim(query_emb, neg_emb).item()\n",
    "        \n",
    "        # X√°c ƒë·ªãnh ƒë√∫ng/sai\n",
    "        correct = pos_sim > neg_sim\n",
    "        \n",
    "        result = {\n",
    "            'query': query[:100] + '...' if len(query) > 100 else query,\n",
    "            'positive': positive[:100] + '...' if len(positive) > 100 else positive,\n",
    "            'negative': negative[:100] + '...' if len(negative) > 100 else negative,\n",
    "            'pos_similarity': pos_sim,\n",
    "            'neg_similarity': neg_sim,\n",
    "            'correct': correct\n",
    "        }\n",
    "        results.append(result)\n",
    "        \n",
    "        # In k·∫øt qu·∫£\n",
    "        print(f\"\\nSample {idx + 1}:\")\n",
    "        print(f\"Query: {result['query']}\")\n",
    "        print(f\"Positive: {result['positive']}\")\n",
    "        print(f\"Negative: {result['negative']}\")\n",
    "        print(f\"Positive Similarity: {pos_sim:.4f}\")\n",
    "        print(f\"Negative Similarity: {neg_sim:.4f}\")\n",
    "        print(f\"Result: {'‚úì CORRECT' if correct else '‚úó INCORRECT'}\")\n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Th·ª±c hi·ªán inference chi ti·∫øt cho c·∫£ hai m√¥ h√¨nh\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED INFERENCE ON SAMPLE DATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "inference_results = {}\n",
    "for model_name in models_to_evaluate:\n",
    "    try:\n",
    "        results = detailed_inference(model_name, val_examples, num_samples=5, device=device)\n",
    "        inference_results[model_name] = results\n",
    "        \n",
    "        # Gi·∫£i ph√≥ng b·ªô nh·ªõ\n",
    "        if device == \"cuda\":\n",
    "            torch.cuda.empty_cache()\n",
    "    except Exception as e:\n",
    "        print(f\"Error in detailed inference for {model_name}: {str(e)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75455aa1",
   "metadata": {},
   "source": [
    "## 10. L∆∞u k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd75e8a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o th∆∞ m·ª•c k·∫øt qu·∫£ n·∫øu ch∆∞a c√≥\n",
    "output_dir = \"./comparison_results\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# L∆∞u k·∫øt qu·∫£ t·ªïng qu√°t\n",
    "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_file = f\"{output_dir}/comparison_results_{timestamp}.json\"\n",
    "\n",
    "summary = {\n",
    "    'timestamp': timestamp,\n",
    "    'dataset': 'anti-ai/ViNLI-Zalo-supervised',\n",
    "    'evaluation_metric': 'TripletEvaluator',\n",
    "    'num_train_samples': len(train_examples),\n",
    "    'num_val_samples': len(val_examples),\n",
    "    'models_evaluated': models_to_evaluate,\n",
    "    'results': results\n",
    "}\n",
    "\n",
    "with open(results_file, 'w', encoding='utf-8') as f:\n",
    "    json.dump(summary, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "print(f\"\\nResults saved to: {results_file}\")\n",
    "\n",
    "# L∆∞u DataFrame\n",
    "csv_file = f\"{output_dir}/comparison_results_{timestamp}.csv\"\n",
    "df_results.to_csv(csv_file, index=False)\n",
    "print(f\"CSV results saved to: {csv_file}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c3a6385",
   "metadata": {},
   "source": [
    "## 11. K·∫øt lu·∫≠n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c8c242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nDataset: anti-ai/ViNLI-Zalo-supervised\")\n",
    "print(f\"Training samples: {len(train_examples)}\")\n",
    "print(f\"Validation samples: {len(val_examples)}\")\n",
    "print(f\"\\nModels compared:\")\n",
    "for i, model in enumerate(models_to_evaluate, 1):\n",
    "    print(f\"{i}. {model}\")\n",
    "\n",
    "print(f\"\\nEvaluation metric: TripletEvaluator (Triplet Accuracy)\")\n",
    "print(f\"\\nResults have been saved to: {output_dir}\")\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
