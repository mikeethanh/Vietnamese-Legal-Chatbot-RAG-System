{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "69bb207f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"chillies/vn-legal-conversation\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a52fdbe2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['Unnamed: 0', 'Question', 'Answer'],\n",
       "        num_rows: 31433\n",
       "    })\n",
       "    validation: Dataset({\n",
       "        features: ['Unnamed: 0', 'Question', 'Answer'],\n",
       "        num_rows: 1746\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['Unnamed: 0', 'Question', 'Answer'],\n",
       "        num_rows: 1747\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch c·∫•u tr√∫c dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "17027765",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Th√¥ng tin dataset:\n",
      "- Train set: 31433 samples\n",
      "- Validation set: 1746 samples\n",
      "- Test set: 1747 samples\n",
      "- Total: 34926 samples\n",
      "\n",
      "üìù C√°c tr∆∞·ªùng c√≥ s·∫µn:\n",
      "- Unnamed: 0\n",
      "- Question\n",
      "- Answer\n",
      "\n",
      "üîç Xem m·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:\n",
      "Unnamed: 0: 2879\n",
      "Question: M·∫π t√¥i v√† d∆∞·ª£ng t√¥i ·ªü v·ªõi nhau g·∫ßn 10 nƒÉm nh∆∞ng kh√¥ng ƒëƒÉng k√Ω k·∫øt h√¥n. Nay d∆∞·ª£ng t√¥i ph·∫£n b·ªôi m·∫π t√¥i...\n",
      "Answer: Kho·∫£n 1 ƒêi·ªÅu 9 Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh 2014 quy ƒë·ªãnh: ‚ÄúVi·ªác k·∫øt h√¥n ph·∫£i ƒë∆∞·ª£c ƒëƒÉng k√Ω v√† do c∆° qua...\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch chi ti·∫øt dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üìä Th√¥ng tin dataset:\")\n",
    "total_samples = len(ds['train']) + len(ds['validation']) + len(ds['test'])\n",
    "print(f\"- Train set: {len(ds['train'])} samples\")\n",
    "print(f\"- Validation set: {len(ds['validation'])} samples\") \n",
    "print(f\"- Test set: {len(ds['test'])} samples\")\n",
    "print(f\"- Total: {total_samples} samples\")\n",
    "\n",
    "print(\"\\nüìù C√°c tr∆∞·ªùng c√≥ s·∫µn:\")\n",
    "for feature in ds['train'].features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "print(\"\\nüîç Xem m·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:\")\n",
    "sample = ds['train'][0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, str) and len(value) > 100:\n",
    "        print(f\"{key}: {value[:100]}...\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b2c1443",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîó G·ªôp t·∫•t c·∫£ splits th√†nh m·ªôt dataset...\n",
      "‚úÖ ƒê√£ g·ªôp 34926 samples t·ª´ t·∫•t c·∫£ splits\n",
      "\n",
      "üìà Ph√¢n t√≠ch to√†n b·ªô dataset:\n",
      "üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\n",
      "   - Trung b√¨nh: 327.0 k√Ω t·ª±\n",
      "   - Min: 0, Max: 11938\n",
      "   - Median: 217.0\n",
      "üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\n",
      "   - Trung b√¨nh: 1435.4 k√Ω t·ª±\n",
      "   - Min: 47, Max: 26802\n",
      "   - Median: 1177.0\n",
      "üî∏ D·ªØ li·ªáu r·ªóng:\n",
      "   - C√¢u h·ªèi r·ªóng: 3\n",
      "   - C√¢u tr·∫£ l·ªùi r·ªóng: 0\n"
     ]
    }
   ],
   "source": [
    "# G·ªôp t·∫•t c·∫£ c√°c splits th√†nh m·ªôt dataset duy nh·∫•t ƒë·ªÉ training\n",
    "print(\"üîó G·ªôp t·∫•t c·∫£ splits th√†nh m·ªôt dataset...\")\n",
    "\n",
    "# Combine all splits\n",
    "all_data = []\n",
    "\n",
    "# Add all train data\n",
    "for item in ds['train']:\n",
    "    all_data.append({\n",
    "        'Question': item['Question'],\n",
    "        'Answer': item['Answer']\n",
    "    })\n",
    "\n",
    "# Add all validation data\n",
    "for item in ds['validation']:\n",
    "    all_data.append({\n",
    "        'Question': item['Question'], \n",
    "        'Answer': item['Answer']\n",
    "    })\n",
    "\n",
    "# Add all test data\n",
    "for item in ds['test']:\n",
    "    all_data.append({\n",
    "        'Question': item['Question'],\n",
    "        'Answer': item['Answer']\n",
    "    })\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ g·ªôp {len(all_data)} samples t·ª´ t·∫•t c·∫£ splits\")\n",
    "\n",
    "# Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu\n",
    "def analyze_text_quality(data):\n",
    "    \"\"\"Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng text trong dataset\"\"\"\n",
    "    questions = [item['Question'] for item in data]\n",
    "    answers = [item['Answer'] for item in data]\n",
    "    \n",
    "    print(f\"\\nüìà Ph√¢n t√≠ch to√†n b·ªô dataset:\")\n",
    "    \n",
    "    # ƒê·ªô d√†i c√¢u h·ªèi\n",
    "    question_lengths = [len(q) if q else 0 for q in questions]\n",
    "    print(f\"üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\")\n",
    "    print(f\"   - Trung b√¨nh: {np.mean(question_lengths):.1f} k√Ω t·ª±\")\n",
    "    print(f\"   - Min: {min(question_lengths)}, Max: {max(question_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(question_lengths):.1f}\")\n",
    "    \n",
    "    # ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi\n",
    "    answer_lengths = [len(a) if a else 0 for a in answers]\n",
    "    print(f\"üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\")\n",
    "    print(f\"   - Trung b√¨nh: {np.mean(answer_lengths):.1f} k√Ω t·ª±\")\n",
    "    print(f\"   - Min: {min(answer_lengths)}, Max: {max(answer_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(answer_lengths):.1f}\")\n",
    "    \n",
    "    # Ki·ªÉm tra d·ªØ li·ªáu r·ªóng\n",
    "    empty_questions = sum(1 for q in questions if not q or q.strip() == \"\")\n",
    "    empty_answers = sum(1 for a in answers if not a or a.strip() == \"\")\n",
    "    \n",
    "    print(f\"üî∏ D·ªØ li·ªáu r·ªóng:\")\n",
    "    print(f\"   - C√¢u h·ªèi r·ªóng: {empty_questions}\")\n",
    "    print(f\"   - C√¢u tr·∫£ l·ªùi r·ªóng: {empty_answers}\")\n",
    "    \n",
    "    return {\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "        'question_lengths': question_lengths,\n",
    "        'answer_lengths': answer_lengths,\n",
    "        'empty_questions': empty_questions,\n",
    "        'empty_answers': empty_answers\n",
    "    }\n",
    "\n",
    "analysis = analyze_text_quality(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "593edc58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ƒêang x·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu...\n",
      "‚úÖ Dataset: 34926 ‚Üí 34566 (gi·ªØ l·∫°i 99.0%)\n",
      "\n",
      "üìä T·ªïng k·∫øt sau x·ª≠ l√Ω:\n",
      "- T·ªïng s·ªë m·∫´u training: 34566\n",
      "- Trung b√¨nh ƒë·ªô d√†i c√¢u h·ªèi: 306.9 k√Ω t·ª±\n",
      "- Trung b√¨nh ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi: 1409.2 k√Ω t·ª±\n",
      "‚úÖ Dataset: 34926 ‚Üí 34566 (gi·ªØ l·∫°i 99.0%)\n",
      "\n",
      "üìä T·ªïng k·∫øt sau x·ª≠ l√Ω:\n",
      "- T·ªïng s·ªë m·∫´u training: 34566\n",
      "- Trung b√¨nh ƒë·ªô d√†i c√¢u h·ªèi: 306.9 k√Ω t·ª±\n",
      "- Trung b√¨nh ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi: 1409.2 k√Ω t·ª±\n"
     ]
    }
   ],
   "source": [
    "# L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def process_dataset(data, max_answer_length=8000):\n",
    "    \"\"\"X·ª≠ l√Ω dataset v√† l·ªçc d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        question = clean_text(item['Question'])\n",
    "        answer = clean_text(item['Answer'])\n",
    "        \n",
    "        # L·ªçc d·ªØ li·ªáu theo ti√™u ch√≠ ch·∫•t l∆∞·ª£ng\n",
    "        if (len(question) >= 10 and  # C√¢u h·ªèi √≠t nh·∫•t 10 k√Ω t·ª±\n",
    "            len(answer) >= 50 and    # C√¢u tr·∫£ l·ªùi √≠t nh·∫•t 50 k√Ω t·ª±\n",
    "            len(answer) <= max_answer_length and  # Gi·ªõi h·∫°n ƒë·ªô d√†i tr·∫£ l·ªùi\n",
    "            len(question) <= 2000):  # Gi·ªõi h·∫°n ƒë·ªô d√†i c√¢u h·ªèi\n",
    "            \n",
    "            processed_data.append({\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "print(\"üîß ƒêang x·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu...\")\n",
    "\n",
    "# X·ª≠ l√Ω to√†n b·ªô dataset\n",
    "processed_data = process_dataset(all_data)\n",
    "print(f\"‚úÖ Dataset: {len(all_data)} ‚Üí {len(processed_data)} (gi·ªØ l·∫°i {len(processed_data)/len(all_data)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä T·ªïng k·∫øt sau x·ª≠ l√Ω:\")\n",
    "print(f\"- T·ªïng s·ªë m·∫´u training: {len(processed_data)}\")\n",
    "print(f\"- Trung b√¨nh ƒë·ªô d√†i c√¢u h·ªèi: {np.mean([len(item['question']) for item in processed_data]):.1f} k√Ω t·ª±\")\n",
    "print(f\"- Trung b√¨nh ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi: {np.mean([len(item['answer']) for item in processed_data]):.1f} k√Ω t·ª±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e20c158e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ T·∫°o th∆∞ m·ª•c: ../data/finetune_data3\n",
      "üíæ ƒêang l∆∞u d·ªØ li·ªáu...\n",
      "‚úÖ ƒê√£ l∆∞u QA format: ../data/finetune_data3\\vilqa_qa_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u QA format: ../data/finetune_data3\\vilqa_qa_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u instruction format: ../data/finetune_data3\\vilqa_instruction_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u instruction format: ../data/finetune_data3\\vilqa_instruction_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u conversation format: ../data/finetune_data3\\vilqa_conversation_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u conversation format: ../data/finetune_data3\\vilqa_conversation_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o th∆∞ m·ª•c v√† l∆∞u d·ªØ li·ªáu v√†o data/finetune_data2\n",
    "import os\n",
    "import json\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c data/finetune_data2 (kh√°c v·ªõi dataset tr∆∞·ªõc)\n",
    "output_dir = \"../data/finetune_data3\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ T·∫°o th∆∞ m·ª•c: {output_dir}\")\n",
    "\n",
    "# L∆∞u d·ªØ li·ªáu d∆∞·ªõi nhi·ªÅu format kh√°c nhau\n",
    "\n",
    "# 1. Format JSONL (m·ªói d√≤ng l√† m·ªôt JSON object)\n",
    "def save_jsonl(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng JSONL\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 2. Format instruction (cho fine-tuning)\n",
    "def save_instruction_format(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng instruction tuning\"\"\"\n",
    "    instruction_data = []\n",
    "    for item in data:\n",
    "        instruction_item = {\n",
    "            \"instruction\": \"Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\",\n",
    "            \"input\": item['question'],\n",
    "            \"output\": item['answer']\n",
    "        }\n",
    "        instruction_data.append(instruction_item)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in instruction_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 3. Format conversation (cho chatbot training)\n",
    "def save_conversation_format(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng conversation\"\"\"\n",
    "    conversation_data = []\n",
    "    for item in data:\n",
    "        conversation_item = {\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"user\", \"content\": item['question']},\n",
    "                {\"role\": \"assistant\", \"content\": item['answer']}\n",
    "            ]\n",
    "        }\n",
    "        conversation_data.append(conversation_item)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in conversation_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "print(\"üíæ ƒêang l∆∞u d·ªØ li·ªáu...\")\n",
    "\n",
    "# L∆∞u QA format\n",
    "qa_path = os.path.join(output_dir, \"vilqa_qa_format.jsonl\")\n",
    "save_jsonl(processed_data, qa_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u QA format: {qa_path}\")\n",
    "\n",
    "# L∆∞u instruction format  \n",
    "instruction_path = os.path.join(output_dir, \"vilqa_instruction_format.jsonl\")\n",
    "save_instruction_format(processed_data, instruction_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u instruction format: {instruction_path}\")\n",
    "\n",
    "# L∆∞u conversation format\n",
    "conversation_path = os.path.join(output_dir, \"vilqa_conversation_format.jsonl\")\n",
    "save_conversation_format(processed_data, conversation_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u conversation format: {conversation_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1fc83d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u metadata: ../data/finetune_data3\\vilqa_metadata.json\n",
      "\n",
      "üìù M·∫´u d·ªØ li·ªáu c√°c format:\n",
      "\n",
      "üî∏ QA Format:\n",
      "Question: M·∫π t√¥i v√† d∆∞·ª£ng t√¥i ·ªü v·ªõi nhau g·∫ßn 10 nƒÉm nh∆∞ng kh√¥ng ƒëƒÉng k√Ω k·∫øt h√¥n. Nay d∆∞·ª£ng t√¥i ph·∫£n b·ªôi m·∫π t√¥i, c√≥ v·ª£ m·ªõi v√† mu·ªën chia ƒë√¥i s·ªë t√†i s·∫£n, trong ƒë√≥ ...\n",
      "Answer: Kho·∫£n 1 ƒêi·ªÅu 9 Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh 2014 quy ƒë·ªãnh: ‚ÄúVi·ªác k·∫øt h√¥n ph·∫£i ƒë∆∞·ª£c ƒëƒÉng k√Ω v√† do c∆° quan nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn th·ª±c hi·ªán theo quy ƒë·ªãnh c...\n",
      "\n",
      "üî∏ Instruction Format:\n",
      "Instruction: Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\n",
      "Input: M·∫π t√¥i v√† d∆∞·ª£ng t√¥i ·ªü v·ªõi nhau g·∫ßn 10 nƒÉm nh∆∞ng kh√¥ng ƒëƒÉng k√Ω k·∫øt h√¥n. Nay d∆∞·ª£ng t√¥i ph·∫£n b·ªôi m·∫π t√¥i, c√≥ v·ª£ m·ªõi v√† mu·ªën chia ƒë√¥i s·ªë t√†i s·∫£n, trong ƒë√≥ ...\n",
      "Output: Kho·∫£n 1 ƒêi·ªÅu 9 Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh 2014 quy ƒë·ªãnh: ‚ÄúVi·ªác k·∫øt h√¥n ph·∫£i ƒë∆∞·ª£c ƒëƒÉng k√Ω v√† do c∆° quan nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn th·ª±c hi·ªán theo quy ƒë·ªãnh c...\n",
      "\n",
      "üî∏ Conversation Format:\n",
      "User: M·∫π t√¥i v√† d∆∞·ª£ng t√¥i ·ªü v·ªõi nhau g·∫ßn 10 nƒÉm nh∆∞ng kh√¥ng ƒëƒÉng k√Ω k·∫øt h√¥n. Nay d∆∞·ª£ng t√¥i ph·∫£n b·ªôi m·∫π t√¥i, c√≥ v·ª£ m·ªõi v√† mu·ªën chia ƒë√¥i s·ªë t√†i s·∫£n, trong ƒë√≥ ...\n",
      "Assistant: Kho·∫£n 1 ƒêi·ªÅu 9 Lu·∫≠t h√¥n nh√¢n v√† gia ƒë√¨nh 2014 quy ƒë·ªãnh: ‚ÄúVi·ªác k·∫øt h√¥n ph·∫£i ƒë∆∞·ª£c ƒëƒÉng k√Ω v√† do c∆° quan nh√† n∆∞·ªõc c√≥ th·∫©m quy·ªÅn th·ª±c hi·ªán theo quy ƒë·ªãnh c...\n",
      "\n",
      "üéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω v√† l∆∞u 34566 m·∫´u d·ªØ li·ªáu v√†o ../data/finetune_data3\n",
      "\n",
      "üìÅ Files ƒë√£ t·∫°o:\n",
      "   - vilqa_conversation_format.jsonl: 78.61 MB\n",
      "   - vilqa_instruction_format.jsonl: 78.47 MB\n",
      "   - vilqa_metadata.json: 0.00 MB\n",
      "   - vilqa_qa_format.jsonl: 76.63 MB\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o metadata v√† t·ªïng k·∫øt\n",
    "metadata = {\n",
    "    \"dataset_info\": {\n",
    "        \"source\": \"chillies/vn-legal-conversation\",\n",
    "        \"description\": \"Vietnamese Legal Conversation Dataset processed for fine-tuning\",\n",
    "        \"total_samples\": len(processed_data),\n",
    "        \"original_splits\": {\n",
    "            \"train\": len(ds['train']),\n",
    "            \"validation\": len(ds['validation']), \n",
    "            \"test\": len(ds['test'])\n",
    "        },\n",
    "        \"combined_for_training\": True\n",
    "    },\n",
    "    \"processing_info\": {\n",
    "        \"filters_applied\": [\n",
    "            \"Minimum question length: 10 characters\",\n",
    "            \"Maximum question length: 2000 characters\",\n",
    "            \"Minimum answer length: 50 characters\", \n",
    "            \"Maximum answer length: 8000 characters\",\n",
    "            \"Text cleaning: removed extra whitespace and special characters\"\n",
    "        ],\n",
    "        \"retention_rate\": f\"{len(processed_data)/len(all_data)*100:.1f}%\"\n",
    "    },\n",
    "    \"file_formats\": {\n",
    "        \"qa_format\": \"Simple question-answer pairs\",\n",
    "        \"instruction_format\": \"Instruction tuning format with instruction/input/output\",\n",
    "        \"conversation_format\": \"Multi-turn conversation format for chatbot training\"\n",
    "    },\n",
    "    \"statistics\": {\n",
    "        \"total_samples\": len(processed_data),\n",
    "        \"avg_question_length\": round(np.mean([len(item['question']) for item in processed_data]), 1),\n",
    "        \"avg_answer_length\": round(np.mean([len(item['answer']) for item in processed_data]), 1)\n",
    "    }\n",
    "}\n",
    "\n",
    "# L∆∞u metadata\n",
    "metadata_path = os.path.join(output_dir, \"vilqa_metadata.json\")\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u metadata: {metadata_path}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu t·ª´ m·ªói format\n",
    "print(\"\\nüìù M·∫´u d·ªØ li·ªáu c√°c format:\")\n",
    "\n",
    "print(\"\\nüî∏ QA Format:\")\n",
    "sample_qa = processed_data[0]\n",
    "print(f\"Question: {sample_qa['question'][:150]}...\")\n",
    "print(f\"Answer: {sample_qa['answer'][:150]}...\")\n",
    "\n",
    "print(\"\\nüî∏ Instruction Format:\")\n",
    "print(f\"Instruction: Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\")\n",
    "print(f\"Input: {sample_qa['question'][:150]}...\")\n",
    "print(f\"Output: {sample_qa['answer'][:150]}...\")\n",
    "\n",
    "print(\"\\nüî∏ Conversation Format:\")\n",
    "print(f\"User: {sample_qa['question'][:150]}...\")\n",
    "print(f\"Assistant: {sample_qa['answer'][:150]}...\")\n",
    "\n",
    "print(f\"\\nüéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω v√† l∆∞u {len(processed_data)} m·∫´u d·ªØ li·ªáu v√†o {output_dir}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th·ªëng k√™ file\n",
    "print(f\"\\nüìÅ Files ƒë√£ t·∫°o:\")\n",
    "for filename in os.listdir(output_dir):\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"   - {filename}: {size_mb:.2f} MB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
