{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa692255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Zappu/Legal-vn\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871ddc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'context', 'cid', 'qid'],\n",
       "        num_rows: 79456\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch c·∫•u tr√∫c dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484e59c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Th√¥ng tin dataset:\n",
      "- Train set: 79456 samples\n",
      "- Total: 79456 samples\n",
      "\n",
      "üìù C√°c tr∆∞·ªùng c√≥ s·∫µn:\n",
      "- question\n",
      "- context\n",
      "- cid\n",
      "- qid\n",
      "\n",
      "üîç Xem m·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:\n",
      "question: ['Ng∆∞·ªùi h·ªçc ng√†nh qu·∫£n l√Ω khai th√°c c√¥ng tr√¨nh th·ªßy l·ª£i tr√¨nh ƒë·ªô cao ƒë·∫≥ng kh·∫£ nƒÉng h·ªçc t·∫≠p n√¢ng tr√¨nh ƒë·ªô ?']\n",
      "context: ['Kh·∫£ nƒÉng h·ªçc t·∫≠p , n√¢ng tr√¨nh ƒë·ªô - Kh·ªëi l∆∞·ª£ng ki·∫øn th·ª©c t·ªëi thi·ªÉu , nƒÉng l·ª±c h·ªçc t·ªët nghi·ªáp ng√†nh , ngh·ªÅ qu·∫£n l√Ω , khai th√°c c√¥ng tr√¨nh th·ªßy l·ª£i , t...\n",
      "cid: [62492]\n",
      "qid: 161615\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch chi ti·∫øt dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"üìä Th√¥ng tin dataset:\")\n",
    "print(f\"- Train set: {len(ds['train'])} samples\")\n",
    "print(f\"- Total: {len(ds['train'])} samples\")\n",
    "\n",
    "print(\"\\nüìù C√°c tr∆∞·ªùng c√≥ s·∫µn:\")\n",
    "for feature in ds['train'].features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "print(\"\\nüîç Xem m·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:\")\n",
    "sample = ds['train'][0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, str) and len(value) > 150:\n",
    "        print(f\"{key}: {value[:150]}...\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a70f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Chuy·ªÉn ƒë·ªïi format d·ªØ li·ªáu...\n",
      "‚úÖ ƒê√£ chuy·ªÉn ƒë·ªïi 79456 samples\n",
      "\n",
      "üìà Ph√¢n t√≠ch dataset sau chuy·ªÉn ƒë·ªïi:\n",
      "üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\n",
      "   - Trung b√¨nh: 1.0 k√Ω t·ª±\n",
      "   - Min: 1, Max: 1\n",
      "   - Median: 1.0\n",
      "üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\n",
      "   - Trung b√¨nh: 1.0 k√Ω t·ª±\n",
      "   - Min: 1, Max: 1\n",
      "   - Median: 1.0\n",
      "üî∏ D·ªØ li·ªáu r·ªóng:\n",
      "   - C√¢u h·ªèi r·ªóng: 0\n",
      "   - C√¢u tr·∫£ l·ªùi r·ªóng: 0\n",
      "\n",
      "üìù M·∫´u d·ªØ li·ªáu sau chuy·ªÉn ƒë·ªïi:\n",
      "Question: [...\n",
      "Answer: [...\n",
      "‚úÖ ƒê√£ chuy·ªÉn ƒë·ªïi 79456 samples\n",
      "\n",
      "üìà Ph√¢n t√≠ch dataset sau chuy·ªÉn ƒë·ªïi:\n",
      "üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\n",
      "   - Trung b√¨nh: 1.0 k√Ω t·ª±\n",
      "   - Min: 1, Max: 1\n",
      "   - Median: 1.0\n",
      "üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\n",
      "   - Trung b√¨nh: 1.0 k√Ω t·ª±\n",
      "   - Min: 1, Max: 1\n",
      "   - Median: 1.0\n",
      "üî∏ D·ªØ li·ªáu r·ªóng:\n",
      "   - C√¢u h·ªèi r·ªóng: 0\n",
      "   - C√¢u tr·∫£ l·ªùi r·ªóng: 0\n",
      "\n",
      "üìù M·∫´u d·ªØ li·ªáu sau chuy·ªÉn ƒë·ªïi:\n",
      "Question: [...\n",
      "Answer: [...\n"
     ]
    }
   ],
   "source": [
    "# Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ format danh s√°ch th√†nh text ƒë∆°n gi·∫£n\n",
    "print(\"üîß Chuy·ªÉn ƒë·ªïi format d·ªØ li·ªáu...\")\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi t·ª´ list th√†nh string\n",
    "converted_data = []\n",
    "for item in ds['train']:\n",
    "    # L·∫•y text ƒë·∫ßu ti√™n t·ª´ danh s√°ch question v√† context\n",
    "    question_text = item['question'][0] if item['question'] and len(item['question']) > 0 else \"\"\n",
    "    context_text = item['context'][0] if item['context'] and len(item['context']) > 0 else \"\"\n",
    "    \n",
    "    converted_data.append({\n",
    "        'Question': question_text,\n",
    "        'Answer': context_text,\n",
    "        'cid': item['cid'][0] if item['cid'] and len(item['cid']) > 0 else None,\n",
    "        'qid': item['qid']\n",
    "    })\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ chuy·ªÉn ƒë·ªïi {len(converted_data)} samples\")\n",
    "\n",
    "# Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu sau chuy·ªÉn ƒë·ªïi\n",
    "def analyze_text_quality(data):\n",
    "    \"\"\"Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng text trong dataset\"\"\"\n",
    "    questions = [item['Question'] for item in data]\n",
    "    answers = [item['Answer'] for item in data]\n",
    "    \n",
    "    print(f\"\\nüìà Ph√¢n t√≠ch dataset sau chuy·ªÉn ƒë·ªïi:\")\n",
    "    \n",
    "    # ƒê·ªô d√†i c√¢u h·ªèi\n",
    "    question_lengths = [len(q) if q else 0 for q in questions]\n",
    "    print(f\"üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\")\n",
    "    print(f\"   - Trung b√¨nh: {np.mean(question_lengths):.1f} k√Ω t·ª±\")\n",
    "    print(f\"   - Min: {min(question_lengths)}, Max: {max(question_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(question_lengths):.1f}\")\n",
    "    \n",
    "    # ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi\n",
    "    answer_lengths = [len(a) if a else 0 for a in answers]\n",
    "    print(f\"üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\")\n",
    "    print(f\"   - Trung b√¨nh: {np.mean(answer_lengths):.1f} k√Ω t·ª±\")\n",
    "    print(f\"   - Min: {min(answer_lengths)}, Max: {max(answer_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(answer_lengths):.1f}\")\n",
    "    \n",
    "    # Ki·ªÉm tra d·ªØ li·ªáu r·ªóng\n",
    "    empty_questions = sum(1 for q in questions if not q or q.strip() == \"\")\n",
    "    empty_answers = sum(1 for a in answers if not a or a.strip() == \"\")\n",
    "    \n",
    "    print(f\"üî∏ D·ªØ li·ªáu r·ªóng:\")\n",
    "    print(f\"   - C√¢u h·ªèi r·ªóng: {empty_questions}\")\n",
    "    print(f\"   - C√¢u tr·∫£ l·ªùi r·ªóng: {empty_answers}\")\n",
    "    \n",
    "    return {\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "        'question_lengths': question_lengths,\n",
    "        'answer_lengths': answer_lengths,\n",
    "        'empty_questions': empty_questions,\n",
    "        'empty_answers': empty_answers\n",
    "    }\n",
    "\n",
    "analysis = analyze_text_quality(converted_data)\n",
    "\n",
    "print(f\"\\nüìù M·∫´u d·ªØ li·ªáu sau chuy·ªÉn ƒë·ªïi:\")\n",
    "sample = converted_data[0]\n",
    "print(f\"Question: {sample['Question'][:100]}...\")\n",
    "print(f\"Answer: {sample['Answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50997925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ki·ªÉm tra chi ti·∫øt c·∫•u tr√∫c d·ªØ li·ªáu:\n",
      "\n",
      "Sample 1:\n",
      "  question type: <class 'str'>\n",
      "  question content: ['Ng∆∞·ªùi h·ªçc ng√†nh qu·∫£n l√Ω khai th√°c c√¥ng tr√¨nh th·ªßy l·ª£i tr√¨nh ƒë·ªô cao ƒë·∫≥ng kh·∫£ nƒÉng h·ªçc t·∫≠p n√¢ng tr√¨nh ƒë·ªô ?']\n",
      "  context type: <class 'str'>\n",
      "  context content: ['Kh·∫£ nƒÉng h·ªçc t·∫≠p , n√¢ng tr√¨nh ƒë·ªô - Kh·ªëi l∆∞·ª£ng ki·∫øn th·ª©c t·ªëi thi·ªÉu , nƒÉng l·ª±c h·ªçc t·ªët nghi·ªáp ng√†nh \n",
      "  cid: [62492]\n",
      "  qid: 161615\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "  question type: <class 'str'>\n",
      "  question content: ['N·ªôi dung l·ªìng gh√©p b√¨nh ƒë·∫≥ng gi·ªõi x√¢y d·ª±ng vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t quy ƒë·ªãnh ?']\n",
      "  context type: <class 'str'>\n",
      "  context content: ['N·ªôi dung l·ªìng gh√©p b√¨nh ƒë·∫≥ng gi·ªõi x√¢y d·ª±ng vƒÉn b·∫£n quy ph·∫°m ph√°p lu·∫≠t Trong ph·∫°m vi ƒëi·ªÅu ch·ªânh vƒÉn\n",
      "  cid: [151154]\n",
      "  qid: 80037\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "  question type: <class 'str'>\n",
      "  question content: ['S·∫£n ph·∫©m ph·∫ßn m·ªÅm h∆∞·ªüng ∆∞u ƒë√£i mi·ªÖn thu·∫ø , thu·∫ø ?', 'N·∫øu v√≤ng ?']\n",
      "  context type: <class 'str'>\n",
      "  context content: ['ƒêi·ªÅu 20 .', '∆Øu ƒë√£i mi·ªÖn thu·∫ø , thu·∫ø 1 .', 'Mi·ªÖn thu·∫ø b·ªën , 50 % thu·∫ø n·ªôp ch√≠n : a ) Thu nh·∫≠p doan\n",
      "  cid: [75071]\n",
      "  qid: 124074\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra l·∫°i c·∫•u tr√∫c d·ªØ li·ªáu chi ti·∫øt\n",
    "print(\"üîç Ki·ªÉm tra chi ti·∫øt c·∫•u tr√∫c d·ªØ li·ªáu:\")\n",
    "\n",
    "# Xem m·∫´u chi ti·∫øt h∆°n\n",
    "for i in range(3):\n",
    "    sample = ds['train'][i]\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  question type: {type(sample['question'])}\")\n",
    "    print(f\"  question content: {sample['question']}\")\n",
    "    print(f\"  context type: {type(sample['context'])}\")\n",
    "    print(f\"  context content: {sample['context'][:100] if len(str(sample['context'])) > 100 else sample['context']}\")\n",
    "    print(f\"  cid: {sample['cid']}\")\n",
    "    print(f\"  qid: {sample['qid']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d88332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß X·ª≠ l√Ω d·ªØ li·ªáu ƒë√∫ng c√°ch...\n",
      "   Processed 0 samples...\n",
      "   Error at sample 7: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "   Error at sample 28: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "   Error at sample 53: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "   Error at sample 58: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "   Processed 10000 samples...\n",
      "   Processed 10000 samples...\n",
      "   Processed 20000 samples...\n",
      "   Processed 20000 samples...\n",
      "   Processed 30000 samples...\n",
      "   Processed 30000 samples...\n",
      "   Processed 40000 samples...\n",
      "   Processed 40000 samples...\n",
      "   Processed 60000 samples...\n",
      "   Processed 60000 samples...\n",
      "   Processed 70000 samples...\n",
      "   Processed 70000 samples...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω 71881 Q&A pairs t·ª´ 79456 samples\n",
      "   C√≥ 8413 l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω\n",
      "\n",
      "üìà Ph√¢n t√≠ch dataset ƒë√£ x·ª≠ l√Ω:\n",
      "üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\n",
      "   - Trung b√¨nh: 63.8 k√Ω t·ª±\n",
      "   - Min: 1, Max: 191\n",
      "   - Median: 61.0\n",
      "üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\n",
      "   - Trung b√¨nh: 66.2 k√Ω t·ª±\n",
      "   - Min: 2, Max: 2915\n",
      "   - Median: 38.0\n",
      "\n",
      "üìù M·∫´u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω:\n",
      "Question: Ng∆∞·ªùi h·ªçc ng√†nh qu·∫£n l√Ω khai th√°c c√¥ng tr√¨nh th·ªßy l·ª£i tr√¨nh ƒë·ªô cao ƒë·∫≥ng kh·∫£ nƒÉng h·ªçc t·∫≠p n√¢ng tr√¨nh ƒë·ªô ?...\n",
      "Answer: Kh·∫£ nƒÉng h·ªçc t·∫≠p , n√¢ng tr√¨nh ƒë·ªô - Kh·ªëi l∆∞·ª£ng ki·∫øn th·ª©c t·ªëi thi·ªÉu , nƒÉng l·ª±c h·ªçc t·ªët nghi·ªáp ng√†nh , ngh·ªÅ qu·∫£n l√Ω , khai th√°c c√¥ng tr√¨nh th·ªßy l·ª£i , tr√¨...\n",
      "‚úÖ ƒê√£ x·ª≠ l√Ω 71881 Q&A pairs t·ª´ 79456 samples\n",
      "   C√≥ 8413 l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω\n",
      "\n",
      "üìà Ph√¢n t√≠ch dataset ƒë√£ x·ª≠ l√Ω:\n",
      "üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\n",
      "   - Trung b√¨nh: 63.8 k√Ω t·ª±\n",
      "   - Min: 1, Max: 191\n",
      "   - Median: 61.0\n",
      "üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\n",
      "   - Trung b√¨nh: 66.2 k√Ω t·ª±\n",
      "   - Min: 2, Max: 2915\n",
      "   - Median: 38.0\n",
      "\n",
      "üìù M·∫´u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω:\n",
      "Question: Ng∆∞·ªùi h·ªçc ng√†nh qu·∫£n l√Ω khai th√°c c√¥ng tr√¨nh th·ªßy l·ª£i tr√¨nh ƒë·ªô cao ƒë·∫≥ng kh·∫£ nƒÉng h·ªçc t·∫≠p n√¢ng tr√¨nh ƒë·ªô ?...\n",
      "Answer: Kh·∫£ nƒÉng h·ªçc t·∫≠p , n√¢ng tr√¨nh ƒë·ªô - Kh·ªëi l∆∞·ª£ng ki·∫øn th·ª©c t·ªëi thi·ªÉu , nƒÉng l·ª±c h·ªçc t·ªët nghi·ªáp ng√†nh , ngh·ªÅ qu·∫£n l√Ω , khai th√°c c√¥ng tr√¨nh th·ªßy l·ª£i , tr√¨...\n"
     ]
    }
   ],
   "source": [
    "# X·ª≠ l√Ω d·ªØ li·ªáu ƒë√∫ng c√°ch - parse JSON strings\n",
    "import ast\n",
    "import json\n",
    "\n",
    "print(\"üîß X·ª≠ l√Ω d·ªØ li·ªáu ƒë√∫ng c√°ch...\")\n",
    "\n",
    "# Chuy·ªÉn ƒë·ªïi t·ª´ JSON strings th√†nh d·ªØ li·ªáu th·ª±c\n",
    "processed_data = []\n",
    "errors = 0\n",
    "\n",
    "for i, item in enumerate(ds['train']):\n",
    "    try:\n",
    "        # Parse JSON strings\n",
    "        questions = ast.literal_eval(item['question'])\n",
    "        contexts = ast.literal_eval(item['context'])\n",
    "        cids = ast.literal_eval(item['cid'])\n",
    "        \n",
    "        # X·ª≠ l√Ω t·ª´ng c·∫∑p question-context\n",
    "        if isinstance(questions, list) and isinstance(contexts, list):\n",
    "            # L·∫•y c·∫∑p ƒë·∫ßu ti√™n ho·∫∑c t·∫•t c·∫£ c·∫∑p\n",
    "            for j in range(min(len(questions), len(contexts))):\n",
    "                question_text = questions[j].strip() if questions[j] else \"\"\n",
    "                context_text = contexts[j].strip() if contexts[j] else \"\"\n",
    "                \n",
    "                if question_text and context_text:\n",
    "                    processed_data.append({\n",
    "                        'question': question_text,\n",
    "                        'answer': context_text,\n",
    "                        'qid': item['qid'],\n",
    "                        'cid': cids[j] if j < len(cids) else None\n",
    "                    })\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(f\"   Processed {i} samples...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        if errors < 5:  # Ch·ªâ in 5 l·ªói ƒë·∫ßu ti√™n\n",
    "            print(f\"   Error at sample {i}: {e}\")\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ x·ª≠ l√Ω {len(processed_data)} Q&A pairs t·ª´ {len(ds['train'])} samples\")\n",
    "print(f\"   C√≥ {errors} l·ªói trong qu√° tr√¨nh x·ª≠ l√Ω\")\n",
    "\n",
    "# Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu\n",
    "def analyze_processed_data(data):\n",
    "    \"\"\"Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω\"\"\"\n",
    "    questions = [item['question'] for item in data]\n",
    "    answers = [item['answer'] for item in data]\n",
    "    \n",
    "    print(f\"\\nüìà Ph√¢n t√≠ch dataset ƒë√£ x·ª≠ l√Ω:\")\n",
    "    \n",
    "    # ƒê·ªô d√†i c√¢u h·ªèi\n",
    "    question_lengths = [len(q) for q in questions]\n",
    "    print(f\"üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\")\n",
    "    print(f\"   - Trung b√¨nh: {np.mean(question_lengths):.1f} k√Ω t·ª±\")\n",
    "    print(f\"   - Min: {min(question_lengths)}, Max: {max(question_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(question_lengths):.1f}\")\n",
    "    \n",
    "    # ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi\n",
    "    answer_lengths = [len(a) for a in answers]\n",
    "    print(f\"üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\")\n",
    "    print(f\"   - Trung b√¨nh: {np.mean(answer_lengths):.1f} k√Ω t·ª±\")\n",
    "    print(f\"   - Min: {min(answer_lengths)}, Max: {max(answer_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(answer_lengths):.1f}\")\n",
    "    \n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = analyze_processed_data(processed_data)\n",
    "\n",
    "print(f\"\\nüìù M·∫´u d·ªØ li·ªáu ƒë√£ x·ª≠ l√Ω:\")\n",
    "sample = processed_data[0]\n",
    "print(f\"Question: {sample['question'][:150]}...\")\n",
    "print(f\"Answer: {sample['answer'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c709efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß L√†m s·∫°ch v√† l·ªçc d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng...\n",
      "‚úÖ D·ªØ li·ªáu ch·∫•t l∆∞·ª£ng: 71881 ‚Üí 52864 (gi·ªØ l·∫°i 73.5%)\n",
      "\n",
      "üìä Th·ªëng k√™ d·ªØ li·ªáu cu·ªëi c√πng:\n",
      "- T·ªïng s·ªë m·∫´u training: 52864\n",
      "- Trung b√¨nh ƒë·ªô d√†i c√¢u h·ªèi: 66.8 k√Ω t·ª±\n",
      "- Trung b√¨nh ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi: 86.3 k√Ω t·ª±\n",
      "‚úÖ D·ªØ li·ªáu ch·∫•t l∆∞·ª£ng: 71881 ‚Üí 52864 (gi·ªØ l·∫°i 73.5%)\n",
      "\n",
      "üìä Th·ªëng k√™ d·ªØ li·ªáu cu·ªëi c√πng:\n",
      "- T·ªïng s·ªë m·∫´u training: 52864\n",
      "- Trung b√¨nh ƒë·ªô d√†i c√¢u h·ªèi: 66.8 k√Ω t·ª±\n",
      "- Trung b√¨nh ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi: 86.3 k√Ω t·ª±\n"
     ]
    }
   ],
   "source": [
    "# L√†m s·∫°ch v√† l·ªçc d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def filter_quality_data(data, min_question_len=10, min_answer_len=20, max_answer_len=3000):\n",
    "    \"\"\"L·ªçc d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng\"\"\"\n",
    "    filtered_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        question = clean_text(item['question'])\n",
    "        answer = clean_text(item['answer'])\n",
    "        \n",
    "        # L·ªçc theo ti√™u ch√≠ ch·∫•t l∆∞·ª£ng\n",
    "        if (len(question) >= min_question_len and \n",
    "            len(answer) >= min_answer_len and \n",
    "            len(answer) <= max_answer_len and\n",
    "            question.endswith('?')):  # C√¢u h·ªèi ph·∫£i k·∫øt th√∫c b·∫±ng d·∫•u ?\n",
    "            \n",
    "            filtered_data.append({\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            })\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "print(\"üîß L√†m s·∫°ch v√† l·ªçc d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng...\")\n",
    "\n",
    "# L·ªçc d·ªØ li·ªáu\n",
    "quality_data = filter_quality_data(processed_data)\n",
    "\n",
    "print(f\"‚úÖ D·ªØ li·ªáu ch·∫•t l∆∞·ª£ng: {len(processed_data)} ‚Üí {len(quality_data)} (gi·ªØ l·∫°i {len(quality_data)/len(processed_data)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä Th·ªëng k√™ d·ªØ li·ªáu cu·ªëi c√πng:\")\n",
    "print(f\"- T·ªïng s·ªë m·∫´u training: {len(quality_data)}\")\n",
    "\n",
    "if quality_data:\n",
    "    avg_q_len = np.mean([len(item['question']) for item in quality_data])\n",
    "    avg_a_len = np.mean([len(item['answer']) for item in quality_data])\n",
    "    print(f\"- Trung b√¨nh ƒë·ªô d√†i c√¢u h·ªèi: {avg_q_len:.1f} k√Ω t·ª±\")\n",
    "    print(f\"- Trung b√¨nh ƒë·ªô d√†i c√¢u tr·∫£ l·ªùi: {avg_a_len:.1f} k√Ω t·ª±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1177fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ T·∫°o th∆∞ m·ª•c: ../data/finetune_data4\n",
      "üíæ ƒêang l∆∞u d·ªØ li·ªáu...\n",
      "‚úÖ ƒê√£ l∆∞u QA format: ../data/finetune_data4\\legal_vn_qa_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u QA format: ../data/finetune_data4\\legal_vn_qa_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u instruction format: ../data/finetune_data4\\legal_vn_instruction_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u instruction format: ../data/finetune_data4\\legal_vn_instruction_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u conversation format: ../data/finetune_data4\\legal_vn_conversation_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u conversation format: ../data/finetune_data4\\legal_vn_conversation_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o th∆∞ m·ª•c v√† l∆∞u d·ªØ li·ªáu v√†o data/finetune_data4\n",
    "import os\n",
    "import json\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c data/finetune_data4 (kh√°c v·ªõi c√°c dataset tr∆∞·ªõc)\n",
    "output_dir = \"../data/finetune_data4\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ T·∫°o th∆∞ m·ª•c: {output_dir}\")\n",
    "\n",
    "# L∆∞u d·ªØ li·ªáu d∆∞·ªõi nhi·ªÅu format kh√°c nhau\n",
    "\n",
    "# 1. Format JSONL (m·ªói d√≤ng l√† m·ªôt JSON object)\n",
    "def save_jsonl(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng JSONL\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 2. Format instruction (cho fine-tuning)\n",
    "def save_instruction_format(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng instruction tuning\"\"\"\n",
    "    instruction_data = []\n",
    "    for item in data:\n",
    "        instruction_item = {\n",
    "            \"instruction\": \"Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\",\n",
    "            \"input\": item['question'],\n",
    "            \"output\": item['answer']\n",
    "        }\n",
    "        instruction_data.append(instruction_item)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in instruction_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 3. Format conversation (cho chatbot training)\n",
    "def save_conversation_format(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng conversation\"\"\"\n",
    "    conversation_data = []\n",
    "    for item in data:\n",
    "        conversation_item = {\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"user\", \"content\": item['question']},\n",
    "                {\"role\": \"assistant\", \"content\": item['answer']}\n",
    "            ]\n",
    "        }\n",
    "        conversation_data.append(conversation_item)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in conversation_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "print(\"üíæ ƒêang l∆∞u d·ªØ li·ªáu...\")\n",
    "\n",
    "# L∆∞u QA format\n",
    "qa_path = os.path.join(output_dir, \"legal_vn_qa_format.jsonl\")\n",
    "save_jsonl(quality_data, qa_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u QA format: {qa_path}\")\n",
    "\n",
    "# L∆∞u instruction format  \n",
    "instruction_path = os.path.join(output_dir, \"legal_vn_instruction_format.jsonl\")\n",
    "save_instruction_format(quality_data, instruction_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u instruction format: {instruction_path}\")\n",
    "\n",
    "# L∆∞u conversation format\n",
    "conversation_path = os.path.join(output_dir, \"legal_vn_conversation_format.jsonl\")\n",
    "save_conversation_format(quality_data, conversation_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u conversation format: {conversation_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
