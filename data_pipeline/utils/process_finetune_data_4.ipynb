{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fa692255",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Zappu/Legal-vn\", \"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "871ddc73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'context', 'cid', 'qid'],\n",
       "        num_rows: 79456\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# PhÃ¢n tÃ­ch cáº¥u trÃºc dataset\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "484e59c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“Š ThÃ´ng tin dataset:\n",
      "- Train set: 79456 samples\n",
      "- Total: 79456 samples\n",
      "\n",
      "ğŸ“ CÃ¡c trÆ°á»ng cÃ³ sáºµn:\n",
      "- question\n",
      "- context\n",
      "- cid\n",
      "- qid\n",
      "\n",
      "ğŸ” Xem máº«u dá»¯ liá»‡u Ä‘áº§u tiÃªn:\n",
      "question: ['NgÆ°á»i há»c ngÃ nh quáº£n lÃ½ khai thÃ¡c cÃ´ng trÃ¬nh thá»§y lá»£i trÃ¬nh Ä‘á»™ cao Ä‘áº³ng kháº£ nÄƒng há»c táº­p nÃ¢ng trÃ¬nh Ä‘á»™ ?']\n",
      "context: ['Kháº£ nÄƒng há»c táº­p , nÃ¢ng trÃ¬nh Ä‘á»™ - Khá»‘i lÆ°á»£ng kiáº¿n thá»©c tá»‘i thiá»ƒu , nÄƒng lá»±c há»c tá»‘t nghiá»‡p ngÃ nh , nghá» quáº£n lÃ½ , khai thÃ¡c cÃ´ng trÃ¬nh thá»§y lá»£i , t...\n",
      "cid: [62492]\n",
      "qid: 161615\n"
     ]
    }
   ],
   "source": [
    "# PhÃ¢n tÃ­ch chi tiáº¿t dataset\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ“Š ThÃ´ng tin dataset:\")\n",
    "print(f\"- Train set: {len(ds['train'])} samples\")\n",
    "print(f\"- Total: {len(ds['train'])} samples\")\n",
    "\n",
    "print(\"\\nğŸ“ CÃ¡c trÆ°á»ng cÃ³ sáºµn:\")\n",
    "for feature in ds['train'].features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "print(\"\\nğŸ” Xem máº«u dá»¯ liá»‡u Ä‘áº§u tiÃªn:\")\n",
    "sample = ds['train'][0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, str) and len(value) > 150:\n",
    "        print(f\"{key}: {value[:150]}...\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "31a70f01",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Chuyá»ƒn Ä‘á»•i format dá»¯ liá»‡u...\n",
      "âœ… ÄÃ£ chuyá»ƒn Ä‘á»•i 79456 samples\n",
      "\n",
      "ğŸ“ˆ PhÃ¢n tÃ­ch dataset sau chuyá»ƒn Ä‘á»•i:\n",
      "ğŸ”¸ Äá»™ dÃ i cÃ¢u há»i:\n",
      "   - Trung bÃ¬nh: 1.0 kÃ½ tá»±\n",
      "   - Min: 1, Max: 1\n",
      "   - Median: 1.0\n",
      "ğŸ”¸ Äá»™ dÃ i cÃ¢u tráº£ lá»i:\n",
      "   - Trung bÃ¬nh: 1.0 kÃ½ tá»±\n",
      "   - Min: 1, Max: 1\n",
      "   - Median: 1.0\n",
      "ğŸ”¸ Dá»¯ liá»‡u rá»—ng:\n",
      "   - CÃ¢u há»i rá»—ng: 0\n",
      "   - CÃ¢u tráº£ lá»i rá»—ng: 0\n",
      "\n",
      "ğŸ“ Máº«u dá»¯ liá»‡u sau chuyá»ƒn Ä‘á»•i:\n",
      "Question: [...\n",
      "Answer: [...\n",
      "âœ… ÄÃ£ chuyá»ƒn Ä‘á»•i 79456 samples\n",
      "\n",
      "ğŸ“ˆ PhÃ¢n tÃ­ch dataset sau chuyá»ƒn Ä‘á»•i:\n",
      "ğŸ”¸ Äá»™ dÃ i cÃ¢u há»i:\n",
      "   - Trung bÃ¬nh: 1.0 kÃ½ tá»±\n",
      "   - Min: 1, Max: 1\n",
      "   - Median: 1.0\n",
      "ğŸ”¸ Äá»™ dÃ i cÃ¢u tráº£ lá»i:\n",
      "   - Trung bÃ¬nh: 1.0 kÃ½ tá»±\n",
      "   - Min: 1, Max: 1\n",
      "   - Median: 1.0\n",
      "ğŸ”¸ Dá»¯ liá»‡u rá»—ng:\n",
      "   - CÃ¢u há»i rá»—ng: 0\n",
      "   - CÃ¢u tráº£ lá»i rá»—ng: 0\n",
      "\n",
      "ğŸ“ Máº«u dá»¯ liá»‡u sau chuyá»ƒn Ä‘á»•i:\n",
      "Question: [...\n",
      "Answer: [...\n"
     ]
    }
   ],
   "source": [
    "# Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u tá»« format danh sÃ¡ch thÃ nh text Ä‘Æ¡n giáº£n\n",
    "print(\"ğŸ”§ Chuyá»ƒn Ä‘á»•i format dá»¯ liá»‡u...\")\n",
    "\n",
    "# Chuyá»ƒn Ä‘á»•i tá»« list thÃ nh string\n",
    "converted_data = []\n",
    "for item in ds['train']:\n",
    "    # Láº¥y text Ä‘áº§u tiÃªn tá»« danh sÃ¡ch question vÃ  context\n",
    "    question_text = item['question'][0] if item['question'] and len(item['question']) > 0 else \"\"\n",
    "    context_text = item['context'][0] if item['context'] and len(item['context']) > 0 else \"\"\n",
    "    \n",
    "    converted_data.append({\n",
    "        'Question': question_text,\n",
    "        'Answer': context_text,\n",
    "        'cid': item['cid'][0] if item['cid'] and len(item['cid']) > 0 else None,\n",
    "        'qid': item['qid']\n",
    "    })\n",
    "\n",
    "print(f\"âœ… ÄÃ£ chuyá»ƒn Ä‘á»•i {len(converted_data)} samples\")\n",
    "\n",
    "# PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng dá»¯ liá»‡u sau chuyá»ƒn Ä‘á»•i\n",
    "def analyze_text_quality(data):\n",
    "    \"\"\"PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng text trong dataset\"\"\"\n",
    "    questions = [item['Question'] for item in data]\n",
    "    answers = [item['Answer'] for item in data]\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ PhÃ¢n tÃ­ch dataset sau chuyá»ƒn Ä‘á»•i:\")\n",
    "    \n",
    "    # Äá»™ dÃ i cÃ¢u há»i\n",
    "    question_lengths = [len(q) if q else 0 for q in questions]\n",
    "    print(f\"ğŸ”¸ Äá»™ dÃ i cÃ¢u há»i:\")\n",
    "    print(f\"   - Trung bÃ¬nh: {np.mean(question_lengths):.1f} kÃ½ tá»±\")\n",
    "    print(f\"   - Min: {min(question_lengths)}, Max: {max(question_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(question_lengths):.1f}\")\n",
    "    \n",
    "    # Äá»™ dÃ i cÃ¢u tráº£ lá»i\n",
    "    answer_lengths = [len(a) if a else 0 for a in answers]\n",
    "    print(f\"ğŸ”¸ Äá»™ dÃ i cÃ¢u tráº£ lá»i:\")\n",
    "    print(f\"   - Trung bÃ¬nh: {np.mean(answer_lengths):.1f} kÃ½ tá»±\")\n",
    "    print(f\"   - Min: {min(answer_lengths)}, Max: {max(answer_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(answer_lengths):.1f}\")\n",
    "    \n",
    "    # Kiá»ƒm tra dá»¯ liá»‡u rá»—ng\n",
    "    empty_questions = sum(1 for q in questions if not q or q.strip() == \"\")\n",
    "    empty_answers = sum(1 for a in answers if not a or a.strip() == \"\")\n",
    "    \n",
    "    print(f\"ğŸ”¸ Dá»¯ liá»‡u rá»—ng:\")\n",
    "    print(f\"   - CÃ¢u há»i rá»—ng: {empty_questions}\")\n",
    "    print(f\"   - CÃ¢u tráº£ lá»i rá»—ng: {empty_answers}\")\n",
    "    \n",
    "    return {\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "        'question_lengths': question_lengths,\n",
    "        'answer_lengths': answer_lengths,\n",
    "        'empty_questions': empty_questions,\n",
    "        'empty_answers': empty_answers\n",
    "    }\n",
    "\n",
    "analysis = analyze_text_quality(converted_data)\n",
    "\n",
    "print(f\"\\nğŸ“ Máº«u dá»¯ liá»‡u sau chuyá»ƒn Ä‘á»•i:\")\n",
    "sample = converted_data[0]\n",
    "print(f\"Question: {sample['Question'][:100]}...\")\n",
    "print(f\"Answer: {sample['Answer'][:100]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "50997925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” Kiá»ƒm tra chi tiáº¿t cáº¥u trÃºc dá»¯ liá»‡u:\n",
      "\n",
      "Sample 1:\n",
      "  question type: <class 'str'>\n",
      "  question content: ['NgÆ°á»i há»c ngÃ nh quáº£n lÃ½ khai thÃ¡c cÃ´ng trÃ¬nh thá»§y lá»£i trÃ¬nh Ä‘á»™ cao Ä‘áº³ng kháº£ nÄƒng há»c táº­p nÃ¢ng trÃ¬nh Ä‘á»™ ?']\n",
      "  context type: <class 'str'>\n",
      "  context content: ['Kháº£ nÄƒng há»c táº­p , nÃ¢ng trÃ¬nh Ä‘á»™ - Khá»‘i lÆ°á»£ng kiáº¿n thá»©c tá»‘i thiá»ƒu , nÄƒng lá»±c há»c tá»‘t nghiá»‡p ngÃ nh \n",
      "  cid: [62492]\n",
      "  qid: 161615\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 2:\n",
      "  question type: <class 'str'>\n",
      "  question content: ['Ná»™i dung lá»“ng ghÃ©p bÃ¬nh Ä‘áº³ng giá»›i xÃ¢y dá»±ng vÄƒn báº£n quy pháº¡m phÃ¡p luáº­t quy Ä‘á»‹nh ?']\n",
      "  context type: <class 'str'>\n",
      "  context content: ['Ná»™i dung lá»“ng ghÃ©p bÃ¬nh Ä‘áº³ng giá»›i xÃ¢y dá»±ng vÄƒn báº£n quy pháº¡m phÃ¡p luáº­t Trong pháº¡m vi Ä‘iá»u chá»‰nh vÄƒn\n",
      "  cid: [151154]\n",
      "  qid: 80037\n",
      "--------------------------------------------------\n",
      "\n",
      "Sample 3:\n",
      "  question type: <class 'str'>\n",
      "  question content: ['Sáº£n pháº©m pháº§n má»m hÆ°á»Ÿng Æ°u Ä‘Ã£i miá»…n thuáº¿ , thuáº¿ ?', 'Náº¿u vÃ²ng ?']\n",
      "  context type: <class 'str'>\n",
      "  context content: ['Äiá»u 20 .', 'Æ¯u Ä‘Ã£i miá»…n thuáº¿ , thuáº¿ 1 .', 'Miá»…n thuáº¿ bá»‘n , 50 % thuáº¿ ná»™p chÃ­n : a ) Thu nháº­p doan\n",
      "  cid: [75071]\n",
      "  qid: 124074\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Kiá»ƒm tra láº¡i cáº¥u trÃºc dá»¯ liá»‡u chi tiáº¿t\n",
    "print(\"ğŸ” Kiá»ƒm tra chi tiáº¿t cáº¥u trÃºc dá»¯ liá»‡u:\")\n",
    "\n",
    "# Xem máº«u chi tiáº¿t hÆ¡n\n",
    "for i in range(3):\n",
    "    sample = ds['train'][i]\n",
    "    print(f\"\\nSample {i+1}:\")\n",
    "    print(f\"  question type: {type(sample['question'])}\")\n",
    "    print(f\"  question content: {sample['question']}\")\n",
    "    print(f\"  context type: {type(sample['context'])}\")\n",
    "    print(f\"  context content: {sample['context'][:100] if len(str(sample['context'])) > 100 else sample['context']}\")\n",
    "    print(f\"  cid: {sample['cid']}\")\n",
    "    print(f\"  qid: {sample['qid']}\")\n",
    "    print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5d88332",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Xá»­ lÃ½ dá»¯ liá»‡u Ä‘Ãºng cÃ¡ch...\n",
      "   Processed 0 samples...\n",
      "   Error at sample 7: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "   Error at sample 28: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "   Error at sample 53: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "   Error at sample 58: invalid syntax. Perhaps you forgot a comma? (<unknown>, line 1)\n",
      "   Processed 10000 samples...\n",
      "   Processed 10000 samples...\n",
      "   Processed 20000 samples...\n",
      "   Processed 20000 samples...\n",
      "   Processed 30000 samples...\n",
      "   Processed 30000 samples...\n",
      "   Processed 40000 samples...\n",
      "   Processed 40000 samples...\n",
      "   Processed 60000 samples...\n",
      "   Processed 60000 samples...\n",
      "   Processed 70000 samples...\n",
      "   Processed 70000 samples...\n",
      "âœ… ÄÃ£ xá»­ lÃ½ 71881 Q&A pairs tá»« 79456 samples\n",
      "   CÃ³ 8413 lá»—i trong quÃ¡ trÃ¬nh xá»­ lÃ½\n",
      "\n",
      "ğŸ“ˆ PhÃ¢n tÃ­ch dataset Ä‘Ã£ xá»­ lÃ½:\n",
      "ğŸ”¸ Äá»™ dÃ i cÃ¢u há»i:\n",
      "   - Trung bÃ¬nh: 63.8 kÃ½ tá»±\n",
      "   - Min: 1, Max: 191\n",
      "   - Median: 61.0\n",
      "ğŸ”¸ Äá»™ dÃ i cÃ¢u tráº£ lá»i:\n",
      "   - Trung bÃ¬nh: 66.2 kÃ½ tá»±\n",
      "   - Min: 2, Max: 2915\n",
      "   - Median: 38.0\n",
      "\n",
      "ğŸ“ Máº«u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½:\n",
      "Question: NgÆ°á»i há»c ngÃ nh quáº£n lÃ½ khai thÃ¡c cÃ´ng trÃ¬nh thá»§y lá»£i trÃ¬nh Ä‘á»™ cao Ä‘áº³ng kháº£ nÄƒng há»c táº­p nÃ¢ng trÃ¬nh Ä‘á»™ ?...\n",
      "Answer: Kháº£ nÄƒng há»c táº­p , nÃ¢ng trÃ¬nh Ä‘á»™ - Khá»‘i lÆ°á»£ng kiáº¿n thá»©c tá»‘i thiá»ƒu , nÄƒng lá»±c há»c tá»‘t nghiá»‡p ngÃ nh , nghá» quáº£n lÃ½ , khai thÃ¡c cÃ´ng trÃ¬nh thá»§y lá»£i , trÃ¬...\n",
      "âœ… ÄÃ£ xá»­ lÃ½ 71881 Q&A pairs tá»« 79456 samples\n",
      "   CÃ³ 8413 lá»—i trong quÃ¡ trÃ¬nh xá»­ lÃ½\n",
      "\n",
      "ğŸ“ˆ PhÃ¢n tÃ­ch dataset Ä‘Ã£ xá»­ lÃ½:\n",
      "ğŸ”¸ Äá»™ dÃ i cÃ¢u há»i:\n",
      "   - Trung bÃ¬nh: 63.8 kÃ½ tá»±\n",
      "   - Min: 1, Max: 191\n",
      "   - Median: 61.0\n",
      "ğŸ”¸ Äá»™ dÃ i cÃ¢u tráº£ lá»i:\n",
      "   - Trung bÃ¬nh: 66.2 kÃ½ tá»±\n",
      "   - Min: 2, Max: 2915\n",
      "   - Median: 38.0\n",
      "\n",
      "ğŸ“ Máº«u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½:\n",
      "Question: NgÆ°á»i há»c ngÃ nh quáº£n lÃ½ khai thÃ¡c cÃ´ng trÃ¬nh thá»§y lá»£i trÃ¬nh Ä‘á»™ cao Ä‘áº³ng kháº£ nÄƒng há»c táº­p nÃ¢ng trÃ¬nh Ä‘á»™ ?...\n",
      "Answer: Kháº£ nÄƒng há»c táº­p , nÃ¢ng trÃ¬nh Ä‘á»™ - Khá»‘i lÆ°á»£ng kiáº¿n thá»©c tá»‘i thiá»ƒu , nÄƒng lá»±c há»c tá»‘t nghiá»‡p ngÃ nh , nghá» quáº£n lÃ½ , khai thÃ¡c cÃ´ng trÃ¬nh thá»§y lá»£i , trÃ¬...\n"
     ]
    }
   ],
   "source": [
    "# Xá»­ lÃ½ dá»¯ liá»‡u Ä‘Ãºng cÃ¡ch - parse JSON strings\n",
    "import ast\n",
    "import json\n",
    "\n",
    "print(\"ğŸ”§ Xá»­ lÃ½ dá»¯ liá»‡u Ä‘Ãºng cÃ¡ch...\")\n",
    "\n",
    "# Chuyá»ƒn Ä‘á»•i tá»« JSON strings thÃ nh dá»¯ liá»‡u thá»±c\n",
    "processed_data = []\n",
    "errors = 0\n",
    "\n",
    "for i, item in enumerate(ds['train']):\n",
    "    try:\n",
    "        # Parse JSON strings\n",
    "        questions = ast.literal_eval(item['question'])\n",
    "        contexts = ast.literal_eval(item['context'])\n",
    "        cids = ast.literal_eval(item['cid'])\n",
    "        \n",
    "        # Xá»­ lÃ½ tá»«ng cáº·p question-context\n",
    "        if isinstance(questions, list) and isinstance(contexts, list):\n",
    "            # Láº¥y cáº·p Ä‘áº§u tiÃªn hoáº·c táº¥t cáº£ cáº·p\n",
    "            for j in range(min(len(questions), len(contexts))):\n",
    "                question_text = questions[j].strip() if questions[j] else \"\"\n",
    "                context_text = contexts[j].strip() if contexts[j] else \"\"\n",
    "                \n",
    "                if question_text and context_text:\n",
    "                    processed_data.append({\n",
    "                        'question': question_text,\n",
    "                        'answer': context_text,\n",
    "                        'qid': item['qid'],\n",
    "                        'cid': cids[j] if j < len(cids) else None\n",
    "                    })\n",
    "        \n",
    "        if i % 10000 == 0:\n",
    "            print(f\"   Processed {i} samples...\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        errors += 1\n",
    "        if errors < 5:  # Chá»‰ in 5 lá»—i Ä‘áº§u tiÃªn\n",
    "            print(f\"   Error at sample {i}: {e}\")\n",
    "\n",
    "print(f\"âœ… ÄÃ£ xá»­ lÃ½ {len(processed_data)} Q&A pairs tá»« {len(ds['train'])} samples\")\n",
    "print(f\"   CÃ³ {errors} lá»—i trong quÃ¡ trÃ¬nh xá»­ lÃ½\")\n",
    "\n",
    "# PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng dá»¯ liá»‡u\n",
    "def analyze_processed_data(data):\n",
    "    \"\"\"PhÃ¢n tÃ­ch cháº¥t lÆ°á»£ng dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½\"\"\"\n",
    "    questions = [item['question'] for item in data]\n",
    "    answers = [item['answer'] for item in data]\n",
    "    \n",
    "    print(f\"\\nğŸ“ˆ PhÃ¢n tÃ­ch dataset Ä‘Ã£ xá»­ lÃ½:\")\n",
    "    \n",
    "    # Äá»™ dÃ i cÃ¢u há»i\n",
    "    question_lengths = [len(q) for q in questions]\n",
    "    print(f\"ğŸ”¸ Äá»™ dÃ i cÃ¢u há»i:\")\n",
    "    print(f\"   - Trung bÃ¬nh: {np.mean(question_lengths):.1f} kÃ½ tá»±\")\n",
    "    print(f\"   - Min: {min(question_lengths)}, Max: {max(question_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(question_lengths):.1f}\")\n",
    "    \n",
    "    # Äá»™ dÃ i cÃ¢u tráº£ lá»i\n",
    "    answer_lengths = [len(a) for a in answers]\n",
    "    print(f\"ğŸ”¸ Äá»™ dÃ i cÃ¢u tráº£ lá»i:\")\n",
    "    print(f\"   - Trung bÃ¬nh: {np.mean(answer_lengths):.1f} kÃ½ tá»±\")\n",
    "    print(f\"   - Min: {min(answer_lengths)}, Max: {max(answer_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(answer_lengths):.1f}\")\n",
    "    \n",
    "    return questions, answers\n",
    "\n",
    "questions, answers = analyze_processed_data(processed_data)\n",
    "\n",
    "print(f\"\\nğŸ“ Máº«u dá»¯ liá»‡u Ä‘Ã£ xá»­ lÃ½:\")\n",
    "sample = processed_data[0]\n",
    "print(f\"Question: {sample['question'][:150]}...\")\n",
    "print(f\"Answer: {sample['answer'][:150]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c709efcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ LÃ m sáº¡ch vÃ  lá»c dá»¯ liá»‡u cháº¥t lÆ°á»£ng...\n",
      "âœ… Dá»¯ liá»‡u cháº¥t lÆ°á»£ng: 71881 â†’ 52864 (giá»¯ láº¡i 73.5%)\n",
      "\n",
      "ğŸ“Š Thá»‘ng kÃª dá»¯ liá»‡u cuá»‘i cÃ¹ng:\n",
      "- Tá»•ng sá»‘ máº«u training: 52864\n",
      "- Trung bÃ¬nh Ä‘á»™ dÃ i cÃ¢u há»i: 66.8 kÃ½ tá»±\n",
      "- Trung bÃ¬nh Ä‘á»™ dÃ i cÃ¢u tráº£ lá»i: 86.3 kÃ½ tá»±\n",
      "âœ… Dá»¯ liá»‡u cháº¥t lÆ°á»£ng: 71881 â†’ 52864 (giá»¯ láº¡i 73.5%)\n",
      "\n",
      "ğŸ“Š Thá»‘ng kÃª dá»¯ liá»‡u cuá»‘i cÃ¹ng:\n",
      "- Tá»•ng sá»‘ máº«u training: 52864\n",
      "- Trung bÃ¬nh Ä‘á»™ dÃ i cÃ¢u há»i: 66.8 kÃ½ tá»±\n",
      "- Trung bÃ¬nh Ä‘á»™ dÃ i cÃ¢u tráº£ lá»i: 86.3 kÃ½ tá»±\n"
     ]
    }
   ],
   "source": [
    "# LÃ m sáº¡ch vÃ  lá»c dá»¯ liá»‡u cháº¥t lÆ°á»£ng\n",
    "def clean_text(text):\n",
    "    \"\"\"LÃ m sáº¡ch text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Loáº¡i bá» khoáº£ng tráº¯ng thá»«a\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Loáº¡i bá» kÃ½ tá»± Ä‘áº·c biá»‡t khÃ´ng cáº§n thiáº¿t\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def filter_quality_data(data, min_question_len=10, min_answer_len=20, max_answer_len=3000):\n",
    "    \"\"\"Lá»c dá»¯ liá»‡u cháº¥t lÆ°á»£ng\"\"\"\n",
    "    filtered_data = []\n",
    "    \n",
    "    for item in data:\n",
    "        question = clean_text(item['question'])\n",
    "        answer = clean_text(item['answer'])\n",
    "        \n",
    "        # Lá»c theo tiÃªu chÃ­ cháº¥t lÆ°á»£ng\n",
    "        if (len(question) >= min_question_len and \n",
    "            len(answer) >= min_answer_len and \n",
    "            len(answer) <= max_answer_len and\n",
    "            question.endswith('?')):  # CÃ¢u há»i pháº£i káº¿t thÃºc báº±ng dáº¥u ?\n",
    "            \n",
    "            filtered_data.append({\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            })\n",
    "    \n",
    "    return filtered_data\n",
    "\n",
    "print(\"ğŸ”§ LÃ m sáº¡ch vÃ  lá»c dá»¯ liá»‡u cháº¥t lÆ°á»£ng...\")\n",
    "\n",
    "# Lá»c dá»¯ liá»‡u\n",
    "quality_data = filter_quality_data(processed_data)\n",
    "\n",
    "print(f\"âœ… Dá»¯ liá»‡u cháº¥t lÆ°á»£ng: {len(processed_data)} â†’ {len(quality_data)} (giá»¯ láº¡i {len(quality_data)/len(processed_data)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nğŸ“Š Thá»‘ng kÃª dá»¯ liá»‡u cuá»‘i cÃ¹ng:\")\n",
    "print(f\"- Tá»•ng sá»‘ máº«u training: {len(quality_data)}\")\n",
    "\n",
    "if quality_data:\n",
    "    avg_q_len = np.mean([len(item['question']) for item in quality_data])\n",
    "    avg_a_len = np.mean([len(item['answer']) for item in quality_data])\n",
    "    print(f\"- Trung bÃ¬nh Ä‘á»™ dÃ i cÃ¢u há»i: {avg_q_len:.1f} kÃ½ tá»±\")\n",
    "    print(f\"- Trung bÃ¬nh Ä‘á»™ dÃ i cÃ¢u tráº£ lá»i: {avg_a_len:.1f} kÃ½ tá»±\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a1177fca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Táº¡o thÆ° má»¥c: ../data/finetune_data4\n",
      "ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u...\n",
      "âœ… ÄÃ£ lÆ°u QA format: ../data/finetune_data4\\legal_vn_qa_format.jsonl\n",
      "âœ… ÄÃ£ lÆ°u QA format: ../data/finetune_data4\\legal_vn_qa_format.jsonl\n",
      "âœ… ÄÃ£ lÆ°u instruction format: ../data/finetune_data4\\legal_vn_instruction_format.jsonl\n",
      "âœ… ÄÃ£ lÆ°u instruction format: ../data/finetune_data4\\legal_vn_instruction_format.jsonl\n",
      "âœ… ÄÃ£ lÆ°u conversation format: ../data/finetune_data4\\legal_vn_conversation_format.jsonl\n",
      "âœ… ÄÃ£ lÆ°u conversation format: ../data/finetune_data4\\legal_vn_conversation_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "# Táº¡o thÆ° má»¥c vÃ  lÆ°u dá»¯ liá»‡u vÃ o data/finetune_data4\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Táº¡o thÆ° má»¥c data/finetune_data4 (khÃ¡c vá»›i cÃ¡c dataset trÆ°á»›c)\n",
    "output_dir = \"../data/finetune_data4\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"ğŸ“ Táº¡o thÆ° má»¥c: {output_dir}\")\n",
    "\n",
    "# LÆ°u dá»¯ liá»‡u dÆ°á»›i nhiá»u format khÃ¡c nhau\n",
    "\n",
    "# 1. Format JSONL (má»—i dÃ²ng lÃ  má»™t JSON object)\n",
    "def save_jsonl(data, filepath):\n",
    "    \"\"\"LÆ°u dá»¯ liá»‡u dÆ°á»›i Ä‘á»‹nh dáº¡ng JSONL\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 2. Format instruction (cho fine-tuning)\n",
    "def save_instruction_format(data, filepath):\n",
    "    \"\"\"LÆ°u dá»¯ liá»‡u dÆ°á»›i Ä‘á»‹nh dáº¡ng instruction tuning\"\"\"\n",
    "    instruction_data = []\n",
    "    for item in data:\n",
    "        instruction_item = {\n",
    "            \"instruction\": \"Tráº£ lá»i cÃ¢u há»i phÃ¡p luáº­t sau:\",\n",
    "            \"input\": item['question'],\n",
    "            \"output\": item['answer']\n",
    "        }\n",
    "        instruction_data.append(instruction_item)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in instruction_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 3. Format conversation (cho chatbot training)\n",
    "def save_conversation_format(data, filepath):\n",
    "    \"\"\"LÆ°u dá»¯ liá»‡u dÆ°á»›i Ä‘á»‹nh dáº¡ng conversation\"\"\"\n",
    "    conversation_data = []\n",
    "    for item in data:\n",
    "        conversation_item = {\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"user\", \"content\": item['question']},\n",
    "                {\"role\": \"assistant\", \"content\": item['answer']}\n",
    "            ]\n",
    "        }\n",
    "        conversation_data.append(conversation_item)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in conversation_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "print(\"ğŸ’¾ Äang lÆ°u dá»¯ liá»‡u...\")\n",
    "\n",
    "# LÆ°u QA format\n",
    "qa_path = os.path.join(output_dir, \"legal_vn_qa_format.jsonl\")\n",
    "save_jsonl(quality_data, qa_path)\n",
    "print(f\"âœ… ÄÃ£ lÆ°u QA format: {qa_path}\")\n",
    "\n",
    "# LÆ°u instruction format  \n",
    "instruction_path = os.path.join(output_dir, \"legal_vn_instruction_format.jsonl\")\n",
    "save_instruction_format(quality_data, instruction_path)\n",
    "print(f\"âœ… ÄÃ£ lÆ°u instruction format: {instruction_path}\")\n",
    "\n",
    "# LÆ°u conversation format\n",
    "conversation_path = os.path.join(output_dir, \"legal_vn_conversation_format.jsonl\")\n",
    "save_conversation_format(quality_data, conversation_path)\n",
    "print(f\"âœ… ÄÃ£ lÆ°u conversation format: {conversation_path}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
