{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "86aa3ee1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: datasets in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (4.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.20.0)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=21.0.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (22.0.0)\n",
      "Requirement already satisfied: dill<0.4.1,>=0.3.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.4.0)\n",
      "Requirement already satisfied: pandas in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.3.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2.32.5)\n",
      "Requirement already satisfied: httpx<1.0.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Requirement already satisfied: xxhash in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (3.6.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec[http]<=2025.9.0,>=2023.1.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (2025.9.0)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.25.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (1.0.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from datasets) (6.0.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (3.13.1)\n",
      "Requirement already satisfied: anyio in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: certifi in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0->datasets) (2024.2.2)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0->datasets) (1.0.9)\n",
      "Requirement already satisfied: idna in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpx<1.0.0->datasets) (3.6)\n",
      "Requirement already satisfied: h11>=0.16 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from httpcore==1.*->httpx<1.0.0->datasets) (0.16.0)\n",
      "Requirement already satisfied: shellingham in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.5.4)\n",
      "Requirement already satisfied: typer-slim in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (4.10.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.2.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from huggingface-hub<2.0,>=0.25.0->datasets) (1.2.0)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests>=2.32.2->datasets) (2.2.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pandas->datasets) (2025.2)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.5.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.4.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.4.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.8.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (6.7.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (0.4.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.9.0,>=2023.1.0->datasets) (1.22.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: sniffio>=1.1 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from anyio->httpx<1.0.0->datasets) (1.3.1)\n",
      "Requirement already satisfied: click>=8.0.0 in c:\\users\\nguye\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from typer-slim->huggingface-hub<2.0,>=0.25.0->datasets) (8.1.7)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 23.2.1 -> 25.3\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d595d232",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "# Login using e.g. `huggingface-cli login` to access this dataset\n",
    "ds = load_dataset(\"phuocsang/hoidap-tvpl-20k\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f6bf7b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'question', 'answer', 'url', 'keyword', 'time_published', 'date_published', 'type', 'author', '__index_level_0__'],\n",
       "        num_rows: 19965\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'question', 'answer', 'url', 'keyword', 'time_published', 'date_published', 'type', 'author', '__index_level_0__'],\n",
       "        num_rows: 2035\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "44e28110",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Th√¥ng tin dataset:\n",
      "- Train set: 19965 samples\n",
      "- Test set: 2035 samples\n",
      "- Total: 22000 samples\n",
      "\n",
      "üìù C√°c tr∆∞·ªùng c√≥ s·∫µn:\n",
      "- id\n",
      "- question\n",
      "- answer\n",
      "- url\n",
      "- keyword\n",
      "- time_published\n",
      "- date_published\n",
      "- type\n",
      "- author\n",
      "- __index_level_0__\n",
      "\n",
      "üîç Xem m·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:\n",
      "id: 35a9fd5e-b75e-42ff-a268-bb0a895251bf\n",
      "question: Trong B·ªô lu·∫≠t H√¨nh s·ª± th√¨ bao nhi√™u tu·ªïi ƒë∆∞·ª£c xem l√† ng∆∞·ªùi gi√†, ng∆∞·ªùi cao tu·ªïi?\n",
      "answer: Ng∆∞·ªùi cao tu·ªïi, ng∆∞·ªùi gi√†, ng∆∞·ªùi gi√† y·∫øu ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 2 Lu·∫≠t Ng∆∞·ªùi cao tu·ªïi 2009: \"Ng∆∞·ªùi c...\n",
      "url: https://thuvienphapluat.vn/phap-luat/nguoi-71-tuoi-pham-toi-phai-chiu-trach-nhiem-hinh-su-thi-co-duo...\n",
      "keyword: ['Ng∆∞·ªùi cao tu·ªïi', 'Tr√°ch nhi·ªám h√¨nh s·ª±']\n",
      "time_published: 06/05/2022\n",
      "date_published: 14:02\n",
      "type: thu-tuc-to-tung\n",
      "author: Nguy·ªÖn Kh√°nh Huy·ªÅn\n",
      "__index_level_0__: 349345\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch c·∫•u tr√∫c d·ªØ li·ªáu chi ti·∫øt\n",
    "import pandas as pd\n",
    "\n",
    "print(\"üìä Th√¥ng tin dataset:\")\n",
    "print(f\"- Train set: {len(ds['train'])} samples\")\n",
    "print(f\"- Test set: {len(ds['test'])} samples\")\n",
    "print(f\"- Total: {len(ds['train']) + len(ds['test'])} samples\")\n",
    "\n",
    "print(\"\\nüìù C√°c tr∆∞·ªùng c√≥ s·∫µn:\")\n",
    "for feature in ds['train'].features:\n",
    "    print(f\"- {feature}\")\n",
    "\n",
    "print(\"\\nüîç Xem m·∫´u d·ªØ li·ªáu ƒë·∫ßu ti√™n:\")\n",
    "sample = ds['train'][0]\n",
    "for key, value in sample.items():\n",
    "    if isinstance(value, str) and len(value) > 100:\n",
    "        print(f\"{key}: {value[:100]}...\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b609be33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üìà Ph√¢n t√≠ch Train Set:\n",
      "üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\n",
      "   - Trung b√¨nh: 88.7 k√Ω t·ª±\n",
      "   - Min: 2, Max: 373\n",
      "   - Median: 87.0\n",
      "üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\n",
      "   - Trung b√¨nh: 1792.2 k√Ω t·ª±\n",
      "   - Min: 55, Max: 765892\n",
      "   - Median: 1497.0\n",
      "üî∏ D·ªØ li·ªáu r·ªóng:\n",
      "   - C√¢u h·ªèi r·ªóng: 0\n",
      "   - C√¢u tr·∫£ l·ªùi r·ªóng: 0\n",
      "\n",
      "üìà Ph√¢n t√≠ch Test Set:\n",
      "üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\n",
      "   - Trung b√¨nh: 89.7 k√Ω t·ª±\n",
      "   - Min: 9, Max: 218\n",
      "   - Median: 88.0\n",
      "üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\n",
      "   - Trung b√¨nh: 1758.9 k√Ω t·ª±\n",
      "   - Min: 132, Max: 238002\n",
      "   - Median: 1499.0\n",
      "üî∏ D·ªØ li·ªáu r·ªóng:\n",
      "   - C√¢u h·ªèi r·ªóng: 0\n",
      "   - C√¢u tr·∫£ l·ªùi r·ªóng: 0\n"
     ]
    }
   ],
   "source": [
    "# Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng d·ªØ li·ªáu question v√† answer\n",
    "import numpy as np\n",
    "\n",
    "def analyze_text_quality(dataset_split, split_name):\n",
    "    \"\"\"Ph√¢n t√≠ch ch·∫•t l∆∞·ª£ng text trong dataset\"\"\"\n",
    "    questions = [item['question'] for item in dataset_split]\n",
    "    answers = [item['answer'] for item in dataset_split]\n",
    "    \n",
    "    print(f\"\\nüìà Ph√¢n t√≠ch {split_name}:\")\n",
    "    \n",
    "    # ƒê·ªô d√†i c√¢u h·ªèi\n",
    "    question_lengths = [len(q) if q else 0 for q in questions]\n",
    "    print(f\"üî∏ ƒê·ªô d√†i c√¢u h·ªèi:\")\n",
    "    print(f\"   - Trung b√¨nh: {np.mean(question_lengths):.1f} k√Ω t·ª±\")\n",
    "    print(f\"   - Min: {min(question_lengths)}, Max: {max(question_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(question_lengths):.1f}\")\n",
    "    \n",
    "    # ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi\n",
    "    answer_lengths = [len(a) if a else 0 for a in answers]\n",
    "    print(f\"üî∏ ƒê·ªô d√†i c√¢u tr·∫£ l·ªùi:\")\n",
    "    print(f\"   - Trung b√¨nh: {np.mean(answer_lengths):.1f} k√Ω t·ª±\")\n",
    "    print(f\"   - Min: {min(answer_lengths)}, Max: {max(answer_lengths)}\")\n",
    "    print(f\"   - Median: {np.median(answer_lengths):.1f}\")\n",
    "    \n",
    "    # Ki·ªÉm tra d·ªØ li·ªáu r·ªóng\n",
    "    empty_questions = sum(1 for q in questions if not q or q.strip() == \"\")\n",
    "    empty_answers = sum(1 for a in answers if not a or a.strip() == \"\")\n",
    "    \n",
    "    print(f\"üî∏ D·ªØ li·ªáu r·ªóng:\")\n",
    "    print(f\"   - C√¢u h·ªèi r·ªóng: {empty_questions}\")\n",
    "    print(f\"   - C√¢u tr·∫£ l·ªùi r·ªóng: {empty_answers}\")\n",
    "    \n",
    "    return {\n",
    "        'questions': questions,\n",
    "        'answers': answers,\n",
    "        'question_lengths': question_lengths,\n",
    "        'answer_lengths': answer_lengths,\n",
    "        'empty_questions': empty_questions,\n",
    "        'empty_answers': empty_answers\n",
    "    }\n",
    "\n",
    "# Ph√¢n t√≠ch train set\n",
    "train_analysis = analyze_text_quality(ds['train'], \"Train Set\")\n",
    "\n",
    "# Ph√¢n t√≠ch test set  \n",
    "test_analysis = analyze_text_quality(ds['test'], \"Test Set\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6203cac6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß ƒêang x·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu...\n",
      "‚úÖ Train set: 19965 ‚Üí 19536 (gi·ªØ l·∫°i 97.9%)\n",
      "‚úÖ Test set: 2035 ‚Üí 1993 (gi·ªØ l·∫°i 97.9%)\n",
      "\n",
      "üìä T·ªïng k·∫øt sau x·ª≠ l√Ω:\n",
      "- T·ªïng s·ªë m·∫´u: 21529\n",
      "- Train: 19536 m·∫´u\n",
      "- Test: 1993 m·∫´u\n",
      "‚úÖ Train set: 19965 ‚Üí 19536 (gi·ªØ l·∫°i 97.9%)\n",
      "‚úÖ Test set: 2035 ‚Üí 1993 (gi·ªØ l·∫°i 97.9%)\n",
      "\n",
      "üìä T·ªïng k·∫øt sau x·ª≠ l√Ω:\n",
      "- T·ªïng s·ªë m·∫´u: 21529\n",
      "- Train: 19536 m·∫´u\n",
      "- Test: 1993 m·∫´u\n"
     ]
    }
   ],
   "source": [
    "# L√†m s·∫°ch v√† chu·∫©n h√≥a d·ªØ li·ªáu\n",
    "def clean_text(text):\n",
    "    \"\"\"L√†m s·∫°ch text\"\"\"\n",
    "    if not text:\n",
    "        return \"\"\n",
    "    \n",
    "    # Lo·∫°i b·ªè kho·∫£ng tr·∫Øng th·ª´a\n",
    "    text = \" \".join(text.split())\n",
    "    \n",
    "    # Lo·∫°i b·ªè k√Ω t·ª± ƒë·∫∑c bi·ªát kh√¥ng c·∫ßn thi·∫øt\n",
    "    text = text.replace(\"\\n\", \" \").replace(\"\\r\", \" \").replace(\"\\t\", \" \")\n",
    "    \n",
    "    return text.strip()\n",
    "\n",
    "def process_dataset(dataset_split, max_answer_length=5000):\n",
    "    \"\"\"X·ª≠ l√Ω dataset v√† l·ªçc d·ªØ li·ªáu ch·∫•t l∆∞·ª£ng\"\"\"\n",
    "    processed_data = []\n",
    "    \n",
    "    for item in dataset_split:\n",
    "        question = clean_text(item['question'])\n",
    "        answer = clean_text(item['answer'])\n",
    "        \n",
    "        # L·ªçc d·ªØ li·ªáu theo ti√™u ch√≠ ch·∫•t l∆∞·ª£ng\n",
    "        if (len(question) >= 10 and  # C√¢u h·ªèi √≠t nh·∫•t 10 k√Ω t·ª±\n",
    "            len(answer) >= 50 and    # C√¢u tr·∫£ l·ªùi √≠t nh·∫•t 50 k√Ω t·ª±\n",
    "            len(answer) <= max_answer_length and  # Gi·ªõi h·∫°n ƒë·ªô d√†i tr·∫£ l·ªùi\n",
    "            question.endswith('?')):  # C√¢u h·ªèi ph·∫£i k·∫øt th√∫c b·∫±ng d·∫•u ?\n",
    "            \n",
    "            processed_data.append({\n",
    "                'question': question,\n",
    "                'answer': answer\n",
    "            })\n",
    "    \n",
    "    return processed_data\n",
    "\n",
    "print(\"üîß ƒêang x·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu...\")\n",
    "\n",
    "# X·ª≠ l√Ω train set\n",
    "train_processed = process_dataset(ds['train'])\n",
    "print(f\"‚úÖ Train set: {len(ds['train'])} ‚Üí {len(train_processed)} (gi·ªØ l·∫°i {len(train_processed)/len(ds['train'])*100:.1f}%)\")\n",
    "\n",
    "# X·ª≠ l√Ω test set\n",
    "test_processed = process_dataset(ds['test'])\n",
    "print(f\"‚úÖ Test set: {len(ds['test'])} ‚Üí {len(test_processed)} (gi·ªØ l·∫°i {len(test_processed)/len(ds['test'])*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüìä T·ªïng k·∫øt sau x·ª≠ l√Ω:\")\n",
    "print(f\"- T·ªïng s·ªë m·∫´u: {len(train_processed) + len(test_processed)}\")\n",
    "print(f\"- Train: {len(train_processed)} m·∫´u\")\n",
    "print(f\"- Test: {len(test_processed)} m·∫´u\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "97f742bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìÅ T·∫°o th∆∞ m·ª•c: ../data/finetune_data\n",
      "üíæ ƒêang l∆∞u d·ªØ li·ªáu...\n",
      "‚úÖ ƒê√£ l∆∞u train QA format: ../data/finetune_data\\train_qa_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u train QA format: ../data/finetune_data\\train_qa_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u train instruction format: ../data/finetune_data\\train_instruction_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u train instruction format: ../data/finetune_data\\train_instruction_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u train conversation format: ../data/finetune_data\\train_conversation_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u test QA format: ../data/finetune_data\\test_qa_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u test instruction format: ../data/finetune_data\\test_instruction_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u test conversation format: ../data/finetune_data\\test_conversation_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u train conversation format: ../data/finetune_data\\train_conversation_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u test QA format: ../data/finetune_data\\test_qa_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u test instruction format: ../data/finetune_data\\test_instruction_format.jsonl\n",
      "‚úÖ ƒê√£ l∆∞u test conversation format: ../data/finetune_data\\test_conversation_format.jsonl\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o th∆∞ m·ª•c v√† l∆∞u d·ªØ li·ªáu\n",
    "import os\n",
    "import json\n",
    "\n",
    "# T·∫°o th∆∞ m·ª•c data/finetune_data\n",
    "output_dir = \"../data/finetune_data\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "print(f\"üìÅ T·∫°o th∆∞ m·ª•c: {output_dir}\")\n",
    "\n",
    "# L∆∞u d·ªØ li·ªáu d∆∞·ªõi nhi·ªÅu format kh√°c nhau\n",
    "\n",
    "# 1. Format JSONL (m·ªói d√≤ng l√† m·ªôt JSON object)\n",
    "def save_jsonl(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng JSONL\"\"\"\n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 2. Format instruction (cho fine-tuning)\n",
    "def save_instruction_format(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng instruction tuning\"\"\"\n",
    "    instruction_data = []\n",
    "    for item in data:\n",
    "        instruction_item = {\n",
    "            \"instruction\": \"Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\",\n",
    "            \"input\": item['question'],\n",
    "            \"output\": item['answer']\n",
    "        }\n",
    "        instruction_data.append(instruction_item)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in instruction_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "# 3. Format conversation (cho chatbot training)\n",
    "def save_conversation_format(data, filepath):\n",
    "    \"\"\"L∆∞u d·ªØ li·ªáu d∆∞·ªõi ƒë·ªãnh d·∫°ng conversation\"\"\"\n",
    "    conversation_data = []\n",
    "    for item in data:\n",
    "        conversation_item = {\n",
    "            \"conversations\": [\n",
    "                {\"role\": \"user\", \"content\": item['question']},\n",
    "                {\"role\": \"assistant\", \"content\": item['answer']}\n",
    "            ]\n",
    "        }\n",
    "        conversation_data.append(conversation_item)\n",
    "    \n",
    "    with open(filepath, 'w', encoding='utf-8') as f:\n",
    "        for item in conversation_data:\n",
    "            json.dump(item, f, ensure_ascii=False)\n",
    "            f.write('\\n')\n",
    "\n",
    "print(\"üíæ ƒêang l∆∞u d·ªØ li·ªáu...\")\n",
    "\n",
    "# L∆∞u train set\n",
    "train_files = {\n",
    "    'qa_format.jsonl': train_processed,\n",
    "    'instruction_format.jsonl': None,  # Will be handled by save_instruction_format\n",
    "    'conversation_format.jsonl': None  # Will be handled by save_conversation_format\n",
    "}\n",
    "\n",
    "# L∆∞u QA format\n",
    "train_qa_path = os.path.join(output_dir, \"train_qa_format.jsonl\")\n",
    "save_jsonl(train_processed, train_qa_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u train QA format: {train_qa_path}\")\n",
    "\n",
    "# L∆∞u instruction format\n",
    "train_instruction_path = os.path.join(output_dir, \"train_instruction_format.jsonl\")\n",
    "save_instruction_format(train_processed, train_instruction_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u train instruction format: {train_instruction_path}\")\n",
    "\n",
    "# L∆∞u conversation format\n",
    "train_conversation_path = os.path.join(output_dir, \"train_conversation_format.jsonl\")\n",
    "save_conversation_format(train_processed, train_conversation_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u train conversation format: {train_conversation_path}\")\n",
    "\n",
    "# L∆∞u test set\n",
    "test_qa_path = os.path.join(output_dir, \"test_qa_format.jsonl\")\n",
    "save_jsonl(test_processed, test_qa_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u test QA format: {test_qa_path}\")\n",
    "\n",
    "test_instruction_path = os.path.join(output_dir, \"test_instruction_format.jsonl\")\n",
    "save_instruction_format(test_processed, test_instruction_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u test instruction format: {test_instruction_path}\")\n",
    "\n",
    "test_conversation_path = os.path.join(output_dir, \"test_conversation_format.jsonl\")\n",
    "save_conversation_format(test_processed, test_conversation_path)\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u test conversation format: {test_conversation_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "36feba28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ ƒê√£ l∆∞u metadata: ../data/finetune_data\\metadata.json\n",
      "\n",
      "üìù M·∫´u d·ªØ li·ªáu c√°c format:\n",
      "\n",
      "üî∏ QA Format:\n",
      "Question: Trong B·ªô lu·∫≠t H√¨nh s·ª± th√¨ bao nhi√™u tu·ªïi ƒë∆∞·ª£c xem l√† ng∆∞·ªùi gi√†, ng∆∞·ªùi cao tu·ªïi?...\n",
      "Answer: Ng∆∞·ªùi cao tu·ªïi, ng∆∞·ªùi gi√†, ng∆∞·ªùi gi√† y·∫øu ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 2 Lu·∫≠t Ng∆∞·ªùi cao tu·ªïi 2009: \"Ng∆∞·ªùi c...\n",
      "\n",
      "üî∏ Instruction Format:\n",
      "Instruction: Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\n",
      "Input: Trong B·ªô lu·∫≠t H√¨nh s·ª± th√¨ bao nhi√™u tu·ªïi ƒë∆∞·ª£c xem l√† ng∆∞·ªùi gi√†, ng∆∞·ªùi cao tu·ªïi?...\n",
      "Output: Ng∆∞·ªùi cao tu·ªïi, ng∆∞·ªùi gi√†, ng∆∞·ªùi gi√† y·∫øu ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 2 Lu·∫≠t Ng∆∞·ªùi cao tu·ªïi 2009: \"Ng∆∞·ªùi c...\n",
      "\n",
      "üî∏ Conversation Format:\n",
      "User: Trong B·ªô lu·∫≠t H√¨nh s·ª± th√¨ bao nhi√™u tu·ªïi ƒë∆∞·ª£c xem l√† ng∆∞·ªùi gi√†, ng∆∞·ªùi cao tu·ªïi?...\n",
      "Assistant: Ng∆∞·ªùi cao tu·ªïi, ng∆∞·ªùi gi√†, ng∆∞·ªùi gi√† y·∫øu ƒë∆∞·ª£c quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 2 Lu·∫≠t Ng∆∞·ªùi cao tu·ªïi 2009: \"Ng∆∞·ªùi c...\n",
      "\n",
      "üéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω v√† l∆∞u 21529 m·∫´u d·ªØ li·ªáu v√†o ../data/finetune_data\n",
      "\n",
      "üìÅ Files ƒë√£ t·∫°o:\n",
      "   - metadata.json: 0.00 MB\n",
      "   - test_conversation_format.jsonl: 4.39 MB\n",
      "   - test_instruction_format.jsonl: 4.39 MB\n",
      "   - test_qa_format.jsonl: 4.28 MB\n",
      "   - train_conversation_format.jsonl: 43.39 MB\n",
      "   - train_instruction_format.jsonl: 43.31 MB\n",
      "   - train_qa_format.jsonl: 42.27 MB\n"
     ]
    }
   ],
   "source": [
    "# T·∫°o metadata v√† t·ªïng k·∫øt\n",
    "metadata = {\n",
    "    \"dataset_info\": {\n",
    "        \"source\": \"phuocsang/hoidap-tvpl-20k\",\n",
    "        \"description\": \"Vietnamese Legal Q&A Dataset processed for fine-tuning\",\n",
    "        \"total_samples\": len(train_processed) + len(test_processed),\n",
    "        \"train_samples\": len(train_processed),\n",
    "        \"test_samples\": len(test_processed)\n",
    "    },\n",
    "    \"processing_info\": {\n",
    "        \"filters_applied\": [\n",
    "            \"Minimum question length: 10 characters\",\n",
    "            \"Minimum answer length: 50 characters\", \n",
    "            \"Maximum answer length: 5000 characters\",\n",
    "            \"Questions must end with '?'\",\n",
    "            \"Text cleaning: removed extra whitespace and special characters\"\n",
    "        ],\n",
    "        \"retention_rate\": f\"{(len(train_processed) + len(test_processed))/(len(ds['train']) + len(ds['test']))*100:.1f}%\"\n",
    "    },\n",
    "    \"file_formats\": {\n",
    "        \"qa_format\": \"Simple question-answer pairs\",\n",
    "        \"instruction_format\": \"Instruction tuning format with instruction/input/output\",\n",
    "        \"conversation_format\": \"Multi-turn conversation format for chatbot training\"\n",
    "    },\n",
    "    \"sample_counts\": {\n",
    "        \"train\": len(train_processed),\n",
    "        \"test\": len(test_processed)\n",
    "    }\n",
    "}\n",
    "\n",
    "# L∆∞u metadata\n",
    "metadata_path = os.path.join(output_dir, \"metadata.json\")\n",
    "with open(metadata_path, 'w', encoding='utf-8') as f:\n",
    "    json.dump(metadata, f, indent=2, ensure_ascii=False)\n",
    "\n",
    "print(f\"‚úÖ ƒê√£ l∆∞u metadata: {metadata_path}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã m·∫´u d·ªØ li·ªáu t·ª´ m·ªói format\n",
    "print(\"\\nüìù M·∫´u d·ªØ li·ªáu c√°c format:\")\n",
    "\n",
    "print(\"\\nüî∏ QA Format:\")\n",
    "sample_qa = train_processed[0]\n",
    "print(f\"Question: {sample_qa['question'][:100]}...\")\n",
    "print(f\"Answer: {sample_qa['answer'][:100]}...\")\n",
    "\n",
    "print(\"\\nüî∏ Instruction Format:\")\n",
    "print(f\"Instruction: Tr·∫£ l·ªùi c√¢u h·ªèi ph√°p lu·∫≠t sau:\")\n",
    "print(f\"Input: {sample_qa['question'][:100]}...\")\n",
    "print(f\"Output: {sample_qa['answer'][:100]}...\")\n",
    "\n",
    "print(\"\\nüî∏ Conversation Format:\")\n",
    "print(f\"User: {sample_qa['question'][:100]}...\")\n",
    "print(f\"Assistant: {sample_qa['answer'][:100]}...\")\n",
    "\n",
    "print(f\"\\nüéâ Ho√†n th√†nh! ƒê√£ x·ª≠ l√Ω v√† l∆∞u {len(train_processed) + len(test_processed)} m·∫´u d·ªØ li·ªáu v√†o {output_dir}\")\n",
    "\n",
    "# Hi·ªÉn th·ªã th·ªëng k√™ file\n",
    "print(f\"\\nüìÅ Files ƒë√£ t·∫°o:\")\n",
    "for filename in os.listdir(output_dir):\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    if os.path.isfile(filepath):\n",
    "        size_mb = os.path.getsize(filepath) / (1024 * 1024)\n",
    "        print(f\"   - {filename}: {size_mb:.2f} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5e2d692c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîç Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu:\n",
      "\n",
      "üìÇ Train files:\n",
      "‚úÖ train_qa_format.jsonl: 19536 d√≤ng (OK)\n",
      "‚úÖ train_instruction_format.jsonl: 19536 d√≤ng (OK)\n",
      "‚úÖ train_conversation_format.jsonl: 19536 d√≤ng (OK)\n",
      "\n",
      "üìÇ Test files:\n",
      "‚úÖ test_qa_format.jsonl: 1993 d√≤ng (OK)\n",
      "‚úÖ test_instruction_format.jsonl: 1993 d√≤ng (OK)\n",
      "‚úÖ test_conversation_format.jsonl: 1993 d√≤ng (OK)\n",
      "‚úÖ metadata.json: OK\n",
      "\n",
      "üéâ T·∫•t c·∫£ files ƒë·ªÅu h·ª£p l·ªá! Dataset ƒë√£ s·∫µn s√†ng ƒë·ªÉ fine-tuning.\n",
      "\n",
      "üí° S·ª≠ d·ª•ng:\n",
      "   - QA format: Cho traditional Q&A training\n",
      "   - Instruction format: Cho instruction-following models\n",
      "   - Conversation format: Cho chatbot training\n",
      "\n",
      "üìç ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu: d:\\project\\project_1\\Vietnamese-Legal-Chatbot-RAG-System\\data_pipeline\\data\\finetune_data\n"
     ]
    }
   ],
   "source": [
    "# Ki·ªÉm tra v√† validate d·ªØ li·ªáu ƒë√£ l∆∞u\n",
    "def validate_jsonl_file(filepath, expected_count):\n",
    "    \"\"\"Ki·ªÉm tra t√≠nh to√†n v·∫πn c·ªßa file JSONL\"\"\"\n",
    "    try:\n",
    "        count = 0\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                json.loads(line.strip())  # Ki·ªÉm tra JSON h·ª£p l·ªá\n",
    "                count += 1\n",
    "        \n",
    "        if count == expected_count:\n",
    "            print(f\"‚úÖ {os.path.basename(filepath)}: {count} d√≤ng (OK)\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"‚ùå {os.path.basename(filepath)}: {count} d√≤ng (Expected: {expected_count})\")\n",
    "            return False\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error validating {os.path.basename(filepath)}: {e}\")\n",
    "        return False\n",
    "\n",
    "print(\"üîç Ki·ªÉm tra t√≠nh to√†n v·∫πn d·ªØ li·ªáu:\")\n",
    "\n",
    "# Validate train files\n",
    "train_files = [\n",
    "    \"train_qa_format.jsonl\",\n",
    "    \"train_instruction_format.jsonl\", \n",
    "    \"train_conversation_format.jsonl\"\n",
    "]\n",
    "\n",
    "test_files = [\n",
    "    \"test_qa_format.jsonl\",\n",
    "    \"test_instruction_format.jsonl\",\n",
    "    \"test_conversation_format.jsonl\"\n",
    "]\n",
    "\n",
    "all_valid = True\n",
    "\n",
    "print(\"\\nüìÇ Train files:\")\n",
    "for filename in train_files:\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    if not validate_jsonl_file(filepath, len(train_processed)):\n",
    "        all_valid = False\n",
    "\n",
    "print(\"\\nüìÇ Test files:\")\n",
    "for filename in test_files:\n",
    "    filepath = os.path.join(output_dir, filename)\n",
    "    if not validate_jsonl_file(filepath, len(test_processed)):\n",
    "        all_valid = False\n",
    "\n",
    "# Ki·ªÉm tra metadata\n",
    "metadata_file = os.path.join(output_dir, \"metadata.json\")\n",
    "try:\n",
    "    with open(metadata_file, 'r', encoding='utf-8') as f:\n",
    "        metadata_loaded = json.load(f)\n",
    "    print(f\"‚úÖ metadata.json: OK\")\n",
    "except Exception as e:\n",
    "    print(f\"‚ùå metadata.json: Error - {e}\")\n",
    "    all_valid = False\n",
    "\n",
    "if all_valid:\n",
    "    print(f\"\\nüéâ T·∫•t c·∫£ files ƒë·ªÅu h·ª£p l·ªá! Dataset ƒë√£ s·∫µn s√†ng ƒë·ªÉ fine-tuning.\")\n",
    "    print(f\"\\nüí° S·ª≠ d·ª•ng:\")\n",
    "    print(f\"   - QA format: Cho traditional Q&A training\")\n",
    "    print(f\"   - Instruction format: Cho instruction-following models\") \n",
    "    print(f\"   - Conversation format: Cho chatbot training\")\n",
    "else:\n",
    "    print(f\"\\n‚ö†Ô∏è C√≥ l·ªói x·∫£y ra. Vui l√≤ng ki·ªÉm tra l·∫°i.\")\n",
    "\n",
    "print(f\"\\nüìç ƒê∆∞·ªùng d·∫´n d·ªØ li·ªáu: {os.path.abspath(output_dir)}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
