# PhÃ¢n TÃ­ch Chi Tiáº¿t Model Fine-tuning Setup

## ğŸ“Š Dá»¯ Liá»‡u Training

### Thá»‘ng kÃª dá»¯ liá»‡u:
- **Sá»‘ lÆ°á»£ng documents**: 1,875,511 documents
- **Äá»™ dÃ i trung bÃ¬nh**: ~345 kÃ½ tá»±/document
- **Format**: JSONL vá»›i structure `{"id": "...", "text": "..."}`
- **NgÃ´n ngá»¯**: Tiáº¿ng Viá»‡t - Legal domain
- **Cháº¥t lÆ°á»£ng**: ÄÃ£ Ä‘Æ°á»£c preprocess trong data pipeline

### ÄÃ¡nh giÃ¡ dá»¯ liá»‡u:
âœ… **TÃCH Cá»°C**:
- Sá»‘ lÆ°á»£ng ráº¥t lá»›n (1.8M+ documents) - Ä‘á»§ Ä‘á»ƒ fine-tune hiá»‡u quáº£
- Specific domain (Vietnamese legal) - phÃ¹ há»£p cho specialized embedding
- Äá»™ dÃ i vá»«a pháº£i (~345 chars) - khÃ´ng quÃ¡ dÃ i, khÃ´ng quÃ¡ ngáº¯n
- ÄÃ£ Ä‘Æ°á»£c clean vÃ  preprocess

âš ï¸ **Cáº¦N LÆ¯U Ã**:
- Cáº§n kiá»ƒm tra distribution Ä‘á»™ dÃ i text (cÃ³ documents quÃ¡ dÃ i khÃ´ng?)
- Cáº§n verify quality cá»§a má»™t sá»‘ samples random
- NÃªn cÃ³ validation set Ä‘á»ƒ Ä‘Ã¡nh giÃ¡ performance

## ğŸ¤– Base Model Analysis

### Model hiá»‡n táº¡i:
```
sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2
```

### Äáº·c Ä‘iá»ƒm model:
- **Architecture**: MiniLM-L12 (12-layer transformer)
- **Language support**: Multilingual (104 languages including Vietnamese)
- **Vector dimension**: 384
- **Max sequence length**: 512 tokens
- **Size**: ~118MB
- **Performance**: Good balance between quality & speed

### ÄÃ¡nh giÃ¡ lá»±a chá»n:
âœ… **TUYá»†T Vá»œI**:
- Support Vietnamese tá»‘t
- ÄÃ£ Ä‘Æ°á»£c pre-train trÃªn multilingual data
- KÃ­ch thÆ°á»›c vá»«a pháº£i, inference nhanh
- Proven performance trÃªn sentence similarity tasks
- Compatible vá»›i sentence-transformers framework

ğŸ“ˆ **ALTERNATIVES NÃŠN XEMXÃ‰T**:
```python
# Náº¿u cáº§n performance cao hÆ¡n (nhÆ°ng cháº­m hÆ¡n):
'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'  # 768 dims

# Náº¿u cáº§n model nhá» hÆ¡n (nhÆ°ng accuracy tháº¥p hÆ¡n):
'sentence-transformers/paraphrase-MiniLM-L6-v2'  # 384 dims, 6 layers
```

## âš™ï¸ Hyperparameters Analysis

### Current settings:
```python
hyperparameters = {
    'base-model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
    'max-seq-length': 512,
    'batch-size': 16,
    'epochs': 3,
    'learning-rate': 2e-5,
}
```

### Detailed Analysis:

#### 1. **Max Sequence Length: 512**
âœ… **PERFECT**: 
- PhÃ¹ há»£p vá»›i average text length (~345 chars â‰ˆ ~100-150 tokens)
- KhÃ´ng waste computation trÃªn padding
- Capture Ä‘Æ°á»£c context Ä‘áº§y Ä‘á»§

#### 2. **Batch Size: 16**
âš ï¸ **Cáº¦N ÄIá»€U CHá»ˆNH**:
```python
# Recommendations based on instance type:
ml.g4dn.xlarge (16GB GPU): batch_size = 32-64  # CÃ³ thá»ƒ tÄƒng
ml.g4dn.2xlarge (32GB GPU): batch_size = 64-128
ml.p3.2xlarge (16GB GPU): batch_size = 32-64

# Current batch_size = 16 lÃ  conservative, cÃ³ thá»ƒ tÄƒng Ä‘á»ƒ:
# - TÄƒng training speed
# - Better gradient estimation
# - Improved model convergence
```

**RECOMMENDED**: TÄƒng lÃªn `batch_size = 32` hoáº·c `48`

#### 3. **Epochs: 3**
âš ï¸ **CÃ“ THá»‚ CHÆ¯A Äá»¦**:
```python
# Vá»›i 1.8M documents, má»—i epoch sáº½ cÃ³ ráº¥t nhiá»u steps
# NhÆ°ng vá»›i specialized domain (legal), cáº§n nhiá»u epochs hÆ¡n

# Recommendations:
epochs = 5-8  # Cho specialized domain
# Vá»›i early stopping based on validation loss
```

**RECOMMENDED**: TÄƒng lÃªn `epochs = 5` vá»›i early stopping

#### 4. **Learning Rate: 2e-5**
âœ… **Tá»T**: 
- Standard learning rate cho fine-tuning sentence transformers
- KhÃ´ng quÃ¡ cao (avoid catastrophic forgetting)
- KhÃ´ng quÃ¡ tháº¥p (training sáº½ cháº­m)

**ALTERNATIVE**: CÃ³ thá»ƒ thá»­ learning rate schedule:
```python
# Warmup + cosine decay
warmup_steps = 1000
total_steps = (len(dataset) // batch_size) * epochs
```

## ğŸ”§ Training Strategy Analysis

### Current approach:
```python
# Táº¡o positive pairs tá»« same documents (giáº£ Ä‘á»‹nh documents liÃªn quan)
# VÃ  negative pairs tá»« random sampling
for i in range(len(texts)):
    for j in range(i + 1, min(i + 10, len(texts))):  # Positive pairs
        examples.append(InputExample(texts=[texts[i], texts[j]], label=0.8))
    
    # Negative pairs
    if i + 50 < len(texts):
        examples.append(InputExample(texts=[texts[i], texts[i + 50]], label=0.2))
```

âš ï¸ **Váº¤N Äá»€ NGHIÃŠM TRá»ŒNG**:

1. **Weak positive pairs**: Giáº£ Ä‘á»‹nh documents liÃªn tiáº¿p cÃ³ similarity cao lÃ  KHÃ”NG chÃ­nh xÃ¡c
2. **Arbitrary labels**: 0.8 vÃ  0.2 khÃ´ng cÃ³ cÆ¡ sá»Ÿ thá»±c táº¿
3. **Poor negative sampling**: Documents cÃ¡ch xa 50 positions khÃ´ng Ä‘áº£m báº£o lÃ  negative

### ğŸš€ RECOMMENDED IMPROVEMENTS:

#### Option 1: Self-supervised approach
```python
# Sá»­ dá»¥ng cÃ¡c techniques tá»‘t hÆ¡n:
from sentence_transformers.losses import MultipleNegativesRankingLoss

# KhÃ´ng cáº§n manual labeling, model tá»± há»c
# Má»—i sentence lÃ  positive vá»›i chÃ­nh nÃ³, negative vá»›i others trong batch
```

#### Option 2: Domain-specific approach  
```python
# Táº¡o better training pairs based on:
# - Overlapping legal concepts
# - Similar document types
# - Semantic similarity using existing embeddings
```

## ğŸ“ˆ RECOMMENDED CONFIGURATION

```python
# Optimized hyperparameters
hyperparameters = {
    'base-model': 'sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2',
    'max-seq-length': 512,
    'batch-size': 48,  # TÄƒng tá»« 16
    'epochs': 6,       # TÄƒng tá»« 3  
    'learning-rate': 2e-5,
    'warmup-steps': 1000,
    'evaluation-steps': 500,
    'save-steps': 1000
}

# Training strategy
training_strategy = "MultipleNegativesRankingLoss"  # Thay vÃ¬ CosineSimilarityLoss
```

## ğŸ’° Cost & Time Estimation

### With current config (batch_size=16, epochs=3):
- **Training time**: ~6-8 hours trÃªn ml.g4dn.xlarge
- **Cost**: ~$10-15

### With recommended config (batch_size=48, epochs=6):
- **Training time**: ~8-12 hours  
- **Cost**: ~$15-20

### Cost optimization:
- Sá»­ dá»¥ng Spot instances: giáº£m 50-70% cost
- Mixed precision training: tÄƒng speed ~30%

## ğŸ¯ Expected Results

### Vá»›i current setup:
- Moderate improvement trÃªn Vietnamese legal domain
- Better than base multilingual model nhÆ°ng khÃ´ng optimal

### Vá»›i recommended setup:  
- Significant improvement trÃªn legal terminology
- Better semantic understanding cá»§a legal concepts
- Improved retrieval performance cho RAG system

## âœ… Action Items

1. **Immediate fixes**:
   - TÄƒng batch_size lÃªn 48
   - TÄƒng epochs lÃªn 6
   - ThÃªm early stopping

2. **Training strategy improvement**:
   - Thay CosineSimilarityLoss báº±ng MultipleNegativesRankingLoss
   - Remove manual positive/negative pair creation

3. **Evaluation setup**:
   - Táº¡o evaluation dataset
   - Setup proper metrics (cosine similarity, retrieval accuracy)

4. **Monitoring**:
   - Track loss curves
   - Validate trÃªn legal query examples
   - Compare vá»›i base model performance