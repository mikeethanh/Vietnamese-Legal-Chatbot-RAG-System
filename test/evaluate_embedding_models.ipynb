{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04e953f",
   "metadata": {},
   "source": [
    "# ƒê√°nh gi√° c√°c m√¥ h√¨nh Embedding cho h·ªá th·ªëng Legal Chatbot\n",
    "\n",
    "Notebook n√†y ƒë√°nh gi√° v√† so s√°nh hi·ªáu su·∫•t c·ªßa c√°c m√¥ h√¨nh embedding sau:\n",
    "1. **thanhtantran/Vietnamese_Embedding_v2** - M√¥ h√¨nh chuy√™n cho ti·∫øng Vi·ªát\n",
    "2. **Baai/bge-m3** - M√¥ h√¨nh ƒëa ng√¥n ng·ªØ\n",
    "3. **intfloat/multilingual-e5-large** - M√¥ h√¨nh ƒëa ng√¥n ng·ªØ l·ªõn\n",
    "\n",
    "Dataset: **anti-ai/ViNLI-Zalo-supervised** (triplet format: query, positive, hard_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9dc923",
   "metadata": {},
   "source": [
    "## üöÄ QUICK START cho Google Colab T4\n",
    "\n",
    "**QUAN TR·ªåNG - L√†m theo th·ª© t·ª±:**\n",
    "\n",
    "1. ‚úÖ **Runtime ‚Üí Restart runtime** (ƒë·ªÉ clear memory c≈©)\n",
    "2. ‚úÖ Ch·∫°y cell \"C√†i ƒë·∫∑t th∆∞ vi·ªán\"\n",
    "3. ‚úÖ Ch·∫°y cell \"Import th∆∞ vi·ªán\"\n",
    "4. ‚úÖ Ch·∫°y cell \"Clear GPU Memory\"\n",
    "5. ‚úÖ Load dataset\n",
    "6. ‚úÖ **Ch·ªçn c·∫•u h√¨nh:**\n",
    "   - Ch·∫°y c·∫£ 3 models: `MAX_SAMPLES=1000, BATCH_SIZE=4`\n",
    "   - Ch·∫°y 1 model: `MAX_SAMPLES=2000, BATCH_SIZE=8`\n",
    "7. ‚úÖ Ch·∫°y c√°c cell c√≤n l·∫°i\n",
    "\n",
    "**N·∫øu g·∫∑p OOM:** Restart runtime v√† gi·∫£m MAX_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d29bd0",
   "metadata": {},
   "source": [
    "## 1. C√†i ƒë·∫∑t th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408dae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers sentence-transformers torch scikit-learn numpy pandas tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ed461",
   "metadata": {},
   "source": [
    "## 2. Import th∆∞ vi·ªán"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f7669",
   "metadata": {},
   "source": [
    "## ‚ö†Ô∏è GPU Memory Management (Quan tr·ªçng cho Colab T4!)\n",
    "\n",
    "**H∆Ø·ªöNG D·∫™N TR√ÅNH L·ªñI \"CUDA OUT OF MEMORY\" TR√äN COLAB T4 (15GB):**\n",
    "\n",
    "### ‚úÖ B∆∞·ªõc 1: Restart Runtime tr∆∞·ªõc khi ch·∫°y\n",
    "- Menu: **Runtime ‚Üí Restart runtime**\n",
    "- Ho·∫∑c: **Ctrl+M .** (ph√≠m t·∫Øt)\n",
    "\n",
    "### ‚úÖ B∆∞·ªõc 2: Clear GPU memory n·∫øu c·∫ßn\n",
    "- Ch·∫°y cell \"Clear GPU Memory\" ·ªü section 3\n",
    "\n",
    "### ‚úÖ B∆∞·ªõc 3: ƒêi·ªÅu ch·ªânh c·∫•u h√¨nh\n",
    "**Trong cell config (section 3):**\n",
    "```python\n",
    "MAX_SAMPLES = 2000   # Gi·∫£m xu·ªëng 1000 n·∫øu v·∫´n OOM\n",
    "BATCH_SIZE = 8       # Gi·∫£m xu·ªëng 4 n·∫øu v·∫´n OOM  \n",
    "USE_FP16 = True      # B·∫ÆT BU·ªòC ph·∫£i True\n",
    "```\n",
    "\n",
    "### ‚úÖ B∆∞·ªõc 4: Ch·∫°y t·ª´ng model m·ªôt\n",
    "**Trong cell \"ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh\" (section 4):**\n",
    "- Uncomment m·ªôt trong c√°c d√≤ng ƒë·ªÉ ch·ªâ ch·∫°y 1 model\n",
    "- V√≠ d·ª•: `models_to_evaluate = {\"BGE-M3\": \"BAAI/bge-m3\"}`\n",
    "\n",
    "### üìä Khuy·∫øn ngh·ªã c·∫•u h√¨nh cho Colab T4:\n",
    "- **Ch·∫°y t·∫•t c·∫£ 3 models:** MAX_SAMPLES=1000, BATCH_SIZE=4, USE_FP16=True\n",
    "- **Ch·∫°y 1 model:** MAX_SAMPLES=2000, BATCH_SIZE=8, USE_FP16=True\n",
    "- **An to√†n nh·∫•t:** MAX_SAMPLES=500, BATCH_SIZE=4, USE_FP16=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f60ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikeethanh/anaconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thi·∫øt l·∫≠p device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Thi·∫øt l·∫≠p memory management cho CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Allocated GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Cached GPU Memory: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28980581",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693477b8",
   "metadata": {},
   "source": [
    "## üîÑ Clear GPU Memory (Ch·∫°y cell n√†y n·∫øu g·∫∑p OOM)\n",
    "\n",
    "**N·∫øu b·∫°n g·∫∑p l·ªói \"CUDA out of memory\", ch·∫°y cell d∆∞·ªõi ƒë√¢y ƒë·ªÉ clear memory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef785cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear t·∫•t c·∫£ GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Kill t·∫•t c·∫£ process ƒëang d√πng GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Force clear all allocated memory\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"‚úÖ GPU Memory cleared!\")\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "# N·∫øu v·∫´n kh√¥ng ƒë∆∞·ª£c, restart runtime:\n",
    "# Runtime -> Restart runtime (trong Colab menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225dddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32980/32980 [00:00<00:00, 45453.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits: dict_keys(['train'])\n",
      "\n",
      "Sample data:\n",
      "{'query': 'T·ªï s√°t_h·∫°ch c·∫•p gi·∫•y_ph√©p l√°i t√†u_h·ªèa c√≥ bao_nhi√™u th√†nh_vi√™n ?', 'positive': 'ƒêi·ªÅu 30 . T·ªï s√°t_h·∫°ch 1 . T·ªï s√°t_h·∫°ch do C·ª•c_tr∆∞·ªüng C·ª•c ƒê∆∞·ªùng_s·∫Øt Vi·ªát_Nam th√†nh_l·∫≠p , ch·ªãu s·ª± ch·ªâ_ƒë·∫°o tr·ª±c_ti·∫øp c·ªßa H·ªôi_ƒë·ªìng s√°t_h·∫°ch . \\n 2 . T·ªï s√°t_h·∫°ch c√≥ √≠t_nh·∫•t 05 th√†nh_vi√™n , bao_g·ªìm t·ªï_tr∆∞·ªüng , c√°c s√°t_h·∫°ch vi√™n l√Ω_thuy·∫øt v√† s√°t_h·∫°ch vi√™n th·ª±c_h√†nh . T·ªï_tr∆∞·ªüng T·ªï s√°t_h·∫°ch l√† c√¥ng_ch·ª©c C·ª•c ƒê∆∞·ªùng_s·∫Øt Vi·ªát_Nam ho·∫∑c l√£nh_ƒë·∫°o doanh_nghi·ªáp c√≥ th√≠_sinh d·ª± k·ª≥ s√°t_h·∫°ch , c√°c s√°t_h·∫°ch vi√™n l√† ng∆∞·ªùi ƒëang c√¥ng_t√°c t·∫°i doanh_nghi·ªáp c√≥ th√≠_sinh tham_d·ª± k·ª≥ s√°t_h·∫°ch v√† ng∆∞·ªùi ƒëang c√¥ng_t√°c t·∫°i c√°c c∆°_s·ªü ƒë√†o_t·∫°o li√™n_quan ƒë·∫øn l√°i t√†u . \\n 3 . Ti√™u_chu·∫©n c·ªßa s√°t_h·∫°ch vi√™n : \\n a ) C√≥ t∆∞_c√°ch ƒë·∫°o_ƒë·ª©c t·ªët v√† c√≥ chuy√™n_m√¥n ph√π_h·ª£p ; \\n b ) ƒê√£ qua kh√≥a hu·∫•n_luy·ªán v·ªÅ nghi·ªáp_v·ª• s√°t_h·∫°ch l√°i t√†u do C·ª•c ƒê∆∞·ªùng_s·∫Øt Vi·ªát_Nam t·ªï_ch·ª©c v√† ƒë∆∞·ª£c c·∫•p th·∫ª s√°t_h·∫°ch vi√™n ; \\n c ) S√°t_h·∫°ch vi√™n l√Ω_thuy·∫øt ph·∫£i t·ªët_nghi·ªáp ƒë·∫°i_h·ªçc tr·ªü l√™n chuy√™n_ng√†nh ph√π_h·ª£p n·ªôi_dung s√°t_h·∫°ch , c√≥ √≠t_nh·∫•t 03 nƒÉm kinh_nghi·ªám c√¥ng_t√°c li√™n_quan ƒë·∫øn l√°i t√†u ; \\n d ) S√°t_h·∫°ch vi√™n th·ª±c_h√†nh ph·∫£i t·ªët_nghi·ªáp tr√¨nh_ƒë·ªô trung_c·∫•p l√°i t√†u tr·ªü l√™n ; c√≥ √≠t_nh·∫•t 03 nƒÉm kinh_nghi·ªám tr·ª±c_ti·∫øp ƒë·∫£m_nh·∫≠n ch·ª©c_danh l√°i t√†u , ri√™ng s√°t_h·∫°ch vi√™n l√°i t√†u ƒë∆∞·ªùng_s·∫Øt ƒë√¥_th·ªã ph·∫£i c√≥ √≠t_nh·∫•t 01 nƒÉm kinh_nghi·ªám tr·ª±c_ti·∫øp ƒë·∫£m_nh·∫≠n ch·ª©c_danh l√°i t√†u ƒë∆∞·ªùng_s·∫Øt ƒë√¥_th·ªã . \\n 4 . N·ªôi_dung b·ªìi_d∆∞·ª°ng nghi·ªáp_v·ª• s√°t_h·∫°ch vi√™n l√°i t√†u ƒë∆∞·ª£c quy_ƒë·ªãnh t·∫°i Ph·ª•_l·ª•c III ban_h√†nh k√®m theo Th√¥ng_t∆∞ n√†y . \\n 5 . Nhi·ªám_v·ª• , quy·ªÅn_h·∫°n c·ªßa T·ªï s√°t_h·∫°ch : \\n a ) Gi√∫p H·ªôi_ƒë·ªìng s√°t_h·∫°ch x√¢y_d·ª±ng n·ªôi_dung s√°t_h·∫°ch tr√¨nh C·ª•c ƒê∆∞·ªùng_s·∫Øt Vi·ªát_Nam ph√™_duy·ªát ; \\n b ) Ki·ªÉm_tra ti√™u_chu·∫©n , quy_c√°ch c·ªßa ph∆∞∆°ng_ti·ªán , trang thi·∫øt_b·ªã chuy√™n_m√¥n ph·ª•c_v·ª• s√°t_h·∫°ch v√† ph∆∞∆°ng_√°n b·∫£o_ƒë·∫£m an_to√†n cho k·ª≥ s√°t_h·∫°ch ; \\n c ) Ph·ªï_bi·∫øn n·ªôi_dung , quy_tr√¨nh s√°t_h·∫°ch v√† ki·ªÉm_tra vi·ªác ch·∫•p_h√†nh n·ªôi_quy s√°t_h·∫°ch ; \\n d ) Ch·∫•m thi v√† t·ªïng_h·ª£p k·∫øt_qu·∫£ k·ª≥ s√°t_h·∫°ch ƒë·ªÉ b√°o_c√°o H·ªôi_ƒë·ªìng s√°t_h·∫°ch ; \\n ƒë ) S√°t_h·∫°ch vi√™n ch·ªãu tr√°ch_nhi·ªám tr·ª±c_ti·∫øp v·ªÅ k·∫øt_qu·∫£ ch·∫•m thi . T·ªï_tr∆∞·ªüng T·ªï s√°t_h·∫°ch ch·ªãu tr√°ch_nhi·ªám chung v·ªÅ k·∫øt_qu·∫£ s√°t_h·∫°ch ; \\n e ) L·∫≠p bi√™n_b·∫£n , x·ª≠_l√Ω c√°c tr∆∞·ªùng_h·ª£p vi_ph·∫°m n·ªôi_quy s√°t_h·∫°ch theo quy·ªÅn_h·∫°n ƒë∆∞·ª£c giao ho·∫∑c b√°o_c√°o Ch·ªß_t·ªãch , H·ªôi_ƒë·ªìng s√°t_h·∫°ch gi·∫£i_quy·∫øt n·∫øu v∆∞·ª£t th·∫©m_quy·ªÅn . \\n 6 . T·ªï s√°t_h·∫°ch t·ª± gi·∫£i_th·ªÉ khi ho√†n_th√†nh nhi·ªám_v·ª• .', 'hard_neg': 'ƒêi·ªÅu 10 . X√°c_ƒë·ªãnh n·ªôi_dung ƒëƒÉng_k√Ω l·∫°i khai_sinh 1 . Tr∆∞·ªùng_h·ª£p ng∆∞·ªùi y√™u_c·∫ßu ƒëƒÉng_k√Ω l·∫°i khai_sinh c√≥ gi·∫•y_t·ªù theo quy_ƒë·ªãnh t·∫°i kho·∫£n 4 ƒêi·ªÅu 26 Ngh·ªã_ƒë·ªãnh s·ªë 123 / 2015 / Nƒê - CP v√† kho·∫£n 1 , kho·∫£n 2 ƒêi·ªÅu 9 Th√¥ng_t∆∞ n√†y th√¨ n·ªôi_dung ƒëƒÉng_k√Ω l·∫°i khai_sinh ƒë∆∞·ª£c x√°c_ƒë·ªãnh theo gi·∫•y_t·ªù ƒë√≥ . \\n 2 . T·∫°i th·ªùi_ƒëi·ªÉm ƒëƒÉng_k√Ω l·∫°i khai_sinh , n·∫øu th√¥ng_tin v·ªÅ cha , m·∫π v√† c·ªßa b·∫£n_th√¢n ng∆∞·ªùi y√™u_c·∫ßu ƒëƒÉng_k√Ω l·∫°i khai_sinh c√≥ thay_ƒë·ªïi so v·ªõi n·ªôi_dung gi·∫•y_t·ªù t·∫°i kho·∫£n 1 ƒêi·ªÅu n√†y , th√¨ ng∆∞·ªùi ƒë√≥ c√≥ tr√°ch_nhi·ªám xu·∫•t_tr√¨nh gi·∫•y_t·ªù h·ª£p_l·ªá ch·ª©ng_minh vi·ªác thay_ƒë·ªïi . N·∫øu vi·ªác thay_ƒë·ªïi th√¥ng_tin l√† ph√π_h·ª£p v·ªõi quy_ƒë·ªãnh ph√°p_lu·∫≠t th√¨ n·ªôi_dung ƒëƒÉng_k√Ω l·∫°i khai_sinh ƒë∆∞·ª£c x√°c_ƒë·ªãnh theo th√¥ng_tin thay_ƒë·ªïi ; n·ªôi_dung th√¥ng_tin tr∆∞·ªõc khi thay_ƒë·ªïi ƒë∆∞·ª£c ghi v√†o m·∫∑t sau c·ªßa Gi·∫•y khai_sinh v√† m·ª•c ‚Äú Ghi_ch√∫ ‚Äù trong S·ªï ƒëƒÉng_k√Ω khai_sinh . \\n Tr∆∞·ªùng_h·ª£p cha , m·∫π c·ªßa ng∆∞·ªùi y√™u_c·∫ßu ƒëƒÉng_k√Ω l·∫°i khai_sinh ƒë√£ ch·∫øt th√¨ m·ª•c ‚Äú N∆°i c∆∞_tr√∫ ‚Äù trong S·ªï ƒëƒÉng_k√Ω khai_sinh , Gi·∫•y khai_sinh ghi : ‚Äú ƒê√£ ch·∫øt ‚Äù . \\n Tr∆∞·ªùng_h·ª£p ƒë·ªãa_danh h√†nh_ch√≠nh ƒë√£ c√≥ s·ª± thay_ƒë·ªïi so v·ªõi ƒë·ªãa_danh ghi trong gi·∫•y_t·ªù ƒë∆∞·ª£c c·∫•p tr∆∞·ªõc ƒë√¢y th√¨ ghi theo ƒë·ªãa_danh h√†nh_ch√≠nh hi·ªán_t·∫°i ; ƒë·ªãa_danh h√†nh_ch√≠nh tr∆∞·ªõc ƒë√¢y ƒë∆∞·ª£c ghi v√†o m·∫∑t sau c·ªßa Gi·∫•y khai_sinh v√† m·ª•c Ghi_ch√∫ trong S·ªï ƒëƒÉng_k√Ω khai_sinh .'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset t·ª´ HuggingFace\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"anti-ai/ViNLI-Zalo-supervised\")\n",
    "\n",
    "# Ki·ªÉm tra c·∫•u tr√∫c dataset\n",
    "print(f\"\\nDataset splits: {dataset.keys()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(dataset['train'][0] if 'train' in dataset else dataset[list(dataset.keys())[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b9528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of evaluation samples: 20000\n",
      "\n",
      "Columns: ['query', 'positive', 'hard_neg']\n"
     ]
    }
   ],
   "source": [
    "# ‚öôÔ∏è C·∫§U H√åNH CHO GOOGLE COLAB T4 GPU (15GB)\n",
    "MAX_SAMPLES = 2000  # Gi·∫£m xu·ªëng 2000 samples ƒë·ªÉ tr√°nh OOM tr√™n T4\n",
    "BATCH_SIZE = 8      # Gi·∫£m batch size xu·ªëng 8 ƒë·ªÉ ti·∫øt ki·ªám memory\n",
    "USE_FP16 = True     # D√πng FP16 (half precision) ƒë·ªÉ gi·∫£m 50% memory\n",
    "\n",
    "print(\"‚öôÔ∏è CONFIGURATION FOR COLAB T4 GPU:\")\n",
    "print(f\"  - Max samples: {MAX_SAMPLES}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Use FP16: {USE_FP16}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(\"\\nüí° N·∫øu v·∫´n OOM, gi·∫£m MAX_SAMPLES xu·ªëng 1000 ho·∫∑c BATCH_SIZE xu·ªëng 4\")\n",
    "\n",
    "# Chu·∫©n b·ªã d·ªØ li·ªáu - l·∫•y split ph√π h·ª£p\n",
    "if 'test' in dataset:\n",
    "    eval_data = dataset['test']\n",
    "elif 'validation' in dataset:\n",
    "    eval_data = dataset['validation']\n",
    "elif 'train' in dataset:\n",
    "    # N·∫øu ch·ªâ c√≥ train, l·∫•y m·ªôt s·ªë m·∫´u ƒë·ªÉ test\n",
    "    eval_data = dataset['train'].select(range(min(MAX_SAMPLES, len(dataset['train']))))\n",
    "else:\n",
    "    # L·∫•y split ƒë·∫ßu ti√™n\n",
    "    split_name = list(dataset.keys())[0]\n",
    "    eval_data = dataset[split_name].select(range(min(MAX_SAMPLES, len(dataset[split_name]))))\n",
    "\n",
    "print(f\"\\n‚úÖ Number of evaluation samples: {len(eval_data)}\")\n",
    "print(f\"‚úÖ Columns: {eval_data.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e26e80",
   "metadata": {},
   "source": [
    "## 4. ƒê·ªãnh nghƒ©a c√°c m√¥ h√¨nh c·∫ßn ƒë√°nh gi√°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to evaluate:\n",
      "  - Vietnamese_Embedding_v2: thanhtantran/Vietnamese_Embedding_v2\n",
      "  - BGE-M3: BAAI/bge-m3\n",
      "  - Multilingual-E5-Large: intfloat/multilingual-e5-large\n"
     ]
    }
   ],
   "source": [
    "# Danh s√°ch c√°c m√¥ h√¨nh c·∫ßn ƒë√°nh gi√°\n",
    "models_to_evaluate = {\n",
    "    \"Vietnamese_Embedding_v2\": \"thanhtantran/Vietnamese_Embedding_v2\",\n",
    "    \"BGE-M3\": \"BAAI/bge-m3\",\n",
    "    \"Multilingual-E5-Large\": \"intfloat/multilingual-e5-large\"\n",
    "}\n",
    "\n",
    "# üî• OPTION: Ch·ªâ ch·∫°y 1 model ƒë·ªÉ tr√°nh OOM\n",
    "# Uncomment d√≤ng d∆∞·ªõi ƒë·ªÉ ch·ªâ test 1 model\n",
    "# models_to_evaluate = {\"Vietnamese_Embedding_v2\": \"thanhtantran/Vietnamese_Embedding_v2\"}\n",
    "# models_to_evaluate = {\"BGE-M3\": \"BAAI/bge-m3\"}\n",
    "# models_to_evaluate = {\"Multilingual-E5-Large\": \"intfloat/multilingual-e5-large\"}\n",
    "\n",
    "print(\"Models to evaluate:\")\n",
    "for name, model_id in models_to_evaluate.items():\n",
    "    print(f\"  - {name}: {model_id}\")\n",
    "print(f\"\\nüìä Total: {len(models_to_evaluate)} model(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f0748",
   "metadata": {},
   "source": [
    "## 5. H√†m ƒë√°nh gi√°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model: SentenceTransformer, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "    \"\"\"T√≠nh embeddings cho m·ªôt list c√°c vƒÉn b·∫£n\"\"\"\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True  # Normalize ƒë·ªÉ t√≠nh cosine similarity\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def evaluate_triplet_ranking(queries_emb: np.ndarray, \n",
    "                             positives_emb: np.ndarray, \n",
    "                             negatives_emb: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"ƒê√°nh gi√° m√¥ h√¨nh d·ª±a tr√™n triplet ranking\n",
    "    \n",
    "    Args:\n",
    "        queries_emb: Embeddings c·ªßa queries\n",
    "        positives_emb: Embeddings c·ªßa positive documents\n",
    "        negatives_emb: Embeddings c·ªßa negative documents\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary ch·ª©a c√°c metrics\n",
    "    \"\"\"\n",
    "    # T√≠nh similarity scores\n",
    "    pos_scores = np.sum(queries_emb * positives_emb, axis=1)  # Cosine similarity (ƒë√£ normalize)\n",
    "    neg_scores = np.sum(queries_emb * negatives_emb, axis=1)\n",
    "    \n",
    "    # Metrics\n",
    "    # 1. Accuracy: T·ª∑ l·ªá positive c√≥ score cao h∆°n negative\n",
    "    accuracy = np.mean(pos_scores > neg_scores)\n",
    "    \n",
    "    # 2. Mean Positive Score\n",
    "    mean_pos_score = np.mean(pos_scores)\n",
    "    \n",
    "    # 3. Mean Negative Score\n",
    "    mean_neg_score = np.mean(neg_scores)\n",
    "    \n",
    "    # 4. Mean Score Difference (margin)\n",
    "    mean_diff = np.mean(pos_scores - neg_scores)\n",
    "    \n",
    "    # 5. MRR (Mean Reciprocal Rank)\n",
    "    # Trong tr∆∞·ªùng h·ª£p triplet, n·∫øu positive rank 1 th√¨ MRR = 1, rank 2 th√¨ MRR = 0.5\n",
    "    ranks = np.where(pos_scores > neg_scores, 1, 2)\n",
    "    mrr = np.mean(1.0 / ranks)\n",
    "    \n",
    "    # 6. Recall@1: T·ª∑ l·ªá positive n·∫±m ·ªü top-1\n",
    "    recall_at_1 = accuracy  # Gi·ªëng accuracy trong tr∆∞·ªùng h·ª£p triplet\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"mean_positive_score\": mean_pos_score,\n",
    "        \"mean_negative_score\": mean_neg_score,\n",
    "        \"mean_score_difference\": mean_diff,\n",
    "        \"mrr\": mrr,\n",
    "        \"recall@1\": recall_at_1\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model_name: str, model_path: str, eval_data, batch_size: int = 16) -> Dict[str, float]:\n",
    "    \"\"\"ƒê√°nh gi√° m·ªôt m√¥ h√¨nh embedding\n",
    "    \n",
    "    Args:\n",
    "        model_name: T√™n m√¥ h√¨nh\n",
    "        model_path: ƒê∆∞·ªùng d·∫´n ho·∫∑c ID c·ªßa m√¥ h√¨nh tr√™n HuggingFace\n",
    "        eval_data: Dataset ƒë·ªÉ ƒë√°nh gi√°\n",
    "        batch_size: Batch size cho encoding\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary ch·ª©a c√°c metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Clear memory tr∆∞·ªõc khi load model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"GPU Memory before loading: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_path, device=device)\n",
    "        \n",
    "        # D√πng half precision (FP16) ƒë·ªÉ gi·∫£m 50% memory tr√™n T4\n",
    "        if device == \"cuda\" and USE_FP16:\n",
    "            model = model.half()\n",
    "            print(\"‚úÖ Using FP16 (half precision) to save memory\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory after loading: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error loading model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Extract texts\n",
    "    queries = [item['query'] for item in eval_data]\n",
    "    positives = [item['positive'] for item in eval_data]\n",
    "    negatives = [item['hard_neg'] for item in eval_data]\n",
    "    \n",
    "    # Compute embeddings\n",
    "    print(f\"\\nComputing embeddings for {len(queries)} samples...\")\n",
    "    print(\"Encoding queries...\")\n",
    "    queries_emb = compute_embeddings(model, queries, batch_size)\n",
    "    \n",
    "    # Clear cache sau m·ªói l·∫ßn encode\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Encoding positives...\")\n",
    "    positives_emb = compute_embeddings(model, positives, batch_size)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Encoding negatives...\")\n",
    "    negatives_emb = compute_embeddings(model, negatives, batch_size)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    metrics = evaluate_triplet_ranking(queries_emb, positives_emb, negatives_emb)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  - MRR: {metrics['mrr']:.4f}\")\n",
    "    print(f\"  - Recall@1: {metrics['recall@1']:.4f}\")\n",
    "    print(f\"  - Mean Positive Score: {metrics['mean_positive_score']:.4f}\")\n",
    "    print(f\"  - Mean Negative Score: {metrics['mean_negative_score']:.4f}\")\n",
    "    print(f\"  - Mean Score Difference: {metrics['mean_score_difference']:.4f}\")\n",
    "    \n",
    "    # Clear memory - QUAN TR·ªåNG!\n",
    "    del model\n",
    "    del queries_emb, positives_emb, negatives_emb\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n‚úÖ Memory cleared after evaluating {model_name}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920f31b",
   "metadata": {},
   "source": [
    "## 6. Ch·∫°y ƒë√°nh gi√° cho t·∫•t c·∫£ c√°c m√¥ h√¨nh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fa5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u k·∫øt qu·∫£\n",
    "results = {}\n",
    "\n",
    "# ƒê√°nh gi√° t·ª´ng m√¥ h√¨nh\n",
    "for model_name, model_path in models_to_evaluate.items():\n",
    "    try:\n",
    "        # Clear memory tr∆∞·ªõc m·ªói model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        metrics = evaluate_model(model_name, model_path, eval_data, batch_size=BATCH_SIZE)\n",
    "        results[model_name] = metrics\n",
    "        \n",
    "        # ƒê·ª£i m·ªôt ch√∫t ƒë·ªÉ GPU cleanup ho√†n to√†n\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n‚ùå Error evaluating {model_name}: {str(e)}\")\n",
    "        print(f\"\\nüí° Tip: Try reducing MAX_SAMPLES or BATCH_SIZE in the configuration cell\")\n",
    "        results[model_name] = None\n",
    "        \n",
    "        # Clear memory sau l·ªói\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluation completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d320c",
   "metadata": {},
   "source": [
    "## 7. So s√°nh k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d75fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# T·∫°o DataFrame ƒë·ªÉ so s√°nh\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results = df_results.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON OF ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string())\n",
    "print(\"\\n\")\n",
    "\n",
    "# T√¨m m√¥ h√¨nh t·ªët nh·∫•t\n",
    "best_model = df_results['accuracy'].idxmax()\n",
    "print(f\"\\nüèÜ BEST MODEL: {best_model}\")\n",
    "print(f\"   Accuracy: {df_results.loc[best_model, 'accuracy']:.4f}\")\n",
    "print(f\"   MRR: {df_results.loc[best_model, 'mrr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b134e",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thi·∫øt l·∫≠p style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# T·∫°o figure v·ªõi subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Embedding Models Evaluation on ViNLI-Zalo Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "ax1 = axes[0, 0]\n",
    "df_results['accuracy'].plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=10)\n",
    "ax1.set_xlabel('Model', fontsize=10)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_results['accuracy']):\n",
    "    ax1.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. MRR comparison\n",
    "ax2 = axes[0, 1]\n",
    "df_results['mrr'].plot(kind='bar', ax=ax2, color='lightcoral', edgecolor='black')\n",
    "ax2.set_title('Mean Reciprocal Rank (MRR)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('MRR', fontsize=10)\n",
    "ax2.set_xlabel('Model', fontsize=10)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_results['mrr']):\n",
    "    ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Score comparison (Positive vs Negative)\n",
    "ax3 = axes[1, 0]\n",
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, df_results['mean_positive_score'], width, label='Positive', color='lightgreen', edgecolor='black')\n",
    "ax3.bar(x + width/2, df_results['mean_negative_score'], width, label='Negative', color='salmon', edgecolor='black')\n",
    "ax3.set_title('Mean Similarity Scores', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Cosine Similarity', fontsize=10)\n",
    "ax3.set_xlabel('Model', fontsize=10)\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(df_results.index, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Score Difference (Margin)\n",
    "ax4 = axes[1, 1]\n",
    "df_results['mean_score_difference'].plot(kind='bar', ax=ax4, color='plum', edgecolor='black')\n",
    "ax4.set_title('Mean Score Difference (Margin)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Score Difference', fontsize=10)\n",
    "ax4.set_xlabel('Model', fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_results['mean_score_difference']):\n",
    "    ax4.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('embedding_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nüìä Visualization saved as 'embedding_evaluation_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38712f79",
   "metadata": {},
   "source": [
    "## 9. Chi ti·∫øt ph√¢n t√≠ch v√† khuy·∫øn ngh·ªã"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6566cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS & RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ranking c√°c m√¥ h√¨nh theo t·ª´ng metric\n",
    "metrics_to_analyze = ['accuracy', 'mrr', 'mean_score_difference', 'mean_positive_score']\n",
    "\n",
    "print(\"\\nüìä Model Rankings by Metrics:\")\n",
    "for metric in metrics_to_analyze:\n",
    "    sorted_models = df_results[metric].sort_values(ascending=False)\n",
    "    print(f\"\\n{metric.upper().replace('_', ' ')}:\")\n",
    "    for i, (model, score) in enumerate(sorted_models.items(), 1):\n",
    "        print(f\"  {i}. {model}: {score:.4f}\")\n",
    "\n",
    "# T√≠nh overall score (weighted average)\n",
    "weights = {\n",
    "    'accuracy': 0.4,\n",
    "    'mrr': 0.3,\n",
    "    'mean_score_difference': 0.2,\n",
    "    'mean_positive_score': 0.1\n",
    "}\n",
    "\n",
    "df_results['overall_score'] = sum(\n",
    "    df_results[metric] * weight \n",
    "    for metric, weight in weights.items()\n",
    ")\n",
    "\n",
    "best_overall = df_results['overall_score'].idxmax()\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(f\"üèÜ FINAL RECOMMENDATION: {best_overall}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nOverall Score: {df_results.loc[best_overall, 'overall_score']:.4f}\")\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  ‚Ä¢ Accuracy: {df_results.loc[best_overall, 'accuracy']:.4f}\")\n",
    "print(f\"  ‚Ä¢ MRR: {df_results.loc[best_overall, 'mrr']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Mean Score Difference: {df_results.loc[best_overall, 'mean_score_difference']:.4f}\")\n",
    "print(f\"  ‚Ä¢ Mean Positive Score: {df_results.loc[best_overall, 'mean_positive_score']:.4f}\")\n",
    "\n",
    "print(f\"\\nüí° Recommendation for Vietnamese Legal Chatbot:\")\n",
    "print(f\"   Use '{best_overall}' as the embedding model for your RAG system.\")\n",
    "print(f\"\\n   Model path: {models_to_evaluate[best_overall]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190de42",
   "metadata": {},
   "source": [
    "## 10. L∆∞u k·∫øt qu·∫£"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# L∆∞u k·∫øt qu·∫£ ra CSV\n",
    "output_file = 'embedding_evaluation_results.csv'\n",
    "df_results.to_csv(output_file)\n",
    "print(f\"\\n‚úÖ Results saved to '{output_file}'\")\n",
    "\n",
    "# L∆∞u khuy·∫øn ngh·ªã\n",
    "recommendation = f\"\"\"\n",
    "EMBEDDING MODEL EVALUATION RESULTS\n",
    "==================================\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Dataset: anti-ai/ViNLI-Zalo-supervised\n",
    "Number of samples: {len(eval_data)}\n",
    "\n",
    "RECOMMENDED MODEL: {best_overall}\n",
    "Model Path: {models_to_evaluate[best_overall]}\n",
    "\n",
    "Performance Metrics:\n",
    "- Accuracy: {df_results.loc[best_overall, 'accuracy']:.4f}\n",
    "- MRR: {df_results.loc[best_overall, 'mrr']:.4f}\n",
    "- Mean Score Difference: {df_results.loc[best_overall, 'mean_score_difference']:.4f}\n",
    "- Overall Score: {df_results.loc[best_overall, 'overall_score']:.4f}\n",
    "\n",
    "ALL MODELS COMPARISON:\n",
    "{df_results.to_string()}\n",
    "\"\"\"\n",
    "\n",
    "with open('embedding_recommendation.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(recommendation)\n",
    "\n",
    "print(\"‚úÖ Recommendation saved to 'embedding_recommendation.txt'\")\n",
    "print(\"\\n\" + recommendation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
