{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e04e953f",
   "metadata": {},
   "source": [
    "# Đánh giá các mô hình Embedding cho hệ thống Legal Chatbot\n",
    "\n",
    "Notebook này đánh giá và so sánh hiệu suất của các mô hình embedding sau:\n",
    "1. **thanhtantran/Vietnamese_Embedding_v2** - Mô hình chuyên cho tiếng Việt\n",
    "2. **Baai/bge-m3** - Mô hình đa ngôn ngữ\n",
    "3. **intfloat/multilingual-e5-large** - Mô hình đa ngôn ngữ lớn\n",
    "\n",
    "Dataset: **anti-ai/ViNLI-Zalo-supervised** (triplet format: query, positive, hard_neg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a9dc923",
   "metadata": {},
   "source": [
    "## 🚀 QUICK START cho Google Colab T4\n",
    "\n",
    "**QUAN TRỌNG - Làm theo thứ tự:**\n",
    "\n",
    "1. ✅ **Runtime → Restart runtime** (để clear memory cũ)\n",
    "2. ✅ Chạy cell \"Cài đặt thư viện\"\n",
    "3. ✅ Chạy cell \"Import thư viện\"\n",
    "4. ✅ Chạy cell \"Clear GPU Memory\"\n",
    "5. ✅ Load dataset\n",
    "6. ✅ **Chọn cấu hình:**\n",
    "   - Chạy cả 3 models: `MAX_SAMPLES=1000, BATCH_SIZE=4`\n",
    "   - Chạy 1 model: `MAX_SAMPLES=2000, BATCH_SIZE=8`\n",
    "7. ✅ Chạy các cell còn lại\n",
    "\n",
    "**Nếu gặp OOM:** Restart runtime và giảm MAX_SAMPLES"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76d29bd0",
   "metadata": {},
   "source": [
    "## 1. Cài đặt thư viện"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "408dae53",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q datasets transformers sentence-transformers torch scikit-learn numpy pandas tqdm matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722ed461",
   "metadata": {},
   "source": [
    "## 2. Import thư viện"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c89f7669",
   "metadata": {},
   "source": [
    "## ⚠️ GPU Memory Management (Quan trọng cho Colab T4!)\n",
    "\n",
    "**HƯỚNG DẪN TRÁNH LỖI \"CUDA OUT OF MEMORY\" TRÊN COLAB T4 (15GB):**\n",
    "\n",
    "### ✅ Bước 1: Restart Runtime trước khi chạy\n",
    "- Menu: **Runtime → Restart runtime**\n",
    "- Hoặc: **Ctrl+M .** (phím tắt)\n",
    "\n",
    "### ✅ Bước 2: Clear GPU memory nếu cần\n",
    "- Chạy cell \"Clear GPU Memory\" ở section 3\n",
    "\n",
    "### ✅ Bước 3: Điều chỉnh cấu hình\n",
    "**Trong cell config (section 3):**\n",
    "```python\n",
    "MAX_SAMPLES = 2000   # Giảm xuống 1000 nếu vẫn OOM\n",
    "BATCH_SIZE = 8       # Giảm xuống 4 nếu vẫn OOM  \n",
    "USE_FP16 = True      # BẮT BUỘC phải True\n",
    "```\n",
    "\n",
    "### ✅ Bước 4: Chạy từng model một\n",
    "**Trong cell \"Định nghĩa các mô hình\" (section 4):**\n",
    "- Uncomment một trong các dòng để chỉ chạy 1 model\n",
    "- Ví dụ: `models_to_evaluate = {\"BGE-M3\": \"BAAI/bge-m3\"}`\n",
    "\n",
    "### 📊 Khuyến nghị cấu hình cho Colab T4:\n",
    "- **Chạy tất cả 3 models:** MAX_SAMPLES=1000, BATCH_SIZE=4, USE_FP16=True\n",
    "- **Chạy 1 model:** MAX_SAMPLES=2000, BATCH_SIZE=8, USE_FP16=True\n",
    "- **An toàn nhất:** MAX_SAMPLES=500, BATCH_SIZE=4, USE_FP16=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41f60ad7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mikeethanh/anaconda3/envs/dl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from tqdm.auto import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import List, Dict, Tuple\n",
    "import warnings\n",
    "import gc\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Thiết lập device\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Thiết lập memory management cho CUDA\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Total GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.2f} GB\")\n",
    "    print(f\"Allocated GPU Memory: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"Cached GPU Memory: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28980581",
   "metadata": {},
   "source": [
    "## 3. Load Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693477b8",
   "metadata": {},
   "source": [
    "## 🔄 Clear GPU Memory (Chạy cell này nếu gặp OOM)\n",
    "\n",
    "**Nếu bạn gặp lỗi \"CUDA out of memory\", chạy cell dưới đây để clear memory:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ef785cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clear tất cả GPU memory\n",
    "import torch\n",
    "import gc\n",
    "\n",
    "# Kill tất cả process đang dùng GPU\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    # Force clear all allocated memory\n",
    "    torch.cuda.synchronize()\n",
    "    \n",
    "    print(\"✅ GPU Memory cleared!\")\n",
    "    print(f\"GPU Memory allocated: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    print(f\"GPU Memory cached: {torch.cuda.memory_reserved(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "# Nếu vẫn không được, restart runtime:\n",
    "# Runtime -> Restart runtime (trong Colab menu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "225dddc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 32980/32980 [00:00<00:00, 45453.00 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset splits: dict_keys(['train'])\n",
      "\n",
      "Sample data:\n",
      "{'query': 'Tổ sát_hạch cấp giấy_phép lái tàu_hỏa có bao_nhiêu thành_viên ?', 'positive': 'Điều 30 . Tổ sát_hạch 1 . Tổ sát_hạch do Cục_trưởng Cục Đường_sắt Việt_Nam thành_lập , chịu sự chỉ_đạo trực_tiếp của Hội_đồng sát_hạch . \\n 2 . Tổ sát_hạch có ít_nhất 05 thành_viên , bao_gồm tổ_trưởng , các sát_hạch viên lý_thuyết và sát_hạch viên thực_hành . Tổ_trưởng Tổ sát_hạch là công_chức Cục Đường_sắt Việt_Nam hoặc lãnh_đạo doanh_nghiệp có thí_sinh dự kỳ sát_hạch , các sát_hạch viên là người đang công_tác tại doanh_nghiệp có thí_sinh tham_dự kỳ sát_hạch và người đang công_tác tại các cơ_sở đào_tạo liên_quan đến lái tàu . \\n 3 . Tiêu_chuẩn của sát_hạch viên : \\n a ) Có tư_cách đạo_đức tốt và có chuyên_môn phù_hợp ; \\n b ) Đã qua khóa huấn_luyện về nghiệp_vụ sát_hạch lái tàu do Cục Đường_sắt Việt_Nam tổ_chức và được cấp thẻ sát_hạch viên ; \\n c ) Sát_hạch viên lý_thuyết phải tốt_nghiệp đại_học trở lên chuyên_ngành phù_hợp nội_dung sát_hạch , có ít_nhất 03 năm kinh_nghiệm công_tác liên_quan đến lái tàu ; \\n d ) Sát_hạch viên thực_hành phải tốt_nghiệp trình_độ trung_cấp lái tàu trở lên ; có ít_nhất 03 năm kinh_nghiệm trực_tiếp đảm_nhận chức_danh lái tàu , riêng sát_hạch viên lái tàu đường_sắt đô_thị phải có ít_nhất 01 năm kinh_nghiệm trực_tiếp đảm_nhận chức_danh lái tàu đường_sắt đô_thị . \\n 4 . Nội_dung bồi_dưỡng nghiệp_vụ sát_hạch viên lái tàu được quy_định tại Phụ_lục III ban_hành kèm theo Thông_tư này . \\n 5 . Nhiệm_vụ , quyền_hạn của Tổ sát_hạch : \\n a ) Giúp Hội_đồng sát_hạch xây_dựng nội_dung sát_hạch trình Cục Đường_sắt Việt_Nam phê_duyệt ; \\n b ) Kiểm_tra tiêu_chuẩn , quy_cách của phương_tiện , trang thiết_bị chuyên_môn phục_vụ sát_hạch và phương_án bảo_đảm an_toàn cho kỳ sát_hạch ; \\n c ) Phổ_biến nội_dung , quy_trình sát_hạch và kiểm_tra việc chấp_hành nội_quy sát_hạch ; \\n d ) Chấm thi và tổng_hợp kết_quả kỳ sát_hạch để báo_cáo Hội_đồng sát_hạch ; \\n đ ) Sát_hạch viên chịu trách_nhiệm trực_tiếp về kết_quả chấm thi . Tổ_trưởng Tổ sát_hạch chịu trách_nhiệm chung về kết_quả sát_hạch ; \\n e ) Lập biên_bản , xử_lý các trường_hợp vi_phạm nội_quy sát_hạch theo quyền_hạn được giao hoặc báo_cáo Chủ_tịch , Hội_đồng sát_hạch giải_quyết nếu vượt thẩm_quyền . \\n 6 . Tổ sát_hạch tự giải_thể khi hoàn_thành nhiệm_vụ .', 'hard_neg': 'Điều 10 . Xác_định nội_dung đăng_ký lại khai_sinh 1 . Trường_hợp người yêu_cầu đăng_ký lại khai_sinh có giấy_tờ theo quy_định tại khoản 4 Điều 26 Nghị_định số 123 / 2015 / NĐ - CP và khoản 1 , khoản 2 Điều 9 Thông_tư này thì nội_dung đăng_ký lại khai_sinh được xác_định theo giấy_tờ đó . \\n 2 . Tại thời_điểm đăng_ký lại khai_sinh , nếu thông_tin về cha , mẹ và của bản_thân người yêu_cầu đăng_ký lại khai_sinh có thay_đổi so với nội_dung giấy_tờ tại khoản 1 Điều này , thì người đó có trách_nhiệm xuất_trình giấy_tờ hợp_lệ chứng_minh việc thay_đổi . Nếu việc thay_đổi thông_tin là phù_hợp với quy_định pháp_luật thì nội_dung đăng_ký lại khai_sinh được xác_định theo thông_tin thay_đổi ; nội_dung thông_tin trước khi thay_đổi được ghi vào mặt sau của Giấy khai_sinh và mục “ Ghi_chú ” trong Sổ đăng_ký khai_sinh . \\n Trường_hợp cha , mẹ của người yêu_cầu đăng_ký lại khai_sinh đã chết thì mục “ Nơi cư_trú ” trong Sổ đăng_ký khai_sinh , Giấy khai_sinh ghi : “ Đã chết ” . \\n Trường_hợp địa_danh hành_chính đã có sự thay_đổi so với địa_danh ghi trong giấy_tờ được cấp trước đây thì ghi theo địa_danh hành_chính hiện_tại ; địa_danh hành_chính trước đây được ghi vào mặt sau của Giấy khai_sinh và mục Ghi_chú trong Sổ đăng_ký khai_sinh .'}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Load dataset từ HuggingFace\n",
    "print(\"Loading dataset...\")\n",
    "dataset = load_dataset(\"anti-ai/ViNLI-Zalo-supervised\")\n",
    "\n",
    "# Kiểm tra cấu trúc dataset\n",
    "print(f\"\\nDataset splits: {dataset.keys()}\")\n",
    "print(f\"\\nSample data:\")\n",
    "print(dataset['train'][0] if 'train' in dataset else dataset[list(dataset.keys())[0]][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b9528",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of evaluation samples: 20000\n",
      "\n",
      "Columns: ['query', 'positive', 'hard_neg']\n"
     ]
    }
   ],
   "source": [
    "# ⚙️ CẤU HÌNH CHO GOOGLE COLAB T4 GPU (15GB)\n",
    "MAX_SAMPLES = 2000  # Giảm xuống 2000 samples để tránh OOM trên T4\n",
    "BATCH_SIZE = 8      # Giảm batch size xuống 8 để tiết kiệm memory\n",
    "USE_FP16 = True     # Dùng FP16 (half precision) để giảm 50% memory\n",
    "\n",
    "print(\"⚙️ CONFIGURATION FOR COLAB T4 GPU:\")\n",
    "print(f\"  - Max samples: {MAX_SAMPLES}\")\n",
    "print(f\"  - Batch size: {BATCH_SIZE}\")\n",
    "print(f\"  - Use FP16: {USE_FP16}\")\n",
    "print(f\"  - Device: {device}\")\n",
    "print(\"\\n💡 Nếu vẫn OOM, giảm MAX_SAMPLES xuống 1000 hoặc BATCH_SIZE xuống 4\")\n",
    "\n",
    "# Chuẩn bị dữ liệu - lấy split phù hợp\n",
    "if 'test' in dataset:\n",
    "    eval_data = dataset['test']\n",
    "elif 'validation' in dataset:\n",
    "    eval_data = dataset['validation']\n",
    "elif 'train' in dataset:\n",
    "    # Nếu chỉ có train, lấy một số mẫu để test\n",
    "    eval_data = dataset['train'].select(range(min(MAX_SAMPLES, len(dataset['train']))))\n",
    "else:\n",
    "    # Lấy split đầu tiên\n",
    "    split_name = list(dataset.keys())[0]\n",
    "    eval_data = dataset[split_name].select(range(min(MAX_SAMPLES, len(dataset[split_name]))))\n",
    "\n",
    "print(f\"\\n✅ Number of evaluation samples: {len(eval_data)}\")\n",
    "print(f\"✅ Columns: {eval_data.column_names}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2e26e80",
   "metadata": {},
   "source": [
    "## 4. Định nghĩa các mô hình cần đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea68a32e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models to evaluate:\n",
      "  - Vietnamese_Embedding_v2: thanhtantran/Vietnamese_Embedding_v2\n",
      "  - BGE-M3: BAAI/bge-m3\n",
      "  - Multilingual-E5-Large: intfloat/multilingual-e5-large\n"
     ]
    }
   ],
   "source": [
    "# Danh sách các mô hình cần đánh giá\n",
    "models_to_evaluate = {\n",
    "    \"Vietnamese_Embedding_v2\": \"thanhtantran/Vietnamese_Embedding_v2\",\n",
    "    \"BGE-M3\": \"BAAI/bge-m3\",\n",
    "    \"Multilingual-E5-Large\": \"intfloat/multilingual-e5-large\"\n",
    "}\n",
    "\n",
    "# 🔥 OPTION: Chỉ chạy 1 model để tránh OOM\n",
    "# Uncomment dòng dưới để chỉ test 1 model\n",
    "# models_to_evaluate = {\"Vietnamese_Embedding_v2\": \"thanhtantran/Vietnamese_Embedding_v2\"}\n",
    "# models_to_evaluate = {\"BGE-M3\": \"BAAI/bge-m3\"}\n",
    "# models_to_evaluate = {\"Multilingual-E5-Large\": \"intfloat/multilingual-e5-large\"}\n",
    "\n",
    "print(\"Models to evaluate:\")\n",
    "for name, model_id in models_to_evaluate.items():\n",
    "    print(f\"  - {name}: {model_id}\")\n",
    "print(f\"\\n📊 Total: {len(models_to_evaluate)} model(s)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "255f0748",
   "metadata": {},
   "source": [
    "## 5. Hàm đánh giá"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec26a59a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embeddings(model: SentenceTransformer, texts: List[str], batch_size: int = 16) -> np.ndarray:\n",
    "    \"\"\"Tính embeddings cho một list các văn bản\"\"\"\n",
    "    embeddings = model.encode(\n",
    "        texts,\n",
    "        batch_size=batch_size,\n",
    "        show_progress_bar=True,\n",
    "        convert_to_numpy=True,\n",
    "        normalize_embeddings=True  # Normalize để tính cosine similarity\n",
    "    )\n",
    "    return embeddings\n",
    "\n",
    "\n",
    "def evaluate_triplet_ranking(queries_emb: np.ndarray, \n",
    "                             positives_emb: np.ndarray, \n",
    "                             negatives_emb: np.ndarray) -> Dict[str, float]:\n",
    "    \"\"\"Đánh giá mô hình dựa trên triplet ranking\n",
    "    \n",
    "    Args:\n",
    "        queries_emb: Embeddings của queries\n",
    "        positives_emb: Embeddings của positive documents\n",
    "        negatives_emb: Embeddings của negative documents\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary chứa các metrics\n",
    "    \"\"\"\n",
    "    # Tính similarity scores\n",
    "    pos_scores = np.sum(queries_emb * positives_emb, axis=1)  # Cosine similarity (đã normalize)\n",
    "    neg_scores = np.sum(queries_emb * negatives_emb, axis=1)\n",
    "    \n",
    "    # Metrics\n",
    "    # 1. Accuracy: Tỷ lệ positive có score cao hơn negative\n",
    "    accuracy = np.mean(pos_scores > neg_scores)\n",
    "    \n",
    "    # 2. Mean Positive Score\n",
    "    mean_pos_score = np.mean(pos_scores)\n",
    "    \n",
    "    # 3. Mean Negative Score\n",
    "    mean_neg_score = np.mean(neg_scores)\n",
    "    \n",
    "    # 4. Mean Score Difference (margin)\n",
    "    mean_diff = np.mean(pos_scores - neg_scores)\n",
    "    \n",
    "    # 5. MRR (Mean Reciprocal Rank)\n",
    "    # Trong trường hợp triplet, nếu positive rank 1 thì MRR = 1, rank 2 thì MRR = 0.5\n",
    "    ranks = np.where(pos_scores > neg_scores, 1, 2)\n",
    "    mrr = np.mean(1.0 / ranks)\n",
    "    \n",
    "    # 6. Recall@1: Tỷ lệ positive nằm ở top-1\n",
    "    recall_at_1 = accuracy  # Giống accuracy trong trường hợp triplet\n",
    "    \n",
    "    return {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"mean_positive_score\": mean_pos_score,\n",
    "        \"mean_negative_score\": mean_neg_score,\n",
    "        \"mean_score_difference\": mean_diff,\n",
    "        \"mrr\": mrr,\n",
    "        \"recall@1\": recall_at_1\n",
    "    }\n",
    "\n",
    "\n",
    "def evaluate_model(model_name: str, model_path: str, eval_data, batch_size: int = 16) -> Dict[str, float]:\n",
    "    \"\"\"Đánh giá một mô hình embedding\n",
    "    \n",
    "    Args:\n",
    "        model_name: Tên mô hình\n",
    "        model_path: Đường dẫn hoặc ID của mô hình trên HuggingFace\n",
    "        eval_data: Dataset để đánh giá\n",
    "        batch_size: Batch size cho encoding\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary chứa các metrics\n",
    "    \"\"\"\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Evaluating: {model_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Clear memory trước khi load model\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        print(f\"GPU Memory before loading: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    # Load model\n",
    "    print(f\"Loading model from {model_path}...\")\n",
    "    try:\n",
    "        model = SentenceTransformer(model_path, device=device)\n",
    "        \n",
    "        # Dùng half precision (FP16) để giảm 50% memory trên T4\n",
    "        if device == \"cuda\" and USE_FP16:\n",
    "            model = model.half()\n",
    "            print(\"✅ Using FP16 (half precision) to save memory\")\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            print(f\"GPU Memory after loading: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error loading model: {e}\")\n",
    "        raise\n",
    "    \n",
    "    # Extract texts\n",
    "    queries = [item['query'] for item in eval_data]\n",
    "    positives = [item['positive'] for item in eval_data]\n",
    "    negatives = [item['hard_neg'] for item in eval_data]\n",
    "    \n",
    "    # Compute embeddings\n",
    "    print(f\"\\nComputing embeddings for {len(queries)} samples...\")\n",
    "    print(\"Encoding queries...\")\n",
    "    queries_emb = compute_embeddings(model, queries, batch_size)\n",
    "    \n",
    "    # Clear cache sau mỗi lần encode\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Encoding positives...\")\n",
    "    positives_emb = compute_embeddings(model, positives, batch_size)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    print(\"Encoding negatives...\")\n",
    "    negatives_emb = compute_embeddings(model, negatives, batch_size)\n",
    "    \n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    \n",
    "    # Evaluate\n",
    "    print(\"\\nCalculating metrics...\")\n",
    "    metrics = evaluate_triplet_ranking(queries_emb, positives_emb, negatives_emb)\n",
    "    \n",
    "    # Print results\n",
    "    print(f\"\\n{model_name} Results:\")\n",
    "    print(f\"  - Accuracy: {metrics['accuracy']:.4f}\")\n",
    "    print(f\"  - MRR: {metrics['mrr']:.4f}\")\n",
    "    print(f\"  - Recall@1: {metrics['recall@1']:.4f}\")\n",
    "    print(f\"  - Mean Positive Score: {metrics['mean_positive_score']:.4f}\")\n",
    "    print(f\"  - Mean Negative Score: {metrics['mean_negative_score']:.4f}\")\n",
    "    print(f\"  - Mean Score Difference: {metrics['mean_score_difference']:.4f}\")\n",
    "    \n",
    "    # Clear memory - QUAN TRỌNG!\n",
    "    del model\n",
    "    del queries_emb, positives_emb, negatives_emb\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    print(f\"\\n✅ Memory cleared after evaluating {model_name}\")\n",
    "    if torch.cuda.is_available():\n",
    "        print(f\"GPU Memory after cleanup: {torch.cuda.memory_allocated(0) / 1024**3:.2f} GB\")\n",
    "    \n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6920f31b",
   "metadata": {},
   "source": [
    "## 6. Chạy đánh giá cho tất cả các mô hình"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b80fa5f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu kết quả\n",
    "results = {}\n",
    "\n",
    "# Đánh giá từng mô hình\n",
    "for model_name, model_path in models_to_evaluate.items():\n",
    "    try:\n",
    "        # Clear memory trước mỗi model\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "        \n",
    "        metrics = evaluate_model(model_name, model_path, eval_data, batch_size=BATCH_SIZE)\n",
    "        results[model_name] = metrics\n",
    "        \n",
    "        # Đợi một chút để GPU cleanup hoàn toàn\n",
    "        import time\n",
    "        time.sleep(2)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"\\n❌ Error evaluating {model_name}: {str(e)}\")\n",
    "        print(f\"\\n💡 Tip: Try reducing MAX_SAMPLES or BATCH_SIZE in the configuration cell\")\n",
    "        results[model_name] = None\n",
    "        \n",
    "        # Clear memory sau lỗi\n",
    "        if torch.cuda.is_available():\n",
    "            torch.cuda.empty_cache()\n",
    "        gc.collect()\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"Evaluation completed!\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2d320c",
   "metadata": {},
   "source": [
    "## 7. So sánh kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d75fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tạo DataFrame để so sánh\n",
    "df_results = pd.DataFrame(results).T\n",
    "df_results = df_results.sort_values('accuracy', ascending=False)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPARISON OF ALL MODELS\")\n",
    "print(\"=\"*80)\n",
    "print(df_results.to_string())\n",
    "print(\"\\n\")\n",
    "\n",
    "# Tìm mô hình tốt nhất\n",
    "best_model = df_results['accuracy'].idxmax()\n",
    "print(f\"\\n🏆 BEST MODEL: {best_model}\")\n",
    "print(f\"   Accuracy: {df_results.loc[best_model, 'accuracy']:.4f}\")\n",
    "print(f\"   MRR: {df_results.loc[best_model, 'mrr']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "787b134e",
   "metadata": {},
   "source": [
    "## 8. Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c26d1881",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Thiết lập style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "# Tạo figure với subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "fig.suptitle('Embedding Models Evaluation on ViNLI-Zalo Dataset', fontsize=16, fontweight='bold')\n",
    "\n",
    "# 1. Accuracy comparison\n",
    "ax1 = axes[0, 0]\n",
    "df_results['accuracy'].plot(kind='bar', ax=ax1, color='skyblue', edgecolor='black')\n",
    "ax1.set_title('Accuracy Comparison', fontsize=12, fontweight='bold')\n",
    "ax1.set_ylabel('Accuracy', fontsize=10)\n",
    "ax1.set_xlabel('Model', fontsize=10)\n",
    "ax1.set_ylim([0, 1])\n",
    "ax1.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_results['accuracy']):\n",
    "    ax1.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 2. MRR comparison\n",
    "ax2 = axes[0, 1]\n",
    "df_results['mrr'].plot(kind='bar', ax=ax2, color='lightcoral', edgecolor='black')\n",
    "ax2.set_title('Mean Reciprocal Rank (MRR)', fontsize=12, fontweight='bold')\n",
    "ax2.set_ylabel('MRR', fontsize=10)\n",
    "ax2.set_xlabel('Model', fontsize=10)\n",
    "ax2.set_ylim([0, 1])\n",
    "ax2.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_results['mrr']):\n",
    "    ax2.text(i, v + 0.02, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "# 3. Score comparison (Positive vs Negative)\n",
    "ax3 = axes[1, 0]\n",
    "x = np.arange(len(df_results))\n",
    "width = 0.35\n",
    "ax3.bar(x - width/2, df_results['mean_positive_score'], width, label='Positive', color='lightgreen', edgecolor='black')\n",
    "ax3.bar(x + width/2, df_results['mean_negative_score'], width, label='Negative', color='salmon', edgecolor='black')\n",
    "ax3.set_title('Mean Similarity Scores', fontsize=12, fontweight='bold')\n",
    "ax3.set_ylabel('Cosine Similarity', fontsize=10)\n",
    "ax3.set_xlabel('Model', fontsize=10)\n",
    "ax3.set_xticks(x)\n",
    "ax3.set_xticklabels(df_results.index, rotation=45, ha='right')\n",
    "ax3.legend()\n",
    "ax3.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# 4. Score Difference (Margin)\n",
    "ax4 = axes[1, 1]\n",
    "df_results['mean_score_difference'].plot(kind='bar', ax=ax4, color='plum', edgecolor='black')\n",
    "ax4.set_title('Mean Score Difference (Margin)', fontsize=12, fontweight='bold')\n",
    "ax4.set_ylabel('Score Difference', fontsize=10)\n",
    "ax4.set_xlabel('Model', fontsize=10)\n",
    "ax4.grid(axis='y', alpha=0.3)\n",
    "for i, v in enumerate(df_results['mean_score_difference']):\n",
    "    ax4.text(i, v + 0.01, f'{v:.3f}', ha='center', fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('embedding_evaluation_results.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\n📊 Visualization saved as 'embedding_evaluation_results.png'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38712f79",
   "metadata": {},
   "source": [
    "## 9. Chi tiết phân tích và khuyến nghị"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6566cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"DETAILED ANALYSIS & RECOMMENDATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Ranking các mô hình theo từng metric\n",
    "metrics_to_analyze = ['accuracy', 'mrr', 'mean_score_difference', 'mean_positive_score']\n",
    "\n",
    "print(\"\\n📊 Model Rankings by Metrics:\")\n",
    "for metric in metrics_to_analyze:\n",
    "    sorted_models = df_results[metric].sort_values(ascending=False)\n",
    "    print(f\"\\n{metric.upper().replace('_', ' ')}:\")\n",
    "    for i, (model, score) in enumerate(sorted_models.items(), 1):\n",
    "        print(f\"  {i}. {model}: {score:.4f}\")\n",
    "\n",
    "# Tính overall score (weighted average)\n",
    "weights = {\n",
    "    'accuracy': 0.4,\n",
    "    'mrr': 0.3,\n",
    "    'mean_score_difference': 0.2,\n",
    "    'mean_positive_score': 0.1\n",
    "}\n",
    "\n",
    "df_results['overall_score'] = sum(\n",
    "    df_results[metric] * weight \n",
    "    for metric, weight in weights.items()\n",
    ")\n",
    "\n",
    "best_overall = df_results['overall_score'].idxmax()\n",
    "\n",
    "print(f\"\\n\\n{'='*80}\")\n",
    "print(f\"🏆 FINAL RECOMMENDATION: {best_overall}\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\nOverall Score: {df_results.loc[best_overall, 'overall_score']:.4f}\")\n",
    "print(f\"\\nKey Metrics:\")\n",
    "print(f\"  • Accuracy: {df_results.loc[best_overall, 'accuracy']:.4f}\")\n",
    "print(f\"  • MRR: {df_results.loc[best_overall, 'mrr']:.4f}\")\n",
    "print(f\"  • Mean Score Difference: {df_results.loc[best_overall, 'mean_score_difference']:.4f}\")\n",
    "print(f\"  • Mean Positive Score: {df_results.loc[best_overall, 'mean_positive_score']:.4f}\")\n",
    "\n",
    "print(f\"\\n💡 Recommendation for Vietnamese Legal Chatbot:\")\n",
    "print(f\"   Use '{best_overall}' as the embedding model for your RAG system.\")\n",
    "print(f\"\\n   Model path: {models_to_evaluate[best_overall]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7190de42",
   "metadata": {},
   "source": [
    "## 10. Lưu kết quả"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e8d9464",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lưu kết quả ra CSV\n",
    "output_file = 'embedding_evaluation_results.csv'\n",
    "df_results.to_csv(output_file)\n",
    "print(f\"\\n✅ Results saved to '{output_file}'\")\n",
    "\n",
    "# Lưu khuyến nghị\n",
    "recommendation = f\"\"\"\n",
    "EMBEDDING MODEL EVALUATION RESULTS\n",
    "==================================\n",
    "Date: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Dataset: anti-ai/ViNLI-Zalo-supervised\n",
    "Number of samples: {len(eval_data)}\n",
    "\n",
    "RECOMMENDED MODEL: {best_overall}\n",
    "Model Path: {models_to_evaluate[best_overall]}\n",
    "\n",
    "Performance Metrics:\n",
    "- Accuracy: {df_results.loc[best_overall, 'accuracy']:.4f}\n",
    "- MRR: {df_results.loc[best_overall, 'mrr']:.4f}\n",
    "- Mean Score Difference: {df_results.loc[best_overall, 'mean_score_difference']:.4f}\n",
    "- Overall Score: {df_results.loc[best_overall, 'overall_score']:.4f}\n",
    "\n",
    "ALL MODELS COMPARISON:\n",
    "{df_results.to_string()}\n",
    "\"\"\"\n",
    "\n",
    "with open('embedding_recommendation.txt', 'w', encoding='utf-8') as f:\n",
    "    f.write(recommendation)\n",
    "\n",
    "print(\"✅ Recommendation saved to 'embedding_recommendation.txt'\")\n",
    "print(\"\\n\" + recommendation)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.24"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
