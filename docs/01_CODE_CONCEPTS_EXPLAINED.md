# Code Concepts Deep Dive - Train Llama Script

## M·ª•c l·ª•c
1. [Dataclass vs Class th∆∞·ªùng](#1-dataclass-vs-class-th∆∞·ªùng)
2. [Type Hints v√† Optional](#2-type-hints-v√†-optional)
3. [Class Inheritance v√† Composition](#3-class-inheritance-v√†-composition)
4. [Path Object](#4-path-object)
5. [Advanced Concepts](#5-advanced-concepts)

---

## 1. Dataclass vs Class th∆∞·ªùng

### Class th∆∞·ªùng (Traditional Class)
```python
class FineTuneConfig:
    def __init__(self, model_name, max_seq_length, dtype, load_in_4bit):
        self.model_name = model_name
        self.max_seq_length = max_seq_length
        self.dtype = dtype
        self.load_in_4bit = load_in_4bit
```

### Dataclass (Modern Python)
```python
@dataclass
class FineTuneConfig:
    model_name: str = "unsloth/Llama-3.1-8B-Instruct"
    max_seq_length: int = 8192
    dtype: Optional[torch.dtype] = None
    load_in_4bit: bool = False
```

### T·∫°i sao d√πng `@dataclass`?

#### **∆Øu ƒëi·ªÉm:**

1. **T·ª± ƒë·ªông sinh `__init__`**: Kh√¥ng c·∫ßn vi·∫øt constructor th·ªß c√¥ng
   ```python
   # Dataclass t·ª± ƒë·ªông t·∫°o:
   def __init__(self, model_name="unsloth/Llama-3.1-8B-Instruct", 
                max_seq_length=8192, ...):
       self.model_name = model_name
       self.max_seq_length = max_seq_length
       # ...
   ```

2. **T·ª± ƒë·ªông sinh `__repr__`**: In object d·ªÖ ƒë·ªçc
   ```python
   config = FineTuneConfig()
   print(config)
   # Output: FineTuneConfig(model_name='unsloth/Llama-3.1-8B-Instruct', 
   #                        max_seq_length=8192, ...)
   ```

3. **T·ª± ƒë·ªông sinh `__eq__`**: So s√°nh objects
   ```python
   config1 = FineTuneConfig()
   config2 = FineTuneConfig()
   print(config1 == config2)  # True
   ```

4. **Type hints r√µ r√†ng**: D·ªÖ debug, IDE h·ªó tr·ª£ autocomplete t·ªët h∆°n

5. **Default values**: Gi√° tr·ªã m·∫∑c ƒë·ªãnh ngay trong ƒë·ªãnh nghƒ©a class

#### **Khi n√†o d√πng dataclass?**
- **Configuration classes**: L∆∞u tr·ªØ config, settings
- **Data containers**: Ch·ª©a data kh√¥ng c√≥ logic ph·ª©c t·∫°p

#### **Khi n√†o d√πng class th∆∞·ªùng?**
- **Logic ph·ª©c t·∫°p**: Nhi·ªÅu methods, business logic
- **Inheritance ph·ª©c t·∫°p**: K·∫ø th·ª´a nhi·ªÅu t·∫ßng
- **Validation logic**: C·∫ßn ki·ªÉm so√°t ch·∫∑t ch·∫Ω vi·ªác kh·ªüi t·∫°o
---

## 2. Type Hints v√† Optional

### `Optional[torch.dtype]` l√† g√¨?

```python
from typing import Optional
import torch

dtype: Optional[torch.dtype] = None
```

#### **Ph√¢n t√≠ch:**

1. **`Optional[X]`** = `Union[X, None]`
   ```python
   # Hai c√°ch vi·∫øt t∆∞∆°ng ƒë∆∞∆°ng:
   dtype: Optional[torch.dtype] = None
   dtype: Union[torch.dtype, None] = None
   ```

2. **√ù nghƒ©a**:
   - Bi·∫øn n√†y c√≥ th·ªÉ l√† `torch.dtype` (vd: `torch.float16`, `torch.bfloat16`)
   - Ho·∫∑c c√≥ th·ªÉ l√† `None` (kh√¥ng ch·ªâ ƒë·ªãnh)

3. **T·∫°i sao c·∫ßn `Optional`?**
   ```python
   # Tr∆∞·ªùng h·ª£p 1: User ch·ªâ ƒë·ªãnh dtype r√µ r√†ng
   config = FineTuneConfig(dtype=torch.bfloat16)
   
   # Tr∆∞·ªùng h·ª£p 2: Auto-detect (None)
   config = FineTuneConfig(dtype=None)  # Unsloth s·∫Ω t·ª± ch·ªçn dtype ph√π h·ª£p
   ```

#### **C√°c ki·ªÉu dtype trong PyTorch:**
```python
torch.float32  # chinh xac - toc do cham 
torch.float16  # giam chinh xac - tang toc do
torch.bfloat16 # can bang giua ca 2
torch.int8     
```

#### **Type Hints kh√°c trong code:**
```python
# String type
model_name: str = "..."

# Integer type
max_seq_length: int = 8192

# Boolean type
load_in_4bit: bool = False

# Float type
lora_dropout: float = 0.0

# Dictionary type
def load_datasets(self) -> Dict[str, Dataset]:
    # Tr·∫£ v·ªÅ dictionary v·ªõi key l√† string, value l√† Dataset
    return {"train": dataset1, "val": dataset2}

# Callable type (function)
field(default_factory=lambda: f"vietnamese-legal-llama-...")
# default_factory nh·∫≠n m·ªôt callable (function) ƒë·ªÉ t·∫°o gi√° tr·ªã m·∫∑c ƒë·ªãnh
```
---

## 3. Class Inheritance v√† Composition

### Constructor `__init__` 

```python
class LlamaFineTuner:
    def __init__(self, config: FineTuneConfig, data_dir: str):
        self.config = config
        self.data_dir = Path(data_dir)
        # ...
```

#### **Ph√¢n t√≠ch:**

1. **`def __init__(self, config: FineTuneConfig, data_dir: str)`**
   - `self`: Tham chi·∫øu ƒë·∫øn instance c·ªßa class
   - `config: FineTuneConfig`: Parameter v·ªõi type hint 
   - `data_dir: str`: Parameter ki·ªÉu string

2. **ƒê√¢y l√† COMPOSITION, kh√¥ng ph·∫£i INHERITANCE**
   ```python
   # COMPOSITION (Has-a relationship)
   class LlamaFineTuner:
       def __init__(self, config: FineTuneConfig):
           self.config = config  # LlamaFineTuner "c√≥" m·ªôt FineTuneConfig
   
   # INHERITANCE (Is-a relationship) - V√ç D·ª§
   class LlamaFineTuner(BaseTrainer):  # LlamaFineTuner "l√†" m·ªôt BaseTrainer
       def __init__(self):
           super().__init__()  # G·ªçi constructor c·ªßa BaseTrainer
   ```

### T·∫°i sao d√πng separate Config class?

#### **L√Ω do 1: Separation of Concerns**
```python
# ‚ùå BAD: T·∫•t c·∫£ trong m·ªôt class
class LlamaFineTuner:
    def __init__(self, model_name, max_seq_length, lora_r, lora_alpha, 
                 batch_size, learning_rate, ...):  # 30+ parameters!
        self.model_name = model_name
        # ... r·∫•t d√†i v√† kh√≥ ƒë·ªçc

# ‚úÖ GOOD: T√°ch bi·ªát config v√† logic
class FineTuneConfig:
    # Ch·ªâ ch·ª©a configuration
    model_name: str = "..."
    lora_r: int = 128

class LlamaFineTuner:
    # Ch·ªâ ch·ª©a training logic
    def __init__(self, config: FineTuneConfig):
        self.config = config
```

#### **L√Ω do 2: Reusability**
```python
# T·∫°o nhi·ªÅu configs kh√°c nhau
config_h200 = FineTuneConfig(lora_r=128, batch_size=32)
config_a100 = FineTuneConfig(lora_r=64, batch_size=16)
config_t4 = FineTuneConfig(lora_r=32, batch_size=8)

# D√πng l·∫°i trainer v·ªõi configs kh√°c nhau
trainer1 = LlamaFineTuner(config_h200, data_dir)
trainer2 = LlamaFineTuner(config_a100, data_dir)
```

#### **L√Ω do 4: Validation**
```python
@dataclass
class FineTuneConfig:
    lora_r: int = 128
    
    def __post_init__(self):
        # Validate sau khi init
        if self.lora_r < 8 or self.lora_r > 256:
            raise ValueError("lora_r must be between 8 and 256")
```

---

## 4. Path Object

### `Path` vs `str` - T·∫°i sao d√πng `Path`?

```python
from pathlib import Path

# Code trong script
self.data_dir = Path(data_dir)
self.output_dir = Path(config.output_dir)
```

### So s√°nh `str` vs `Path`

#### **String (c√°ch c≈©)**
```python
import os

# ‚ùå D√πng string - ph·ª©c t·∫°p, d·ªÖ l·ªói
data_dir = "/home/user/data"
config_file = data_dir + "/" + "config.json"  # Ugly concatenation
if os.path.exists(config_file):
    with open(config_file, 'r') as f:
        config = json.load(f)

# T·∫°o th∆∞ m·ª•c
if not os.path.exists(data_dir):
    os.makedirs(data_dir)
```

#### **Path (c√°ch hi·ªán ƒë·∫°i)**
```python
from pathlib import Path

# ‚úÖ D√πng Path - clean, safe, intuitive
data_dir = Path("/home/user/data")
config_file = data_dir / "config.json"  # Elegant operator overloading
if config_file.exists():
    with open(config_file, 'r') as f:
        config = json.load(f)

# T·∫°o th∆∞ m·ª•c
data_dir.mkdir(parents=True, exist_ok=True)  # M·ªôt l·ªánh, r√µ r√†ng!
```

---

## 5. Advanced Concepts

### 5.3. List Comprehension & Generator Expressions

```python
# Trong code
config_dict = {k: v for k, v in config.__dict__.items() 
               if not k.startswith('_')}
```

#### **Dictionary Comprehension:**
```python
# L·ªçc c√°c attribute kh√¥ng b·∫Øt ƒë·∫ßu b·∫±ng '_'
config.__dict__  # {'model_name': '...', 'lora_r': 128, '_private': ...}
config_dict = {k: v for k, v in config.__dict__.items() 
               if not k.startswith('_')}
# {'model_name': '...', 'lora_r': 128}  # '_private' b·ªã lo·∫°i
```

#### **So s√°nh v·ªõi loop th∆∞·ªùng:**
```python
# ‚ùå Traditional way - verbose
config_dict = {}
for k, v in config.__dict__.items():
    if not k.startswith('_'):
        config_dict[k] = v

# ‚úÖ Comprehension - concise
config_dict = {k: v for k, v in config.__dict__.items() 
               if not k.startswith('_')}
```
### 5.5. String Formatting v·ªõi f-strings

```python
# Code trong script
logger.info(f"üìä GPU Memory: {gpu_allocated:.1f}GB / {gpu_memory:.1f}GB used")
```

#### **Format specifiers:**
```python
value = 123.456789

f"{value}"           # "123.456789"
f"{value:.2f}"       # "123.46" (2 decimal places)
f"{value:.1f}"       # "123.5" (1 decimal place)
f"{value:,.2f}"      # "123.46" (thousands separator)
f"{value:>10.2f}"    # "    123.46" (right-aligned, width 10)
f"{value:0>10.2f}"   # "0000123.46" (zero-padded)

count = 1000000
f"{count:,}"         # "1,000,000" (thousands separator)
```

### 5.6. Lambda Functions

```python
# Code trong script
default_factory=lambda: f"vietnamese-legal-llama-{datetime.now().strftime('%Y%m%d-%H%M%S')}"
```

#### **Lambda vs Function:**
```python
# ‚ùå Regular function - verbose
def create_run_name():
    return f"vietnamese-legal-llama-{datetime.now().strftime('%Y%m%d-%H%M%S')}"

field(default_factory=create_run_name)

# ‚úÖ Lambda - concise
field(default_factory=lambda: f"vietnamese-legal-llama-{datetime.now().strftime('%Y%m%d-%H%M%S')}")
```

#### **Khi n√†o d√πng lambda?**
- Function ƒë∆°n gi·∫£n, 1 d√≤ng
- Ch·ªâ d√πng 1 l·∫ßn
- Kh√¥ng c·∫ßn t√™n function


### 5.8. Unpacking v·ªõi `**` operator

```python
# Loading config from dict
config_dict = {"model_name": "llama", "lora_r": 128}
config = FineTuneConfig(**config_dict)

# T∆∞∆°ng ƒë∆∞∆°ng:
config = FineTuneConfig(model_name="llama", lora_r=128)
```
