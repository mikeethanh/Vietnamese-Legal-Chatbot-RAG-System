# TÃ i liá»‡u Embedding Serving - Vietnamese Legal Chatbot RAG System

## ğŸ“‹ Tá»•ng quan

Module Embedding Serving cá»§a Vietnamese Legal Chatbot RAG System cung cáº¥p **RESTful API** Ä‘á»ƒ serve embedding model **BGE-M3** cho viá»‡c táº¡o vector representations cá»§a text tiáº¿ng Viá»‡t trong domain phÃ¡p luáº­t. Há»‡ thá»‘ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ cháº¡y hiá»‡u quáº£ trÃªn **GPU servers** vá»›i performance cao vÃ  cost-effective.

## ğŸ¯ Váº¥n Ä‘á» cáº§n giáº£i quyáº¿t

### BÃ i toÃ¡n Semantic Search trong RAG
Embedding Serving giáº£i quyáº¿t viá»‡c **chuyá»ƒn Ä‘á»•i text thÃ nh vector** Ä‘á»ƒ phá»¥c vá»¥:

1. **ğŸ” Semantic Search**: TÃ¬m kiáº¿m documents liÃªn quan dá»±a trÃªn Ã½ nghÄ©a, khÃ´ng chá»‰ keywords
2. **ğŸ“Š Similarity Computation**: TÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng tá»± giá»¯a cÃ¢u há»i user vÃ  legal documents
3. **âš¡ Real-time Inference**: Serving embedding vá»›i latency tháº¥p cho chatbot

## ğŸ¤– BGE-M3 Model - Lá»±a chá»n Embedding Model

### Táº¡i sao chá»n BGE-M3?

**BGE-M3** (BAAI General Embedding - Multilingual, Multi-Granularity, Multi-Functionality) lÃ  state-of-the-art embedding model Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Beijing Academy of Artificial Intelligence.

#### ğŸ“„ **Paper vÃ  Research Background**

**Research Paper**: [BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation](https://arxiv.org/abs/2402.03216)

**Key Innovation**: Self-knowledge distillation technique Ä‘á»ƒ táº¡o ra unified embedding space cho multiple languages vÃ  functionalities.

#### ğŸ¯ **Æ¯u Ä‘iá»ƒm vÆ°á»£t trá»™i cá»§a BGE-M3**

**1. Multi-Lingual Excellence**
- **Cross-lingual retrieval** - search tiáº¿ng Viá»‡t trong corpus Ä‘a ngÃ´n ngá»¯

**2. Multi-Functionality**
- **Dense Retrieval**: Traditional semantic similarity
- **Sparse Retrieval**: Keyword-based matching (tÆ°Æ¡ng tá»± BM25)

**5. Efficiency**
- **Model size**: 2.3GB (compact cho production)
- **Embedding dim**: 1024 (optimal cho speed/quality balance)

#### ğŸ”¬ **Technical Architecture**

**Base Architecture**: 
- **Backbone**: XLM-RoBERTa-large (560M parameters)
- **Self-Knowledge Distillation**: Novel training approach
- **Multi-task Learning**: Joint training cho dense + sparse + multi-vector


**Input Processing**:
- **Max sequence length**: 8192 tokens (excellent cho legal documents)

## ğŸ—ï¸ Kiáº¿n trÃºc Serving System

### Framework vÃ  Technology Stack

#### ğŸŒ **Táº¡i sao chá»n Flask Framework?**

**Flask** Ä‘Æ°á»£c chá»n lÃ m serving framework thay vÃ¬ FastAPI hay alternatives:

**Æ¯u Ä‘iá»ƒm cá»§a Flask**:
1. **Simplicity**: Minimal boilerplate, easy debugging
2. **Lightweight**: Low memory footprint (quan trá»ng cho CPU serving)
3. **Mature Ecosystem**: Extensive libraries vÃ  community support
4. **Production Proven**: ÄÆ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i trong production
5. **Threading Support**: Built-in multi-threading cho concurrent requests

#### ğŸ§  **Model Loading vÃ  Optimization**

### API Design vÃ  Endpoints

#### ğŸ”§ **Core API Endpoints**

**1. Health Check Endpoint**
```python
@app.route("/health", methods=["GET"])
```

**Chá»©c nÄƒng**: Service discovery, health monitoring, load balancer integration.

**2. Embedding Generation Endpoint**
```python
@app.route("/embed", methods=["POST"])
def embed():
```

**3. Similarity Computation Endpoint**
```python
@app.route("/similarity", methods=["POST"])
```
#### âš¡ **Performance Optimizations**

**Batch Size Management**:
```python
max_batch_size = int(os.getenv("MAX_BATCH_SIZE", "32"))

# Optimal batch sizes cho different CPU configurations:
# 4-core CPU: batch_size = 16
# 8-core CPU: batch_size = 32  
# 16-core CPU: batch_size = 64
```
---

*TÃ i liá»‡u nÃ y mÃ´ táº£ comprehensive architecture vÃ  implementation cá»§a Embedding Serving Module, sá»­ dá»¥ng BGE-M3 model vá»›i Flask framework Ä‘á»ƒ cung cáº¥p high-performance, cost-effective embedding API cho Vietnamese Legal RAG System.*