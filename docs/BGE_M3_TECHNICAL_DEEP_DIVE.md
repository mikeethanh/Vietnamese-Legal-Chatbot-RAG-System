# BGE-M3 Model - Comprehensive Technical Documentation

## üìã T·ªïng quan v·ªÅ BGE-M3

**BGE-M3** (BAAI General Embedding - Multilingual, Multi-Granularity, Multi-Functionality) l√† m·ªôt breakthrough trong lƒ©nh v·ª±c text embedding, ƒë∆∞·ª£c ph√°t tri·ªÉn b·ªüi Beijing Academy of Artificial Intelligence (BAAI). Model n√†y ƒë·∫°i di·ªán cho s·ª± ti·∫øn b·ªô v∆∞·ª£t b·∫≠c trong vi·ªác t·∫°o ra unified embedding space cho multiple languages v√† functionalities.

## üìÑ Research Foundation

### Paper Reference
**Title**: "BGE M3-Embedding: Multi-Lingual, Multi-Functionality, Multi-Granularity Text Embeddings Through Self-Knowledge Distillation"
**Authors**: Jianlv Chen, Shitao Xiao, Peitian Zhang, Kun Luo, Defu Lian, Zheng Liu
**Publication**: arXiv:2402.03216, February 2024
**Link**: https://arxiv.org/abs/2402.03216

## üéØ Ba Tr·ª• C·ªôt Ch√≠nh (The 3 M's)

### 1. Multi-Lingual (ƒêa ng√¥n ng·ªØ)

**Kh√°i ni·ªám**: Kh·∫£ nƒÉng hi·ªÉu v√† t·∫°o embeddings ch·∫•t l∆∞·ª£ng cao cho **100+ ng√¥n ng·ªØ** kh√°c nhau, bao g·ªìm ti·∫øng Vi·ªát.

**ƒê·∫∑c ƒëi·ªÉm n·ªïi b·∫≠t**:
- **Cross-lingual Retrieval**: C√≥ th·ªÉ search b·∫±ng ti·∫øng Vi·ªát trong corpus ti·∫øng Anh v√† ng∆∞·ª£c l·∫°i
- **Language-Agnostic Training**: Training methodology kh√¥ng bias towards specific languages
- **Unified Embedding Space**: T·∫•t c·∫£ languages ƒë∆∞·ª£c map v√†o c√πng m·ªôt vector space

**Vietnamese Performance**:
```
üìä BGE-M3 Vietnamese Metrics:
- MTEB Vietnamese: 68.2% (excellent performance)
- Cross-lingual EN‚ÜíVI: 59.3% (best in class)
- Cross-lingual VI‚ÜíEN: 61.1% (competitive)
```

### 2. Multi-Functionality (ƒêa ch·ª©c nƒÉng)

BGE-M3 kh√¥ng ch·ªâ l√† m·ªôt embedding model m√† integrates **3 retrieval paradigms** kh√°c nhau:

#### üî∏ **Dense Embedding (Dense Vector Retrieval)**

**Kh√°i ni·ªám**: Traditional semantic embedding approach s·ª≠ d·ª•ng dense vectors ƒë·ªÉ capture semantic meaning.

**C∆° ch·∫ø ho·∫°t ƒë·ªông**:
```python
# Dense embedding generation
dense_embedding = model.encode(text, return_dense=True)
# Output: [1024] dimensional vector with semantic information
```

**ƒê·∫∑c ƒëi·ªÉm**:
- **Semantic Understanding**: Hi·ªÉu √Ω nghƒ©a s√¢u c·ªßa text
- **Context Aware**: Capture contextual relationships
- **Similarity Based**: S·ª≠ d·ª•ng cosine similarity cho ranking
- **Dimension**: 1024-dimensional vectors

**Use Cases**: 
- Semantic search
- Document similarity
- Clustering based on meaning

#### üî∏ **Sparse Embedding (Lexical/Keyword Retrieval)**

**Kh√°i ni·ªám**: Simulates traditional keyword-based retrieval (nh∆∞ BM25) nh∆∞ng learnable v√† c√≥ th·ªÉ optimization.

**C∆° ch·∫ø ho·∫°t ƒë·ªông**:
```python
# Sparse embedding generation  
sparse_embedding = model.encode(text, return_sparse=True)
# Output: Sparse vector v·ªõi learned term importance weights
```

**Technical Implementation**:
- **Learned Term Weighting**: Thay v√¨ TF-IDF, s·ª≠ d·ª•ng neural network ƒë·ªÉ weight terms
- **Vocabulary Expansion**: C√≥ th·ªÉ assign weights cho terms kh√¥ng xu·∫•t hi·ªán trong text g·ªëc
- **Sparsity Control**: Automatic sparsity regulation ƒë·ªÉ balance performance vs efficiency

**Advantages over BM25**:
- **Learnable Weights**: Weights ƒë∆∞·ª£c optimize cho specific domain
- **Semantic Term Expansion**: C√≥ th·ªÉ weight related terms cao h∆°n
- **Cross-lingual**: Ho·∫°t ƒë·ªông across languages

**Use Cases**:
- Exact match requirements
- Legal document search (exact term matching)
- Hybrid search systems

#### üî∏ **Multi-Vector Embedding (Fine-grained Interaction)**

**Kh√°i ni·ªám**: Advanced approach s·ª≠ d·ª•ng **multiple vectors per text** ƒë·ªÉ capture fine-grained semantic interactions.

**C∆° ch·∫ø ho·∫°t ƒë·ªông**:
```python
# Multi-vector embedding generation
multi_vectors = model.encode(text, return_multi_vector=True)
# Output: Multiple vectors representing different aspects c·ªßa text
```

**Technical Details**:
- **Token-level Representations**: M·ªói important token c√≥ ri√™ng vector representation
- **Interaction Modeling**: Model interactions gi·ªØa query tokens v√† document tokens
- **Maximum Inner Product Search (MIPS)**: S·ª≠ d·ª•ng MIPS thay v√¨ cosine similarity

**Advantages**:
- **Fine-grained Matching**: Detailed token-to-token interactions
- **Higher Accuracy**: Better performance cho complex queries
- **Interpretability**: C√≥ th·ªÉ trace matching reasons

**Disadvantages**:
- **Storage Overhead**: Requires multiple vectors per document
- **Computational Cost**: More expensive similarity computation
- **Index Complexity**: More complex indexing requirements

### 3. Multi-Granularity (ƒêa ƒë·ªô chi ti·∫øt)

**Kh√°i ni·ªám**: Kh·∫£ nƒÉng x·ª≠ l√Ω text ·ªü **multiple levels of granularity** t·ª´ tokens ƒë·∫øn documents.

#### **Granularity Levels**:

1. **Token Level**:
   - Individual word/subword representations
   - Fine-grained semantic analysis
   - Token-token interactions

2. **Sentence Level**:
   - Sentence embeddings
   - Intra-sentence relationships
   - Standard use case cho most applications

3. **Passage Level**:
   - Paragraph/passage representations
   - Long-form content understanding
   - Document section analysis

4. **Document Level**:
   - Entire document embeddings
   - Global semantic representation
   - Document-level similarity

**Technical Implementation**:
```python
# Multi-granularity processing
embeddings = model.encode(
    text,
    granularity=['token', 'sentence', 'passage', 'document']
)
```

## üß† Self-Knowledge Distillation

### Kh√°i ni·ªám Core

**Self-Knowledge Distillation** l√† breakthrough technique trong BGE-M3, cho ph√©p model h·ªçc t·ª´ ch√≠nh b·∫£n th√¢n n√≥ ƒë·ªÉ improve performance across multiple functionalities.

### Traditional vs Self-Knowledge Distillation

**Traditional Knowledge Distillation**:
```
Teacher Model (Large) ‚Üí Student Model (Small)
                     ‚Üì
               Knowledge Transfer
```

**Self-Knowledge Distillation**:
```
Model (Dense) ‚Üê ‚Üí Model (Sparse) ‚Üê ‚Üí Model (Multi-Vector)
    ‚Üì              ‚Üì                    ‚Üì
      Self-Teaching & Cross-Functionality Learning
```

### Technical Mechanism

#### **Cross-Functionality Learning**:

1. **Dense ‚Üí Sparse Knowledge Transfer**:
   ```python
   # Dense embeddings teach sparse embeddings
   dense_loss = compute_dense_loss(query_dense, doc_dense)
   sparse_loss = compute_sparse_loss(query_sparse, doc_sparse)
   
   # Distillation loss
   distill_loss = KL_divergence(dense_similarity, sparse_similarity)
   total_loss = dense_loss + sparse_loss + Œª * distill_loss
   ```

2. **Sparse ‚Üí Multi-Vector Knowledge Transfer**:
   ```python
   # Sparse weights guide multi-vector attention
   sparse_weights = get_sparse_weights(text)
   multi_vector_attention = compute_attention(
       tokens, 
       guided_by=sparse_weights
   )
   ```

3. **Multi-Vector ‚Üí Dense Knowledge Transfer**:
   ```python
   # Multi-vector interactions improve dense representations
   fine_grained_signals = aggregate_multi_vector_interactions(
       query_vectors, doc_vectors
   )
   dense_embedding = enhance_dense_with_fine_grained(
       original_dense, fine_grained_signals
   )
   ```

#### **Iterative Self-Improvement**:

```python
# Pseudo-code for self-distillation process
for epoch in range(num_epochs):
    # Forward pass v·ªõi all functionalities
    dense_emb = model.encode_dense(batch)
    sparse_emb = model.encode_sparse(batch) 
    multi_vec_emb = model.encode_multi_vector(batch)
    
    # Compute individual losses
    loss_dense = compute_dense_loss(dense_emb, labels)
    loss_sparse = compute_sparse_loss(sparse_emb, labels)
    loss_multi = compute_multi_vector_loss(multi_vec_emb, labels)
    
    # Cross-functionality distillation
    distill_loss = (
        kl_divergence(dense_sim, sparse_sim) +
        kl_divergence(dense_sim, multi_sim) + 
        kl_divergence(sparse_sim, multi_sim)
    )
    
    # Total loss with self-distillation
    total_loss = (
        loss_dense + loss_sparse + loss_multi + 
        Œª * distill_loss
    )
    
    # Backward pass
    total_loss.backward()
    optimizer.step()
```

### ∆Øu ƒëi·ªÉm c·ªßa Self-Knowledge Distillation

1. **Unified Training**: Single model learns multiple functionalities simultaneously
2. **Cross-Functionality Synergy**: Each functionality improves others
3. **No External Teacher**: Kh√¥ng c·∫ßn separate teacher models
4. **Consistent Performance**: All functionalities benefit from shared knowledge
5. **Efficiency**: One model serves multiple purposes

## üìä Training Data v√† Methodology

### Dataset Composition

BGE-M3 ƒë∆∞·ª£c train tr√™n massive multilingual dataset v·ªõi diverse text types:

#### **Core Training Data**:

1. **Retrieval Datasets**:
   ```
   üìö MS MARCO Passages (English)
   üìö Natural Questions (English)
   üìö mMARCO (Multilingual version)
   üìö Mr. TyDi (Multilingual QA)
   ```

2. **Multilingual Corpora**:
   ```
   üåç Wikipedia (100+ languages)
   üåç Common Crawl (multilingual web data)
   üåç News Corpora (multiple languages)
   üåç Academic Papers (multilingual)
   ```

3. **Specialized Domains**:
   ```
   ‚öñÔ∏è  Legal Documents (multiple jurisdictions)
   üè• Medical Literature
   üíº Business Documents
   üî¨ Scientific Papers
   ```

### Training Format: Triplet vs Instruction

BGE-M3 s·ª≠ d·ª•ng **combined training approach** v·ªõi multiple formats:

#### **Triplet Format (Primary)**:

```python
# Triplet training format
triplet = {
    "query": "Lu·∫≠t lao ƒë·ªông Vi·ªát Nam quy ƒë·ªãnh v·ªÅ th·ªùi gian l√†m vi·ªác",
    "positive": "Theo B·ªô lu·∫≠t Lao ƒë·ªông 2019, th·ªùi gian l√†m vi·ªác b√¨nh th∆∞·ªùng kh√¥ng qu√° 8 gi·ªù m·ªôt ng√†y v√† kh√¥ng qu√° 48 gi·ªù m·ªôt tu·∫ßn...",
    "negative": "Quy ƒë·ªãnh v·ªÅ an to√†n lao ƒë·ªông trong m√¥i tr∆∞·ªùng l√†m vi·ªác..."
}
```

**T·∫°i sao d√πng Triplet Format?**:
- **Contrastive Learning**: Learn to distinguish between relevant v√† irrelevant content
- **Ranking Optimization**: Directly optimize cho retrieval ranking
- **Hard Negative Mining**: Improve model's ability to handle difficult cases

#### **Instruction Format (Secondary)**:

```python
# Instruction format for multi-functionality
instruction_data = {
    "instruction": "Generate dense embedding for semantic search",
    "input": "Vietnamese legal document about labor law",
    "output": "[dense_embedding_vector]"
},
{
    "instruction": "Generate sparse embedding for keyword matching", 
    "input": "Vietnamese legal document about labor law",
    "output": "{term_weights_sparse_vector}"
}
```

**Purpose c·ªßa Instruction Format**:
- **Functionality Control**: Teach model when to use which functionality
- **Task Awareness**: Model learns to adapt behavior based on instructions
- **Multi-Task Learning**: Single model handles multiple tasks

### Training Stages

#### **Stage 1: Foundation Training**
```python
# Massive multilingual pre-training
for batch in multilingual_corpus:
    # Contrastive learning v·ªõi large batch sizes
    embeddings = model.encode(batch)
    loss = contrastive_loss(embeddings, similarity_labels)
    loss.backward()
```

#### **Stage 2: Multi-Functionality Learning**
```python
# Joint training cho all functionalities
for batch in retrieval_data:
    dense_emb = model.encode_dense(batch)
    sparse_emb = model.encode_sparse(batch)
    multi_emb = model.encode_multi_vector(batch)
    
    # Individual losses
    loss_dense = contrastive_loss(dense_emb)
    loss_sparse = sparse_ranking_loss(sparse_emb) 
    loss_multi = multi_vector_loss(multi_emb)
    
    # Self-distillation loss
    distill_loss = cross_functionality_distillation(
        dense_emb, sparse_emb, multi_emb
    )
    
    total_loss = loss_dense + loss_sparse + loss_multi + distill_loss
```

#### **Stage 3: Fine-tuning v√† Specialization**
```python
# Domain-specific fine-tuning
for domain_batch in specialized_domains:
    # Fine-tune tr√™n specific domains (legal, medical, etc.)
    embeddings = model.encode(domain_batch)
    domain_loss = domain_specific_loss(embeddings, domain_labels)
    
    # Maintain general capabilities
    general_loss = general_capability_loss(embeddings)
    
    total_loss = domain_loss + Œª * general_loss
```

## üèóÔ∏è Model Architecture Deep Dive

### Base Architecture

```python
# BGE-M3 Architecture Overview
class BGEM3Model(nn.Module):
    def __init__(self):
        # Backbone: XLM-RoBERTa Large (560M parameters)
        self.backbone = XLMRobertaModel.from_pretrained(
            'xlm-roberta-large'
        )
        
        # Dense embedding head
        self.dense_head = nn.Sequential(
            nn.Linear(1024, 1024),
            nn.LayerNorm(1024),
            nn.Tanh()
        )
        
        # Sparse embedding head
        self.sparse_head = nn.Sequential(
            nn.Linear(1024, vocab_size),
            nn.ReLU(),  # Ensure positive weights
            nn.Dropout(0.1)
        )
        
        # Multi-vector head
        self.multi_vector_head = nn.Sequential(
            nn.Linear(1024, 1024),
            nn.LayerNorm(1024)
        )
        
    def forward(self, input_ids, attention_mask, return_type='all'):
        # Backbone encoding
        outputs = self.backbone(
            input_ids=input_ids,
            attention_mask=attention_mask,
            return_dict=True
        )
        
        last_hidden_state = outputs.last_hidden_state
        pooler_output = outputs.pooler_output
        
        results = {}
        
        if return_type in ['dense', 'all']:
            # Dense embedding: CLS token pooling
            dense_emb = self.dense_head(pooler_output)
            results['dense'] = dense_emb
            
        if return_type in ['sparse', 'all']:
            # Sparse embedding: Token-level weights
            token_weights = self.sparse_head(last_hidden_state)
            # Apply attention mask
            token_weights = token_weights * attention_mask.unsqueeze(-1)
            results['sparse'] = token_weights
            
        if return_type in ['multi_vector', 'all']:
            # Multi-vector: Multiple representations
            multi_vectors = self.multi_vector_head(last_hidden_state)
            # Select top-k important tokens
            importance_scores = torch.norm(multi_vectors, dim=-1)
            top_k_indices = torch.topk(importance_scores, k=32).indices
            results['multi_vector'] = multi_vectors[top_k_indices]
            
        return results
```

### Embedding Mechanisms Chi Ti·∫øt

#### **Dense Embedding Process**:

```python
def encode_dense(self, texts):
    """Dense embedding generation"""
    # Tokenization
    inputs = self.tokenizer(
        texts, 
        padding=True, 
        truncation=True, 
        max_length=8192,
        return_tensors='pt'
    )
    
    # Forward pass
    with torch.no_grad():
        outputs = self.model(**inputs, return_type='dense')
        dense_embeddings = outputs['dense']
        
        # L2 normalization for cosine similarity
        dense_embeddings = F.normalize(dense_embeddings, p=2, dim=1)
        
    return dense_embeddings
```

#### **Sparse Embedding Process**:

```python
def encode_sparse(self, texts):
    """Sparse embedding generation"""
    inputs = self.tokenizer(texts, ...)
    
    with torch.no_grad():
        outputs = self.model(**inputs, return_type='sparse')
        token_weights = outputs['sparse']  # [batch, seq_len, vocab_size]
        
        # Aggregate token weights cho each vocab term
        sparse_embeddings = []
        for i, text in enumerate(texts):
            text_tokens = inputs['input_ids'][i]
            text_weights = token_weights[i]
            
            # Create sparse vector
            sparse_vector = torch.zeros(self.vocab_size)
            for j, token_id in enumerate(text_tokens):
                if token_id != self.pad_token_id:
                    # Aggregate weights for each vocabulary term
                    sparse_vector[token_id] += text_weights[j, token_id]
                    
            sparse_embeddings.append(sparse_vector)
            
    return torch.stack(sparse_embeddings)
```

#### **Multi-Vector Embedding Process**:

```python
def encode_multi_vector(self, texts):
    """Multi-vector embedding generation"""
    inputs = self.tokenizer(texts, ...)
    
    with torch.no_grad():
        outputs = self.model(**inputs, return_type='multi_vector')
        all_vectors = outputs['multi_vector']  # [batch, seq_len, hidden]
        
        multi_vector_embeddings = []
        for i, text in enumerate(texts):
            text_vectors = all_vectors[i]  # [seq_len, hidden]
            attention_mask = inputs['attention_mask'][i]
            
            # Select important vectors based on attention weights
            importance_scores = torch.norm(text_vectors, dim=-1)
            masked_scores = importance_scores * attention_mask
            
            # Top-k selection
            top_k = min(32, torch.sum(attention_mask).item())
            top_indices = torch.topk(masked_scores, k=top_k).indices
            
            selected_vectors = text_vectors[top_indices]
            multi_vector_embeddings.append(selected_vectors)
            
    return multi_vector_embeddings
```

## ‚úÖ ∆Øu ƒëi·ªÉm c·ªßa BGE-M3

### 1. **Unified Architecture**
- **Single Model**: Thay v√¨ multiple models cho different functionalities
- **Consistent Interface**: Same API cho dense, sparse, multi-vector
- **Resource Efficiency**: One model deployment thay v√¨ multiple

### 2. **State-of-the-Art Performance**
```
üìä Performance Comparison:
Model               | MTEB Avg | Multilingual | Vietnamese
--------------------|----------|--------------|----------
BGE-M3             | 70.46%   | ‚úÖ Excellent | 68.2%
e5-mistral-7b-instruct | 69.00% | ‚ùå Limited | ~45%
multilingual-e5-large | 65.79% | ‚úÖ Good | 62.1%
```

### 3. **Flexibility v√† Adaptability**
- **Multi-Modal Retrieval**: Dense + Sparse + Multi-Vector
- **Domain Adaptation**: Fine-tuning cho specific domains
- **Language Flexibility**: Cross-lingual capabilities

### 4. **Production Ready**
- **Optimized Inference**: Efficient serving implementations
- **Scalable**: Supports batch processing
- **Memory Efficient**: Reasonable model size (2.3GB)

## ‚ùå Nh∆∞·ª£c ƒëi·ªÉm c·ªßa BGE-M3

### 1. **Complexity Issues**

#### **Training Complexity**:
- **Multi-Objective Optimization**: Balancing multiple loss functions
- **Hyperparameter Tuning**: Complex hyperparameter space
- **Computational Cost**: Requires significant compute resources

```python
# Complex loss function
total_loss = (
    Œ± * dense_loss + 
    Œ≤ * sparse_loss + 
    Œ≥ * multi_vector_loss +
    Œ¥ * distillation_loss_dense_sparse +
    Œµ * distillation_loss_dense_multi +
    Œ∂ * distillation_loss_sparse_multi
)
# Tuning Œ±, Œ≤, Œ≥, Œ¥, Œµ, Œ∂ is challenging
```

#### **Deployment Complexity**:
- **Multiple Inference Modes**: Need to support 3 different output types
- **Storage Overhead**: Multi-vector approach requires more storage
- **API Complexity**: More complex serving interface

### 2. **Resource Requirements**

#### **Memory Usage**:
```
üíæ Memory Requirements:
- Model Parameters: 560M (XLM-RoBERTa backbone)
- Dense Embeddings: 1024 dims per text
- Sparse Embeddings: vocab_size dims per text (~250K)
- Multi-Vector: 32 √ó 1024 dims per text
```

#### **Computational Overhead**:
- **Multi-Vector Similarity**: More expensive than simple cosine similarity
- **Sparse Vector Processing**: Additional computation for sparse weights
- **Cross-Functionality**: Higher inference cost when using all modes

### 3. **Fine-tuning Challenges**

#### **Knowledge Distillation Issues**:
- **Balancing Act**: Hard to balance between different functionalities
- **Catastrophic Forgetting**: Risk of losing general capabilities during specialization
- **Domain Adaptation**: Difficult to adapt all functionalities simultaneously

```python
# Example fine-tuning challenge
def fine_tune_domain_specific(model, domain_data):
    # Risk: Dense performance improves, sparse performance degrades
    for batch in domain_data:
        dense_loss = compute_dense_loss(batch)  # Improves
        sparse_loss = compute_sparse_loss(batch)  # May degrade
        
        # Challenging to maintain balance
        total_loss = dense_loss + Œª * sparse_loss  # How to set Œª?
```

### 4. **Limited Specialization**

#### **Jack of All Trades Problem**:
- **General Purpose**: May not excel in highly specialized domains
- **Trade-offs**: Performance compromises across functionalities
- **Domain Expertise**: May lack deep domain-specific optimizations

### 5. **Interpretability Issues**

#### **Black Box Nature**:
- **Multi-Vector Selection**: Why certain vectors are selected?
- **Sparse Weight Assignment**: How weights are assigned to terms?
- **Cross-Functionality Interactions**: Complex internal dynamics

## üéì Training Process Chi Ti·∫øt

### Phase 1: Multilingual Foundation

```python
# Stage 1: Massive multilingual pre-training
def stage1_multilingual_training():
    """
    Foundation training tr√™n diverse multilingual corpus
    Goal: Establish strong multilingual representations
    """
    
    # Data preparation
    multilingual_corpus = load_multilingual_data([
        'wikipedia_100_languages',
        'common_crawl_multilingual', 
        'news_corpora_multilingual'
    ])
    
    # Training loop
    for epoch in range(foundation_epochs):
        for batch in multilingual_corpus:
            # Simple contrastive learning
            embeddings = model.encode_dense(batch['texts'])
            
            # In-batch negatives
            labels = create_contrastive_labels(batch['similarities'])
            loss = contrastive_loss(embeddings, labels)
            
            # Backpropagation
            loss.backward()
            optimizer.step()
            
        # Evaluation on multiple languages
        evaluate_multilingual_performance(model)
```

### Phase 2: Multi-Functionality Integration

```python
# Stage 2: Joint functionality training
def stage2_multifunctionality_training():
    """
    Joint training ƒë·ªÉ integrate dense, sparse, multi-vector
    Goal: Learn unified representations across functionalities
    """
    
    retrieval_data = load_retrieval_datasets([
        'ms_marco', 'natural_questions', 'mmarco', 'mr_tydi'
    ])
    
    for epoch in range(integration_epochs):
        for batch in retrieval_data:
            # Multi-functionality forward pass
            dense_emb = model.encode_dense(batch['queries'], batch['docs'])
            sparse_emb = model.encode_sparse(batch['queries'], batch['docs'])
            multi_emb = model.encode_multi_vector(batch['queries'], batch['docs'])
            
            # Individual functionality losses
            loss_dense = ranking_loss(dense_emb, batch['labels'])
            loss_sparse = sparse_ranking_loss(sparse_emb, batch['labels'])
            loss_multi = multi_vector_loss(multi_emb, batch['labels'])
            
            # Self-knowledge distillation
            sim_dense = compute_similarity(dense_emb)
            sim_sparse = compute_similarity(sparse_emb)  
            sim_multi = compute_similarity(multi_emb)
            
            distill_loss = (
                kl_divergence(sim_dense, sim_sparse) +
                kl_divergence(sim_dense, sim_multi) +
                kl_divergence(sim_sparse, sim_multi)
            )
            
            # Combined loss
            total_loss = (
                loss_dense + loss_sparse + loss_multi + 
                Œª * distill_loss
            )
            
            total_loss.backward()
            optimizer.step()
            
        # Multi-functionality evaluation
        evaluate_all_functionalities(model)
```

### Phase 3: Domain Specialization

```python
# Stage 3: Domain-specific fine-tuning
def stage3_domain_specialization():
    """
    Fine-tune tr√™n specific domains while maintaining general capabilities
    Goal: Specialize cho target domains without catastrophic forgetting
    """
    
    domain_datasets = {
        'legal': load_legal_corpus(),
        'medical': load_medical_corpus(), 
        'scientific': load_scientific_corpus()
    }
    
    # Gradual domain adaptation
    for domain, dataset in domain_datasets.items():
        print(f"Specializing for {domain} domain...")
        
        for epoch in range(specialization_epochs):
            for batch in dataset:
                # Domain-specific training
                domain_loss = compute_domain_loss(model, batch)
                
                # General capability preservation
                general_batch = sample_general_data()
                general_loss = compute_general_loss(model, general_batch)
                
                # Regularization ƒë·ªÉ prevent forgetting
                regularization_loss = compute_regularization_loss(
                    model, previous_model_weights
                )
                
                total_loss = (
                    domain_loss + 
                    Œ± * general_loss + 
                    Œ≤ * regularization_loss
                )
                
                total_loss.backward()
                optimizer.step()
                
            # Monitor both domain and general performance
            domain_performance = evaluate_domain_specific(model, domain)
            general_performance = evaluate_general_capabilities(model)
            
            # Early stopping if general performance degrades
            if general_performance < threshold:
                print(f"Stopping {domain} specialization to prevent forgetting")
                break
```

### Advanced Training Techniques

#### **Hard Negative Mining**:

```python
def hard_negative_mining(model, queries, documents):
    """
    Dynamically mine hard negatives during training
    Goal: Improve model's discrimination ability
    """
    
    with torch.no_grad():
        # Generate embeddings
        query_embs = model.encode_dense(queries)
        doc_embs = model.encode_dense(documents)
        
        # Compute similarity matrix
        similarities = torch.mm(query_embs, doc_embs.t())
        
        # Find hard negatives (high similarity but wrong labels)
        hard_negatives = []
        for i, query in enumerate(queries):
            # Get documents with high similarity but low relevance
            query_sims = similarities[i]
            
            # Sort by similarity
            sorted_indices = torch.argsort(query_sims, descending=True)
            
            # Select hard negatives
            for j in sorted_indices:
                if j not in positive_docs[i] and query_sims[j] > threshold:
                    hard_negatives.append((i, j))
                    
    return hard_negatives

# Training v·ªõi hard negatives
for batch in training_data:
    # Mine hard negatives for current batch
    hard_negs = hard_negative_mining(model, batch['queries'], batch['docs'])
    
    # Add hard negatives to training batch
    augmented_batch = add_hard_negatives(batch, hard_negs)
    
    # Train with augmented data
    loss = compute_loss(model, augmented_batch)
    loss.backward()
```

#### **Multi-Task Learning Schedule**:

```python
def adaptive_loss_weighting(epoch, performance_metrics):
    """
    Dynamically adjust loss weights based on performance
    Goal: Balance learning across functionalities
    """
    
    # Performance tracking
    dense_perf = performance_metrics['dense']
    sparse_perf = performance_metrics['sparse'] 
    multi_perf = performance_metrics['multi_vector']
    
    # Adaptive weighting
    if dense_perf < target_dense:
        weight_dense = 1.2
    else:
        weight_dense = 0.8
        
    if sparse_perf < target_sparse:
        weight_sparse = 1.2
    else:
        weight_sparse = 0.8
        
    if multi_perf < target_multi:
        weight_multi = 1.2
    else:
        weight_multi = 0.8
        
    return {
        'dense': weight_dense,
        'sparse': weight_sparse, 
        'multi': weight_multi
    }

# Training v·ªõi adaptive weights
for epoch in range(num_epochs):
    # Evaluate current performance
    current_performance = evaluate_model(model)
    
    # Update loss weights
    weights = adaptive_loss_weighting(epoch, current_performance)
    
    for batch in training_data:
        # Compute losses
        loss_dense = compute_dense_loss(model, batch)
        loss_sparse = compute_sparse_loss(model, batch)
        loss_multi = compute_multi_loss(model, batch)
        
        # Weighted combination
        total_loss = (
            weights['dense'] * loss_dense +
            weights['sparse'] * loss_sparse + 
            weights['multi'] * loss_multi
        )
        
        total_loss.backward()
        optimizer.step()
```

## üî¨ Research Impact v√† Future Directions

### Current Impact

1. **Benchmark Performance**: SOTA results tr√™n multiple multilingual benchmarks
2. **Industry Adoption**: Wide adoption trong production systems
3. **Research Foundation**: Basis cho subsequent embedding research
4. **Open Source**: Availability promotes broader research

### Future Research Directions

1. **Efficiency Improvements**:
   - Model compression techniques
   - Quantization strategies
   - Distillation to smaller models

2. **Domain Specialization**:
   - Better domain adaptation methods
   - Few-shot learning capabilities
   - Transfer learning improvements

3. **Architectural Innovations**:
   - Integration with newer transformer architectures
   - Attention mechanism improvements
   - Memory-efficient designs

---

*T√†i li·ªáu n√†y cung c·∫•p comprehensive understanding v·ªÅ BGE-M3 model architecture, training methodology, v√† practical implications cho Vietnamese Legal RAG applications.*