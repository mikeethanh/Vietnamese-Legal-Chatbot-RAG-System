# Llama 3.1 8B Instruct - Technical Deep Dive

## M·ª•c l·ª•c
1. [Llama Model Family Overview](#1-llama-model-family-overview)
2. [Base vs Instruct Models](#2-base-vs-instruct-models)
3. [Llama 3.1 Architecture](#3-llama-31-architecture)
4. [Instruction Tuning Process](#4-instruction-tuning-process)
5. [Llama 3.1 8B Instruct Specifications](#5-llama-31-8b-instruct-specifications)
6. [Chat Template Format](#6-chat-template-format)
7. [Tokenizer Details](#7-tokenizer-details)
8. [Use Cases & Performance](#8-use-cases--performance)

---

## 1. Llama Model Family Overview

### Meta's Llama Evolution

```
Llama 1 (Feb 2023)
‚îú‚îÄ‚îÄ 7B, 13B, 33B, 65B parameters
‚îú‚îÄ‚îÄ Research only (not commercial)
‚îî‚îÄ‚îÄ Pre-trained on public data

‚Üì

Llama 2 (Jul 2023)
‚îú‚îÄ‚îÄ 7B, 13B, 70B parameters
‚îú‚îÄ‚îÄ Commercial use allowed
‚îú‚îÄ‚îÄ Base + Chat variants
‚îî‚îÄ‚îÄ Better training (2T tokens)

‚Üì

Llama 3 (Apr 2024)
‚îú‚îÄ‚îÄ 8B, 70B parameters
‚îú‚îÄ‚îÄ Improved architecture
‚îú‚îÄ‚îÄ Larger vocab (128K ‚Üí 128K tokens)
‚îî‚îÄ‚îÄ Trained on 15T tokens!

‚Üì

Llama 3.1 (Jul 2024) ‚Üê C·ª¶A CH√öNG TA!
‚îú‚îÄ‚îÄ 8B, 70B, 405B parameters
‚îú‚îÄ‚îÄ Extended context (8K ‚Üí 128K tokens)
‚îú‚îÄ‚îÄ Tool use capabilities
‚îú‚îÄ‚îÄ Multilingual improvements
‚îî‚îÄ‚îÄ Better Vietnamese support!
```

### Model Variants

```
Llama-3.1-8B (Base Model)
‚îú‚îÄ‚îÄ Pre-trained on raw text
‚îú‚îÄ‚îÄ No instruction following
‚îú‚îÄ‚îÄ Good for: fine-tuning, research
‚îî‚îÄ‚îÄ Example output: continues text

vs

Llama-3.1-8B-Instruct (Instruct Model) ‚Üê CH√öNG TA D√ôNG!
‚îú‚îÄ‚îÄ Fine-tuned on instructions
‚îú‚îÄ‚îÄ Follows instructions naturally
‚îú‚îÄ‚îÄ Good for: chatbots, Q&A, assistants
‚îî‚îÄ‚îÄ Example output: answers questions
```

---

## 2. Base vs Instruct Models

### 2.1. Base Model Behavior

**Llama-3.1-8B (Base)**:
```python
# Input (prompt)
"ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ"

# Output (continuation - NO instruction following!)
"th·ªùi gi·ªù l√†m vi·ªác, th·ªùi gi·ªù ngh·ªâ ng∆°i. Theo ƒë√≥, th·ªùi gi·ªù l√†m vi·ªác 
b√¨nh th∆∞·ªùng kh√¥ng qu√° 8 gi·ªù trong m·ªôt ng√†y v√† 48 gi·ªù trong m·ªôt tu·∫ßn..."
# ‚ùå C·ª© vi·∫øt ti·∫øp, kh√¥ng tr·∫£ l·ªùi c√¢u h·ªèi!
```

**Problem v·ªõi Base Model**:
```python
# User asks a question
Q: "Th·ªùi gian l√†m vi·ªác t·ªëi ƒëa theo lu·∫≠t l√† bao nhi√™u?"

# Base model continues text
A: "Th·ªùi gian l√†m vi·ªác t·ªëi ƒëa theo lu·∫≠t l√† bao nhi√™u gi·ªù trong ng√†y?
    Th·ªùi gian l√†m vi·ªác t·ªëi ƒëa theo lu·∫≠t l√† bao nhi√™u gi·ªù trong tu·∫ßn?
    C√°c quy ƒë·ªãnh..."
# ‚ùå Kh√¥ng tr·∫£ l·ªùi, c·ª© ti·∫øp t·ª•c ƒë·∫∑t c√¢u h·ªèi!
```

---

### 2.2. Instruct Model Behavior

**Llama-3.1-8B-Instruct**:
```python
# Input (instruction)
"ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ g√¨?"

# Output (follows instruction!)
"ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ th·ªùi gi·ªù l√†m vi·ªác v√† th·ªùi 
gi·ªù ngh·ªâ ng∆°i. C·ª• th·ªÉ:
- Th·ªùi gi·ªù l√†m vi·ªác b√¨nh th∆∞·ªùng: kh√¥ng qu√° 8 gi·ªù/ng√†y v√† 48 gi·ªù/tu·∫ßn
- Ng∆∞·ªùi s·ª≠ d·ª•ng lao ƒë·ªông c√≥ th·ªÉ quy ƒë·ªãnh l√†m vi·ªác theo gi·ªù, ng√†y, tu·∫ßn
- Ph·∫£i b·∫£o ƒë·∫£m th·ªùi gi·ªù ngh·ªâ ng∆°i cho ng∆∞·ªùi lao ƒë·ªông"
# ‚úÖ Tr·∫£ l·ªùi ƒë√∫ng c√¢u h·ªèi!
```

**Question Answering**:
```python
Q: "Th·ªùi gian l√†m vi·ªác t·ªëi ƒëa theo lu·∫≠t l√† bao nhi√™u?"

# Base: Continues writing
A: "Th·ªùi gian l√†m vi·ªác t·ªëi ƒëa theo lu·∫≠t l√† bao nhi√™u gi·ªù?..." ‚ùå

# Instruct: Answers directly
A: "Theo B·ªô lu·∫≠t Lao ƒë·ªông, th·ªùi gian l√†m vi·ªác t·ªëi ƒëa l√†:
    - 8 gi·ªù trong m·ªôt ng√†y
    - 48 gi·ªù trong m·ªôt tu·∫ßn" ‚úÖ
```

---

### 2.3. Training Process Comparison

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ              PRE-TRAINING (Base Model)              ‚îÇ
‚îÇ  Training Data: Raw text t·ª´ internet                ‚îÇ
‚îÇ  Objective: Predict next word                       ‚îÇ
‚îÇ  Size: 15 trillion tokens                          ‚îÇ
‚îÇ  Result: Llama-3.1-8B (Base)                       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                        ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         INSTRUCTION TUNING (Instruct Model)         ‚îÇ
‚îÇ  Training Data: Instruction-response pairs          ‚îÇ
‚îÇ  Objective: Follow instructions                     ‚îÇ
‚îÇ  Size: Millions of examples                        ‚îÇ
‚îÇ  Techniques: SFT + RLHF + DPO                      ‚îÇ
‚îÇ  Result: Llama-3.1-8B-Instruct ‚Üê WE USE THIS!      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

#### **Pre-training Phase**
```python
# Training data format
"The capital of France is Paris. Paris is known for..."
"Python is a programming language. It is widely used..."
"ƒêi·ªÅu 1. B·ªô lu·∫≠t n√†y quy ƒë·ªãnh v·ªÅ quy·ªÅn v√† nghƒ©a v·ª•..."

# Task: Predict next token
Input:  "The capital of France is"
Target: "Paris"

Input:  "ƒêi·ªÅu 1. B·ªô lu·∫≠t n√†y quy ƒë·ªãnh"
Target: "v·ªÅ"
```

#### **Instruction Tuning Phase**
```python
# Training data format (instruction-following)
{
  "instruction": "What is the capital of France?",
  "input": "",
  "output": "The capital of France is Paris."
}

{
  "instruction": "ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ g√¨?",
  "input": "",
  "output": "ƒêi·ªÅu 10 quy ƒë·ªãnh v·ªÅ th·ªùi gi·ªù l√†m vi·ªác v√† ngh·ªâ ng∆°i..."
}

# Task: Generate response following instruction
Input:  Instruction + Input
Target: Expected output
```

---

## 3. Llama 3.1 Architecture

### 3.1. Transformer Architecture

```
Input Text: "ƒêi·ªÅu 10 quy ƒë·ªãnh v·ªÅ g√¨?"
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         Tokenization                 ‚îÇ
‚îÇ  "ƒêi·ªÅu" "10" "quy" "ƒë·ªãnh" "v·ªÅ" "g√¨"‚îÇ
‚îÇ  [123,  456, 789,  012,  345, 678]  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Embedding Layer (128K vocab)    ‚îÇ
‚îÇ  Each token ‚Üí 4096-dim vector       ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Transformer Block 1                ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ Multi-Head Attention           ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ MLP (Feed-Forward)             ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ Layer Normalization            ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Transformer Block 2                ‚îÇ
‚îÇ   ...                                ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
      ...  (32 blocks total)
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ   Transformer Block 32               ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ      Output Head (LM Head)           ‚îÇ
‚îÇ  4096-dim ‚Üí 128K vocab logits        ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
       ‚Üì
   Predicted Token
```

### 3.2. Attention Mechanism

**Multi-Head Attention trong Llama 3.1**:
```python
# Specifications
num_heads = 32              # 32 attention heads
head_dim = 128              # 128 dimensions per head
hidden_size = 4096          # 32 √ó 128 = 4096

# Grouped Query Attention (GQA)
num_key_value_heads = 8     # Share K,V across heads
# Faster inference, less memory!
```

**Standard vs Grouped Query Attention**:
```
Standard Multi-Head Attention (MHA):
Head 1: Q1, K1, V1
Head 2: Q2, K2, V2
...
Head 32: Q32, K32, V32
Memory: 32 √ó (K + V) = lots!

Grouped Query Attention (GQA):
Group 1 (Heads 1-4):  Q1, Q2, Q3, Q4  ‚Üí shared K1, V1
Group 2 (Heads 5-8):  Q5, Q6, Q7, Q8  ‚Üí shared K2, V2
...
Group 8 (Heads 29-32): Q29-Q32        ‚Üí shared K8, V8
Memory: 8 √ó (K + V) = 4x less!
```

**T·∫°i sao GQA t·ªët?**
- ‚úÖ Gi·∫£m memory (KV cache) 4x
- ‚úÖ Faster inference
- ‚úÖ Quality g·∫ßn nh∆∞ kh√¥ng gi·∫£m
- ‚úÖ C√≥ th·ªÉ support longer context

---

### 3.3. MLP (Feed-Forward Network)

```python
# Architecture
hidden_size = 4096
intermediate_size = 14336  # ~3.5√ó hidden_size

# SwiGLU activation
class MLP:
    gate_proj: Linear(4096 ‚Üí 14336)  # Gate
    up_proj:   Linear(4096 ‚Üí 14336)  # Up
    down_proj: Linear(14336 ‚Üí 4096)  # Down
    
    def forward(x):
        # SwiGLU: Swish(gate) √ó up
        return down_proj(
            swish(gate_proj(x)) √ó up_proj(x)
        )
```

**SwiGLU vs GELU**:
```python
# GELU (old transformers)
output = GELU(W1 √ó x) √ó W2

# SwiGLU (Llama 3.1) - better performance!
output = Swish(W_gate √ó x) √ó (W_up √ó x) √ó W_down
```

---

### 3.4. RoPE (Rotary Position Embedding)

**V·∫•n ƒë·ªÅ**: Transformer kh√¥ng c√≥ kh√°i ni·ªám v·ªÅ v·ªã tr√≠ tokens

**Gi·∫£i ph√°p**: RoPE - encode position info v√†o embeddings

```python
# RoPE mechanism
def apply_rope(q, k, position):
    # Rotate query and key based on position
    Œ∏ = position / 10000^(2i/d)  # Different freq for each dim
    
    # Apply rotation matrix
    q_rotated = rotate(q, Œ∏)
    k_rotated = rotate(k, Œ∏)
    
    return q_rotated, k_rotated

# Properties
- Relative position encoding
- Extrapolate to longer sequences
- Better than absolute position
```

**Llama 3.1 RoPE base frequency**:
```python
rope_theta = 500000  # Increased from 10000 in Llama 2
# Allows better extrapolation to 128K context!
```

---

## 4. Instruction Tuning Process

### 4.1. Supervised Fine-Tuning (SFT)

**Stage 1: SFT tr√™n instruction data**
```python
# Training examples
{
  "instruction": "Summarize this text",
  "input": "Long legal document...",
  "output": "Summary: The document states..."
}

{
  "instruction": "Answer the question",
  "input": "What is the maximum working hours?",
  "output": "According to labor law, maximum is 8 hours/day..."
}

# Loss function
loss = CrossEntropy(model_output, target_output)

# Result: Model learns to follow instructions
```

### 4.2. RLHF (Reinforcement Learning from Human Feedback)

**Stage 2: Learn from human preferences**
```python
# Step 1: Collect comparisons
Question: "Explain Vietnamese labor law"
Response A: "Labor law in Vietnam regulates..." (detailed)
Response B: "It's about work stuff" (vague)
Human preference: A > B ‚úÖ

# Step 2: Train reward model
reward_model(Response A) = 0.9  # High score
reward_model(Response B) = 0.3  # Low score

# Step 3: Optimize policy (PPO algorithm)
# Generate response ‚Üí get reward ‚Üí update model
# Maximize: E[reward(response)]

# Result: Model generates human-preferred responses
```

### 4.3. DPO (Direct Preference Optimization)

**Stage 3: Simpler alternative to RLHF**
```python
# Direct optimization without reward model
# Given: preferred response y_w, rejected y_l

loss = -log(œÉ(
    log(œÄ(y_w|x) / œÄ_ref(y_w|x)) - 
    log(œÄ(y_l|x) / œÄ_ref(y_l|x))
))

# Directly increase prob of y_w
# Directly decrease prob of y_l

# Result: Simpler, more stable than RLHF
```

---

## 5. Llama 3.1 8B Instruct Specifications

### 5.1. Model Architecture

```python
{
  "model_type": "llama",
  "architecture": "LlamaForCausalLM",
  
  # Size
  "num_parameters": "8.03B",  # 8 billion parameters
  "num_layers": 32,            # 32 transformer blocks
  
  # Dimensions
  "hidden_size": 4096,         # Hidden dimension
  "intermediate_size": 14336,  # FFN intermediate size
  "num_attention_heads": 32,   # Number of Q heads
  "num_key_value_heads": 8,    # Number of KV heads (GQA)
  "head_dim": 128,             # Dimension per head
  
  # Vocabulary
  "vocab_size": 128256,        # Tokenizer vocabulary
  
  # Context
  "max_position_embeddings": 131072,  # 128K tokens!
  
  # Position encoding
  "rope_theta": 500000,        # RoPE base frequency
  
  # Activation
  "hidden_act": "silu",        # SwiGLU activation
  
  # Normalization
  "rms_norm_eps": 1e-5,        # RMSNorm epsilon
  
  # Precision
  "torch_dtype": "bfloat16"    # BF16 by default
}
```

### 5.2. Model Size Breakdown

```python
# Parameter count per component
Embeddings:        128256 √ó 4096 = 525M params
Transformer blocks: 32 √ó 220M = 7.04B params
  ‚îú‚îÄ‚îÄ Attention:    32 √ó 100M = 3.2B params
  ‚îÇ   ‚îú‚îÄ‚îÄ Q proj:   4096 √ó 4096 √ó 32 = 512M
  ‚îÇ   ‚îú‚îÄ‚îÄ K proj:   4096 √ó 1024 √ó 32 = 128M (GQA!)
  ‚îÇ   ‚îú‚îÄ‚îÄ V proj:   4096 √ó 1024 √ó 32 = 128M (GQA!)
  ‚îÇ   ‚îî‚îÄ‚îÄ O proj:   4096 √ó 4096 √ó 32 = 512M
  ‚îî‚îÄ‚îÄ MLP:          32 √ó 120M = 3.84B params
      ‚îú‚îÄ‚îÄ Gate:     4096 √ó 14336 √ó 32 = 1.8B
      ‚îú‚îÄ‚îÄ Up:       4096 √ó 14336 √ó 32 = 1.8B
      ‚îî‚îÄ‚îÄ Down:     14336 √ó 4096 √ó 32 = 1.8B
LM Head:           128256 √ó 4096 = 525M params (tied with embeddings)

Total: ~8.03 billion parameters
```

### 5.3. Memory Requirements

```python
# Model weights only
FP32:  8.03B √ó 4 bytes = 32.12 GB
FP16:  8.03B √ó 2 bytes = 16.06 GB
BF16:  8.03B √ó 2 bytes = 16.06 GB
INT8:  8.03B √ó 1 byte  = 8.03 GB
INT4:  8.03B √ó 0.5 byte = 4.01 GB

# Inference (with KV cache, batch=1, seq=8192)
FP16: ~20 GB
BF16: ~20 GB

# Training (LoRA r=128, batch=32, seq=8192)
Activations: ~30 GB
Gradients: ~20 GB
Optimizer: ~10 GB
Total: ~80 GB (fits H200!)

# Training (full fine-tuning)
Would need: >200 GB (kh√¥ng kh·∫£ thi!)
```

---

## 6. Chat Template Format

### 6.1. Llama 3.1 Special Tokens

```python
# Special tokens
<|begin_of_text|>     # Start of conversation
<|end_of_text|>       # End of conversation
<|start_header_id|>   # Start of message header
<|end_header_id|>     # End of message header
<|eot_id|>            # End of turn (message)

# Token IDs
{
  "<|begin_of_text|>": 128000,
  "<|end_of_text|>": 128001,
  "<|start_header_id|>": 128006,
  "<|end_header_id|>": 128007,
  "<|eot_id|>": 128009
}
```

### 6.2. Message Format

**Single Turn**:
```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

What is the capital of France?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

The capital of France is Paris.<|eot_id|>
```

**Multi-turn Conversation**:
```
<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant.<|eot_id|><|start_header_id|>user<|end_header_id|>

What is 2+2?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

2+2 equals 4.<|eot_id|><|start_header_id|>user<|end_header_id|>

What about 3+3?<|eot_id|><|start_header_id|>assistant<|end_header_id|>

3+3 equals 6.<|eot_id|>
```

### 6.3. Vietnamese Legal Format (Our Use Case)

```python
# Training format
template = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant that answers questions about Vietnamese law.<|eot_id|><|start_header_id|>user<|end_header_id|>

{instruction} {input}<|eot_id|><|start_header_id|>assistant<|end_header_id|>

{output}<|eot_id|>"""

# Example
{
  "instruction": "ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ g√¨?",
  "input": "",
  "output": "ƒêi·ªÅu 10 quy ƒë·ªãnh v·ªÅ th·ªùi gi·ªù l√†m vi·ªác v√† th·ªùi gi·ªù ngh·ªâ ng∆°i..."
}

# Formatted:
"""<|begin_of_text|><|start_header_id|>system<|end_header_id|>

You are a helpful assistant that answers questions about Vietnamese law.<|eot_id|><|start_header_id|>user<|end_header_id|>

ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ g√¨? <|eot_id|><|start_header_id|>assistant<|end_header_id|>

ƒêi·ªÅu 10 quy ƒë·ªãnh v·ªÅ th·ªùi gi·ªù l√†m vi·ªác v√† th·ªùi gi·ªù ngh·ªâ ng∆°i...<|eot_id|>"""
```

---

## 7. Tokenizer Details

### 7.1. Tokenizer Type

```python
{
  "tokenizer_type": "tiktoken",  # Fast BPE tokenizer
  "vocab_size": 128256,          # Large vocabulary
  "model_max_length": 131072,    # 128K tokens
  "pad_token": "<|finetune_right_pad_id|>",
  "eos_token": "<|eot_id|>",
  "bos_token": "<|begin_of_text|>"
}
```

### 7.2. Tokenization Examples

**English**:
```python
text = "The capital of France is Paris."
tokens = tokenizer.encode(text)
# ["The", " capital", " of", " France", " is", " Paris", "."]
# [791, 6864, 315, 9822, 374, 12366, 13]
# 7 tokens

# Compression ratio: 7 tokens / 6 words ‚âà 1.17
```

**Vietnamese**:
```python
text = "ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ th·ªùi gi·ªù l√†m vi·ªác."
tokens = tokenizer.encode(text)
# ["ƒê", "i", "·ªÅu", " ", "10", " ", "c", "·ªßa", ...]
# Multiple tokens per word!
# ~2-3 tokens per Vietnamese word

# Example: 14 words ‚Üí ~30 tokens
# Compression ratio: 30/14 ‚âà 2.14
```

**Implications for max_seq_length**:
```python
# English: 8192 tokens ‚âà 7000 words
# Vietnamese: 8192 tokens ‚âà 3500-4000 words

# For same content coverage:
# Vietnamese needs higher max_seq_length!
max_seq_length = 8192  # Good for Vietnamese legal docs
```

---

## 8. Use Cases & Performance

### 8.1. Model Comparisons

| Model | Params | Context | Speed | Quality | VRAM | Use Case |
|-------|--------|---------|-------|---------|------|----------|
| Llama-3.1-8B | 8B | 128K | Fast | Good | 16GB | Edge, mobile |
| Llama-3.1-70B | 70B | 128K | Medium | Excellent | 140GB | Servers |
| Llama-3.1-405B | 405B | 128K | Slow | SOTA | 800GB+ | Cloud |

### 8.2. Benchmarks

**General Tasks**:
```
MMLU (Knowledge): 68.4%
HumanEval (Code): 62.2%
GSM8K (Math): 79.6%
```

**Vietnamese-Specific** (estimated):
```
Vietnamese Q&A: ~65-70%
Translation: ~70-75%
Summarization: ~70-75%
Legal Understanding: ~60-65% (before fine-tuning)
Legal Understanding: ~80-85% (after fine-tuning) ‚Üê OUR GOAL!
```

### 8.3. Why Llama-3.1-8B-Instruct for Vietnamese Legal?

**Advantages**:
1. ‚úÖ **Size**: 8B params ‚Üí fits consumer GPUs
2. ‚úÖ **Instruct-tuned**: Follows instructions naturally
3. ‚úÖ **Long context**: 128K tokens ‚Üí long legal docs
4. ‚úÖ **Multilingual**: Good Vietnamese support
5. ‚úÖ **Open source**: Commercial use allowed
6. ‚úÖ **Fine-tunable**: LoRA works excellently
7. ‚úÖ **Unsloth optimized**: 2-5x faster training

**Trade-offs**:
1. ‚ùå Not as good as 70B/405B (but fine-tunable!)
2. ‚ùå Vietnamese tokenization less efficient than English
3. ‚ùå May hallucinate without retrieval (‚Üí RAG needed!)

---

## 9. Comparison: Base vs Instruct Training

### Example: Vietnamese Legal Q&A

**Base Model (Llama-3.1-8B)**:
```python
Input: "ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ"
Output: "th·ªùi gi·ªù l√†m vi·ªác. ƒêi·ªÅu 11 quy ƒë·ªãnh v·ªÅ th·ªùi gi·ªù ngh·ªâ ng∆°i. 
         ƒêi·ªÅu 12 quy ƒë·ªãnh v·ªÅ l√†m th√™m gi·ªù..."
# Just continues the document! Not helpful for Q&A.
```

**Instruct Model (Llama-3.1-8B-Instruct) - Before Fine-tuning**:
```python
Input: "ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ g√¨?"
Output: "ƒêi·ªÅu 10 quy ƒë·ªãnh v·ªÅ th·ªùi gi·ªù l√†m vi·ªác b√¨nh th∆∞·ªùng kh√¥ng qu√° 
         8 gi·ªù m·ªôt ng√†y v√† 48 gi·ªù m·ªôt tu·∫ßn."
# Good! But may not cite exact legal text.
```

**Instruct Model - After Vietnamese Legal Fine-tuning**:
```python
Input: "ƒêi·ªÅu 10 c·ªßa B·ªô lu·∫≠t Lao ƒë·ªông quy ƒë·ªãnh v·ªÅ g√¨?"
Output: "Theo ƒêi·ªÅu 10 B·ªô lu·∫≠t Lao ƒë·ªông 2019:
         1. Th·ªùi gi·ªù l√†m vi·ªác b√¨nh th∆∞·ªùng kh√¥ng qu√° 8 gi·ªù trong 1 ng√†y 
            v√† 48 gi·ªù trong 1 tu·∫ßn.
         2. Ng∆∞·ªùi s·ª≠ d·ª•ng lao ƒë·ªông c√≥ quy·ªÅn quy ƒë·ªãnh th·ªùi gi·ªù l√†m vi·ªác 
            theo ng√†y ho·∫∑c tu·∫ßn nh∆∞ng ph·∫£i th√¥ng b√°o cho ng∆∞·ªùi lao ƒë·ªông.
         3. Tr∆∞·ªùng h·ª£p ƒë·∫∑c bi·ªát, c√≥ th·ªÉ √°p d·ª•ng khung gi·ªù l√†m vi·ªác kh√°c 
            theo quy ƒë·ªãnh t·∫°i ƒêi·ªÅu 105 c·ªßa B·ªô lu·∫≠t n√†y."
# Excellent! Accurate, detailed, with legal references!
```

---

## 10. Technical Innovations in Llama 3.1

### 10.1. Grouped Query Attention (GQA)

**Problem with Multi-Head Attention**:
```python
# 32 attention heads, 128K context
KV_cache = 32 heads √ó 128K tokens √ó 128 dims √ó 2 (K,V) √ó 2 bytes
         = 32 √ó 128000 √ó 128 √ó 2 √ó 2
         = 2.1 GB per sample!
# With batch_size=32: 67 GB just for KV cache!
```

**Solution: Share K,V across head groups**:
```python
# 8 KV heads (4 Q heads share 1 KV head)
KV_cache = 8 heads √ó 128K tokens √ó 128 dims √ó 2 (K,V) √ó 2 bytes
         = 8 √ó 128000 √ó 128 √ó 2 √ó 2
         = 524 MB per sample
# 4x reduction! 67 GB ‚Üí 17 GB
```

### 10.2. Extended Context (128K)

**Llama 2**: 4K context
**Llama 3**: 8K context
**Llama 3.1**: 128K context (16x increase!)

**How?**
1. **RoPE scaling**: Increase rope_theta to 500K
2. **Continued pre-training**: Train on longer sequences
3. **Position interpolation**: Better extrapolation

**Use cases**:
- Full legal documents (10-50 pages)
- Entire conversations (100+ turns)
- Long-form reasoning chains

---

## 11. Unsloth Optimizations

### What is Unsloth?

```python
# Normal Llama loading
from transformers import AutoModelForCausalLM
model = AutoModelForCausalLM.from_pretrained("meta-llama/Llama-3.1-8B")
# Slow, memory-hungry

# Unsloth-optimized Llama
from unsloth import FastLanguageModel
model, tokenizer = FastLanguageModel.from_pretrained(
    "unsloth/Llama-3.1-8B-Instruct"
)
# 2-5x faster, 50% less VRAM!
```

### Optimizations Applied:

1. **Flash Attention 2**:
   ```python
   # Standard attention: O(n¬≤) memory
   # Flash Attention 2: O(n) memory, 2-4x faster
   attn_implementation = "flash_attention_2"
   ```

2. **Kernel fusion**:
   ```python
   # Fuse operations to reduce memory transfers
   # LayerNorm + Linear ‚Üí Single kernel
   # RoPE + Attention ‚Üí Single kernel
   ```

3. **Optimized LoRA**:
   ```python
   # Custom CUDA kernels for LoRA
   # 30% faster backward pass
   ```

4. **Memory optimization**:
   ```python
   # Gradient checkpointing with optimal strategy
   # 50% less VRAM for same batch size
   ```

**Results**:
```
Standard training: 10 hours
Unsloth training: 4 hours (2.5x faster!)

Standard VRAM: 80 GB
Unsloth VRAM: 40 GB (2x less!)
```

---

## T·ªïng k·∫øt

### Llama-3.1-8B-Instruct l√† l·ª±a ch·ªçn t·ªët v√¨:

1. ‚úÖ **Instruct-tuned**: S·∫µn s√†ng follow instructions
2. ‚úÖ **Appropriate size**: 8B params ‚Üí fine-tunable on consumer GPUs
3. ‚úÖ **Long context**: 128K tokens ‚Üí full legal documents
4. ‚úÖ **Multilingual**: Good Vietnamese tokenization
5. ‚úÖ **Modern architecture**: GQA, RoPE, SwiGLU
6. ‚úÖ **Open source**: Commercial use allowed
7. ‚úÖ **Well-supported**: HuggingFace, Unsloth, TRL

### So v·ªõi Base Model:
- ‚ùå Base: Ch·ªâ bi·∫øt ti·∫øp t·ª•c text, kh√¥ng answer questions
- ‚úÖ Instruct: Hi·ªÉu v√† follow instructions naturally

### So v·ªõi model kh√°c:
- Llama-3.1-70B: Better quality, but needs 8x VRAM
- GPT-4: Better, but closed-source, expensive
- Gemini: Good, but API-only
- Vietnamese models: Smaller, less capable

### Fine-tuning cho Vietnamese Legal:
```
Llama-3.1-8B-Instruct (General Vietnamese: ~65%)
            ‚Üì Fine-tuning v·ªõi legal data
Vietnamese-Legal-Llama (Legal Vietnamese: ~85%)
            ‚Üì Integration v·ªõi RAG
Production System (Accuracy: ~90-95%)
```

---

**Congratulations!** üéâ B·∫°n ƒë√£ hi·ªÉu s√¢u v·ªÅ Llama 3.1 8B Instruct model!

**Next Steps**:
1. ƒê·ªçc l·∫°i 3 files ƒë·ªÉ consolidate knowledge
2. Experiment v·ªõi training script
3. Monitor training metrics
4. Evaluate fine-tuned model on Vietnamese legal tasks

**Happy Fine-tuning! üöÄ**
