# üöÄ H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng GPU Droplet cho Training + CPU Droplet cho Serving

## üéØ Chi·∫øn l∆∞·ª£c

**Training**: GPU Droplet (t·∫°m th·ªùi, x√≥a sau khi train xong) ‚Üí Ti·∫øt ki·ªám chi ph√≠
**Serving**: CPU Droplet (ch·∫°y l√¢u d√†i) ‚Üí ·ªîn ƒë·ªãnh v√† r·∫ª

## B∆∞·ªõc 1: T·∫°o GPU Droplet cho Training

### 1.1. V√†o Digital Ocean Dashboard
1. Login: https://cloud.digitalocean.com/
2. Click **"Create"** ‚Üí **"Droplets"**

### 1.2. C·∫•u h√¨nh GPU Droplet
1. **Operating System**: Ubuntu 22.04 (LTS) x64
2. **Plan**: Click tab **"Premium Intel with GPU"**
3. **GPU Options**:
   - **Basic GPU**: $72/month - 1 vCPU, 8GB RAM, 1 GPU (NVIDIA V100) ‚Üê Khuy·∫øn ngh·ªã
   - **Professional GPU**: $144/month - 2 vCPUs, 16GB RAM, 1 GPU (NVIDIA V100) ‚Üê N·∫øu c·∫ßn performance cao
4. **Datacenter**: Singapore (SGP1)
5. **Authentication**: SSH Key (khuy·∫øn ngh·ªã)
6. **Hostname**: `legal-ai-gpu-training`
7. **Tags**: `gpu`, `training`, `temporary`
8. Click **"Create Droplet"**

### 1.3. Ghi l·∫°i th√¥ng tin
- **GPU Droplet IP**: `_______________`

---

## B∆∞·ªõc 2: T·∫°o CPU Droplet cho Serving

### 2.1. T·∫°o CPU Droplet th·ª© 2
1. Click **"Create"** ‚Üí **"Droplets"** (l·∫ßn 2)
2. **Operating System**: Ubuntu 22.04 (LTS) x64
3. **Plan**: Basic - Regular with SSD
4. **Size**: 
   - **$24/month**: 4GB RAM, 2 vCPUs, 80GB SSD ‚Üê Cho serving c∆° b·∫£n
   - **$48/month**: 8GB RAM, 4 vCPUs, 160GB SSD ‚Üê Khuy·∫øn ngh·ªã cho performance t·ªët
5. **Datacenter**: Singapore (SGP1) (c√πng region)
6. **Authentication**: SSH Key (d√πng c√πng key)
7. **Hostname**: `legal-ai-cpu-serving`
8. **Tags**: `cpu`, `serving`, `production`
9. Click **"Create Droplet"**

### 2.2. Ghi l·∫°i th√¥ng tin
- **CPU Droplet IP**: `_______________`

---

## B∆∞·ªõc 3: Setup GPU Droplet cho Training

### 3.1. K·∫øt n·ªëi GPU Droplet
```bash
ssh root@GPU_DROPLET_IP
```

### 3.2. C√†i ƒë·∫∑t NVIDIA Drivers v√† CUDA
```bash
# Update system
apt update && apt upgrade -y

# Install NVIDIA drivers
apt install -y ubuntu-drivers-common
ubuntu-drivers autoinstall

# Reboot ƒë·ªÉ load drivers
reboot
```

**ƒê·ª£i 2-3 ph√∫t v√† k·∫øt n·ªëi l·∫°i:**
```bash
ssh root@GPU_DROPLET_IP
```

### 3.3. Verify GPU
```bash
# Check GPU
nvidia-smi
```

### 3.4. C√†i ƒë·∫∑t Docker v·ªõi GPU support
```bash
# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# Install NVIDIA Container Toolkit
distribution=$(. /etc/os-release;echo $ID$VERSION_ID) \
&& curl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | apt-key add - \
&& curl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | tee /etc/apt/sources.list.d/nvidia-docker.list

apt update && apt install -y nvidia-docker2
systemctl restart docker

# Test GPU in Docker
docker run --rm --gpus all nvidia/cuda:13.0.1-base-ubuntu22.04 nvidia-smi
```

---

## B∆∞·ªõc 4: Setup Repository tr√™n GPU Droplet

### 4.1. Clone repository
```bash
cd /root
git clone https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System.git
cd Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup
```

### 4.2. C·∫•u h√¨nh environment
```bash
cp .env.template .env
nano .env
```

### 4.3. T·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt
```bash
mkdir -p data models logs
```

---

## B∆∞·ªõc 5: Setup GPU Training Environment

### 5.1. Ki·ªÉm tra file training script
```bash
# Ki·ªÉm tra file training script c√≥ t·ªìn t·∫°i
ls -la train_embedding_gpu.py
```

### 5.2. Ki·ªÉm tra requirements file
```bash
# S·ª≠ d·ª•ng requirements_gpu.txt cho GPU training
ls -la requirements_gpu.txt
```
## B∆∞·ªõc 6: Training tr√™n GPU Droplet

### 6.1. Load environment variables
```bash
# Load environment variables
source .env

# Verify environment variables
echo "üîç Checking environment variables..."
echo "SPACES_ACCESS_KEY: ${SPACES_ACCESS_KEY:0:10}..." 
echo "SPACES_SECRET_KEY: ${SPACES_SECRET_KEY:0:10}..."
echo "SPACES_BUCKET: $SPACES_BUCKET"
echo "BASE_MODEL: $BASE_MODEL"
```

### 6.2. Build GPU image
```bash
# Build v·ªõi verbose output ƒë·ªÉ debug
docker build -f Dockerfile.gpu-training -t legal-embedding-gpu:latest . --no-cache

# Ki·ªÉm tra image ƒë√£ build th√†nh c√¥ng
docker images | grep legal-embedding-gpu
```

### 6.3. Test GPU access trong Docker
```bash
# Test GPU tr∆∞·ªõc khi training
docker run --rm --gpus all legal-embedding-gpu:latest nvidia-smi

# Test PyTorch GPU access
docker run --rm --gpus all legal-embedding-gpu:latest python -c "
import torch
print(f'CUDA available: {torch.cuda.is_available()}')
print(f'CUDA devices: {torch.cuda.device_count()}')
if torch.cuda.is_available():
    print(f'Device name: {torch.cuda.get_device_name(0)}')
"
```

### 6.4. Run training v·ªõi GPU (method 1: Direct run)
```bash
# Run training v·ªõi full config t·ª´ .env
docker run --gpus all \
  --name legal-gpu-training \
  -v $(pwd)/data:/tmp/data \
  -v $(pwd)/models:/tmp/model \
  -v $(pwd)/logs:/tmp/logs \
  -v $(pwd)/.env:/app/.env:ro \
  -e SPACES_ACCESS_KEY="$SPACES_ACCESS_KEY" \
  -e SPACES_SECRET_KEY="$SPACES_SECRET_KEY" \
  -e SPACES_ENDPOINT="$SPACES_ENDPOINT" \
  -e SPACES_BUCKET="$SPACES_BUCKET" \
  -e BASE_MODEL="$BASE_MODEL" \
  -e MODEL_PATH="$MODEL_PATH" \
  -e EPOCHS="$EPOCHS" \
  -e GPU_BATCH_SIZE="$GPU_BATCH_SIZE" \
  -e LEARNING_RATE="$LEARNING_RATE" \
  -e WARMUP_STEPS="$WARMUP_STEPS" \
  -e MAX_SEQ_LENGTH="$MAX_SEQ_LENGTH" \
  -e GRADIENT_ACCUMULATION_STEPS="$GRADIENT_ACCUMULATION_STEPS" \
  -e USE_FP16="$USE_FP16" \
  -e USE_GPU="$USE_GPU" \
  -e CUDA_VISIBLE_DEVICES="$CUDA_VISIBLE_DEVICES" \
  -e MAX_SAMPLES="$MAX_SAMPLES" \
  --rm \
  legal-embedding-gpu:latest
```


### 6.6. Monitor training (trong terminal kh√°c)
```bash
# Terminal 1: Monitor GPU usage
watch -n 5 nvidia-smi

# Terminal 2: Monitor Docker logs
docker logs -f legal-gpu-training

# Terminal 3: Monitor system resources
watch -n 10 'echo "=== CPU/Memory ===" && top -n 1 | head -5 && echo "=== Disk ===" && df -h'
```

### 6.7. Training progress tracking
```bash
# Ki·ªÉm tra training progress
ls -la models/
tail -f logs/training.log

# Ki·ªÉm tra model ƒë√£ upload l√™n Spaces
# (N·∫øu training script c√≥ auto-upload)
```

**üö® Troubleshooting:**
```bash
# N·∫øu training fail, check logs
docker logs legal-gpu-training

# Ki·ªÉm tra GPU memory
nvidia-smi

# Restart n·∫øu c·∫ßn
docker stop legal-gpu-training
docker rm legal-gpu-training
# R·ªìi ch·∫°y l·∫°i command ·ªü b∆∞·ªõc 6.4
```

---

## B∆∞·ªõc 7: Setup CPU Droplet cho Serving

### 7.1. K·∫øt n·ªëi CPU Droplet
```bash
ssh root@CPU_DROPLET_IP
```

### 7.2. C√†i ƒë·∫∑t Docker
```bash
# Update system
apt update && apt upgrade -y

# Install Docker
curl -fsSL https://get.docker.com -o get-docker.sh
sh get-docker.sh

# Verify Docker installation
docker --version
docker run hello-world
```

### 7.3. Clone repository
```bash
cd /root
git clone https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System.git
cd Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup
```

### 7.4. C·∫•u h√¨nh environment cho serving
```bash
# T·∫°o file .env.serving v·ªõi c·∫•u h√¨nh ƒë∆°n gi·∫£n
nano .env.serving
```

**N·ªôi dung file `.env.serving`:**
```bash
# API Configuration
API_HOST=0.0.0.0
API_PORT=5000
MAX_BATCH_SIZE=32

# Model Configuration
# S·ª≠ d·ª•ng baseline model BGE-M3 (RECOMMENDED)
# Model s·∫Ω ƒë∆∞·ª£c download t·ª± ƒë·ªông t·ª´ Hugging Face
MODEL_PATH=./models/bge-m3
```

**üí° L√Ω do s·ª≠ d·ª•ng baseline model:**
- ‚úÖ Performance t·ªët h∆°n fine-tuned model trong th·ª≠ nghi·ªám th·ª±c t·∫ø
- ‚úÖ Kh√¥ng c·∫ßn training, ti·∫øt ki·ªám chi ph√≠ GPU
- ‚úÖ Download nhanh t·ª´ Hugging Face (kh√¥ng c·∫ßn Spaces)
- ‚úÖ Model ·ªïn ƒë·ªãnh v√† ƒë∆∞·ª£c c·ªông ƒë·ªìng support t·ªët

### 7.5. T·∫°o th∆∞ m·ª•c c·∫ßn thi·∫øt
```bash
# T·∫°o th∆∞ m·ª•c models v√† logs
mkdir -p models logs
```

---

## B∆∞·ªõc 8: Download Baseline Model v√† Deploy API

### 8.1. Build Docker image
```bash
# ƒê·∫£m b·∫£o ƒëang ·ªü ƒë√∫ng th∆∞ m·ª•c
cd /root/Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup

# Build image v·ªõi all dependencies (bao g·ªìm huggingface_hub)
docker build -f Dockerfile.cpu-serving -t legal-embedding-serving:latest .

# Verify image ƒë√£ build th√†nh c√¥ng
docker images | grep legal-embedding-serving
```

**üí° Image n√†y bao g·ªìm:**
- Python 3.10
- PyTorch (CPU version)
- sentence-transformers
- transformers
- huggingface_hub (ƒë·ªÉ download model)
- Flask (cho API)
- Script `download_model_from_spaces.py`
- Script `serve_model.py`

### 8.2. Download baseline model BGE-M3 t·ª´ Hugging Face

**üéØ Chi·∫øn l∆∞·ª£c m·ªõi:** S·ª≠ d·ª•ng baseline model `BAAI/bge-m3` thay v√¨ fine-tuned model

```bash
# Download model b·∫±ng Docker container
docker run --rm \
  -v $(pwd)/models:/app/models \
  -v $(pwd)/logs:/app/logs \
  legal-embedding-serving:latest \
  python download_model_from_spaces.py

# Verify model ƒë√£ download
ls -lah models/bge-m3/
```

**ÔøΩ Output mong ƒë·ª£i:**
```
üöÄ Starting model download process...

======================================================================
üéØ DOWNLOADING BASELINE MODEL (RECOMMENDED)
======================================================================
üì¶ Model: BAAI/bge-m3
üìÅ Local directory: ./models/bge-m3
‚¨áÔ∏è  Downloading from Hugging Face...
Fetching 15 files: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 15/15 [02:30<00:00]
‚úÖ Baseline model downloaded successfully!
üìç Model path: ./models/bge-m3

üí° Model n√†y ch∆∞a ƒë∆∞·ª£c fine-tune, ph√π h·ª£p cho serving
   v√¨ baseline model c√≥ performance t·ªët h∆°n fine-tuned model.

======================================================================
‚ú® DOWNLOAD COMPLETED SUCCESSFULLY!
======================================================================
üìÇ Model location: ./models/bge-m3

üìã Next steps:
   1. S·ª≠ d·ª•ng model n√†y cho serving v·ªõi serve_model.py
   2. Model ƒë√£ s·∫µn s√†ng ƒë·ªÉ ph·ª•c v·ª• requests
```

**‚è∞ Th·ªùi gian download:**
- Model size: ~2.3GB
- Download time: 2-5 ph√∫t (t√πy network)

**üîç Ki·ªÉm tra structure c·ªßa model:**
```bash
# Xem c√°c file ƒë√£ download
tree -L 2 models/bge-m3/

# Expected structure:
# models/bge-m3/
# ‚îú‚îÄ‚îÄ config.json
# ‚îú‚îÄ‚îÄ config_sentence_transformers.json
# ‚îú‚îÄ‚îÄ model.safetensors
# ‚îú‚îÄ‚îÄ pytorch_model.bin
# ‚îú‚îÄ‚îÄ sentence_bert_config.json
# ‚îú‚îÄ‚îÄ tokenizer.json
# ‚îú‚îÄ‚îÄ tokenizer_config.json
# ‚îî‚îÄ‚îÄ vocab.txt
```

### 8.3. Deploy Serving API v·ªõi Docker Compose (Recommended)

**Method 1: S·ª≠ d·ª•ng Docker Compose (ƒê∆°n gi·∫£n nh·∫•t)**
```bash
# Start service
docker-compose -f docker-compose.serving.yml up -d

# Check logs realtime
docker-compose -f docker-compose.serving.yml logs -f

# Check service status
docker-compose -f docker-compose.serving.yml ps
```

**üìã Output khi API start th√†nh c√¥ng:**
```
legal-embedding-api | üì• Loading model from: ./models/bge-m3
legal-embedding-api | üíª Using device: cpu
legal-embedding-api | ‚úÖ Model loaded successfully!
legal-embedding-api | üìä Embedding dimension: 1024
legal-embedding-api |  * Running on http://0.0.0.0:5000
```

### 8.4. Ho·∫∑c deploy b·∫±ng Docker run (Alternative)

**Method 2: Ch·∫°y tr·ª±c ti·∫øp v·ªõi docker run**
```bash
# Run container serving API
docker run -d \
  --name legal-embedding-api \
  -p 5000:5000 \
  -v $(pwd)/models/bge-m3:/app/models/bge-m3 \
  -v $(pwd)/logs:/app/logs \
  -e MODEL_PATH=/app/models/bge-m3 \
  -e API_HOST=0.0.0.0 \
  -e API_PORT=5000 \
  -e MAX_BATCH_SIZE=32 \
  --restart unless-stopped \
  legal-embedding-serving:latest

# Monitor logs
docker logs -f legal-embedding-api

# Check container status
docker ps | grep legal-embedding-api
```

**üí° Gi·∫£i th√≠ch c√°c options:**
- `-d`: Ch·∫°y container ·ªü background
- `-p 5000:5000`: Map port 5000 ra ngo√†i
- `-v $(pwd)/models/bge-m3:/app/models/bge-m3`: Mount model directory
- `-e MODEL_PATH=/app/models/bge-m3`: Ch·ªâ ƒë·ªãnh path ƒë·∫øn model
- `--restart unless-stopped`: T·ª± ƒë·ªông restart khi droplet reboot

### 8.5. Verify API is running

**Test 1: Health check endpoint**
```bash
# Test t·ª´ trong droplet
curl http://localhost:5000/health
```

**Expected output:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "device": "cpu",
  "embedding_dim": 1024,
  "timestamp": 1730198400.123
}
```

**Test 2: Embedding endpoint**
```bash
# Test t·∫°o embeddings
curl -X POST http://localhost:5000/embed \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["Lu·∫≠t D√¢n s·ª± nƒÉm 2015"]
  }'
```

**Expected output:**
```json
{
  "embeddings": [[0.123, -0.456, 0.789, ...]],  // 1024 dimensions
  "embedding_dim": 1024,
  "num_texts": 1,
  "inference_time": 0.089
}
```

**‚è∞ Th·ªùi gian kh·ªüi ƒë·ªông API:**
- Model loading: 30-60 gi√¢y (l·∫ßn ƒë·∫ßu)
- API ready: 1-2 ph√∫t
- Requests ti·∫øp theo: < 0.1 gi√¢y/c√¢u

### 8.6. üî• C·∫•u h√¨nh Firewall (B·∫ÆT BU·ªòC!)

**‚ö†Ô∏è B∆∞·ªõc n√†y R·∫§T QUAN TR·ªåNG** - N·∫øu kh√¥ng l√†m th√¨ API ch·ªâ ch·∫°y local!

```bash
# Ki·ªÉm tra firewall status
ufw status

# QUAN TR·ªåNG: Allow SSH tr∆∞·ªõc khi enable firewall (tr√°nh b·ªã lock out!)
ufw allow OpenSSH
ufw allow 22/tcp

# Allow API port
ufw allow 5000/tcp

# Enable firewall
ufw --force enable

# Verify firewall rules
ufw status verbose
```

**üìã Expected output:**
```
Status: active

To                         Action      From
--                         ------      ----
22/tcp                     ALLOW       Anywhere
5000/tcp                   ALLOW       Anywhere
OpenSSH                    ALLOW       Anywhere
22/tcp (v6)               ALLOW       Anywhere (v6)
5000/tcp (v6)             ALLOW       Anywhere (v6)
OpenSSH (v6)              ALLOW       Anywhere (v6)
```

### 8.7. üåê Test API t·ª´ b√™n ngo√†i internet

**T·ª´ m√°y local c·ªßa b·∫°n (kh√¥ng ph·∫£i trong droplet):**

```bash
# Thay YOUR_DROPLET_IP b·∫±ng IP th·ª±c c·ªßa droplet
export DROPLET_IP="YOUR_DROPLET_IP"

# Test 1: Health check
curl http://$DROPLET_IP:5000/health

# Test 2: Generate embeddings
curl -X POST http://$DROPLET_IP:5000/embed \
  -H "Content-Type: application/json" \
  -d '{
    "texts": ["Lu·∫≠t D√¢n s·ª± nƒÉm 2015", "B·ªô lu·∫≠t H√¨nh s·ª± nƒÉm 2017"]
  }'

# Test 3: Calculate similarity
curl -X POST http://$DROPLET_IP:5000/similarity \
  -H "Content-Type: application/json" \
  -d '{
    "texts1": ["Lu·∫≠t v·ªÅ quy·ªÅn s·ªü h·ªØu t√†i s·∫£n"],
    "texts2": ["T√†i s·∫£n ri√™ng", "T√†i s·∫£n chung", "Quy·ªÅn k·∫ø th·ª´a"]
  }'
```

**‚úÖ N·∫øu th√†nh c√¥ng:**
- `/health` tr·∫£ v·ªÅ `"status": "healthy"`
- `/embed` tr·∫£ v·ªÅ array of embeddings (1024 dimensions)
- `/similarity` tr·∫£ v·ªÅ ma tr·∫≠n similarity scores

### 8.8. üìä Benchmark v√† Performance Testing

```bash
# Test performance v·ªõi multiple requests
for i in {1..10}; do
  echo "Request $i:"
  time curl -X POST http://localhost:5000/embed \
    -H "Content-Type: application/json" \
    -d '{
      "texts": ["Test sentence for benchmarking performance"]
    }' -s -o /dev/null
done

# Expected inference time:
# - Single sentence: 50-100ms
# - Batch 10 sentences: 200-400ms
# - Batch 32 sentences: 500-1000ms
```

**üí° Performance tips:**
- S·ª≠ d·ª•ng batch requests khi c√≥ nhi·ªÅu texts
- `MAX_BATCH_SIZE=32` l√† optimal cho 4GB RAM
- Upgrade l√™n 8GB RAM n·∫øu c·∫ßn x·ª≠ l√Ω batch l·ªõn h∆°n

### 8.9. ÔøΩ Troubleshooting Common Issues

**Issue 1: API kh√¥ng start ƒë∆∞·ª£c**
```bash
# N·∫øu kh√¥ng k·∫øt n·ªëi ƒë∆∞·ª£c t·ª´ b√™n ngo√†i:

# 1. Check container c√≥ ƒëang ch·∫°y kh√¥ng
docker ps | grep legal-embedding-api

# 2. Check port mapping
docker port legal-embedding-api

# 3. Check firewall
ufw status verbose

# 4. Check logs
docker logs legal-embedding-api

# 5. Test t·ª´ trong droplet tr∆∞·ªõc
curl http://localhost:5000/health

# 6. N·∫øu model kh√¥ng t·ªìn t·∫°i, download l·∫°i
docker run --rm \
  -v $(pwd)/models:/app/models \
  --env-file .env.serving \
  legal-embedding-serving:latest \
  python download_model_from_spaces.py

# 7. Restart API container
docker restart legal-embedding-api
```

**üí° L∆∞u √Ω b·∫£o m·∫≠t:**
- Port 5000 ƒëang m·ªü c√¥ng khai ra internet
- Consider th√™m authentication/API key n·∫øu c·∫ßn
- Ho·∫∑c ch·ªâ allow IP c·ª• th·ªÉ:
```bash
# Ch·ªâ allow t·ª´ IP c·ª• th·ªÉ
ufw delete allow 5000/tcp
ufw allow from YOUR_BACKEND_IP to any port 5000
```

---

## B∆∞·ªõc 9: Test v√† Monitor Serving API

### 9.1. Test nhanh v·ªõi script
```bash
# Tr√™n m√°y local, clone repo n·∫øu ch∆∞a c√≥
git clone https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System.git
cd Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup
```

### 9.2. Ch·∫°y test suite ƒë·∫ßy ƒë·ªß (Python)
```bash
# Test t·ª´ local (tr√™n m√°y local c·ªßa b·∫°n, kh√¥ng ph·∫£i droplet)
cd Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup
python3 test_api.py http://YOUR_DROPLET_IP:5000
```

### 9.3. Test th·ªß c√¥ng v·ªõi curl

**üîç Endpoint 1: Health Check**
```bash
# Check xem API c√≥ s·ªëng kh√¥ng
curl http://YOUR_DROPLET_IP:5000/health
```### 8.9. üîß Troubleshooting Common Issues

**Issue 1: API kh√¥ng start ƒë∆∞·ª£c**
```bash
# Check logs ƒë·ªÉ xem l·ªói g√¨
docker logs legal-embedding-api

# Common errors:
# 1. Model kh√¥ng t·ªìn t·∫°i -> Download l·∫°i model
# 2. Out of memory -> Gi·∫£m MAX_BATCH_SIZE ho·∫∑c upgrade droplet
# 3. Port conflict -> ƒê·ªïi API_PORT
```

**Solution cho Issue 1:**
```bash
# Download l·∫°i model n·∫øu b·ªã l·ªói
docker run --rm \
  -v $(pwd)/models:/app/models \
  legal-embedding-serving:latest \
  python download_model_from_spaces.py

# Restart API
docker restart legal-embedding-api
```

**Issue 2: Kh√¥ng connect ƒë∆∞·ª£c t·ª´ b√™n ngo√†i**
```bash
# Checklist:
# 1. Container c√≥ ƒëang ch·∫°y?
docker ps | grep legal-embedding-api

# 2. Port mapping ƒë√∫ng ch∆∞a?
docker port legal-embedding-api

# 3. Firewall ƒë√£ m·ªü ch∆∞a?
ufw status | grep 5000

# 4. Test t·ª´ trong droplet tr∆∞·ªõc
curl http://localhost:5000/health

# 5. N·∫øu local OK nh∆∞ng external fail -> Check firewall
ufw allow 5000/tcp
```

**Issue 3: Model download b·ªã l·ªói**
```bash
# L·ªói: Connection timeout, HTTP errors

# Solution 1: Check network
ping huggingface.co

# Solution 2: Retry download
docker run --rm \
  -v $(pwd)/models:/app/models \
  legal-embedding-serving:latest \
  python download_model_from_spaces.py

# Solution 3: Download tr·ª±c ti·∫øp b·∫±ng git (n·∫øu c·∫ßn)
cd models
git lfs install
git clone https://huggingface.co/BAAI/bge-m3
mv bge-m3 bge-m3-temp && mv bge-m3-temp/* bge-m3/ && rm -rf bge-m3-temp
```

**Issue 4: Performance ch·∫≠m**
```bash
# Check system resources
docker stats legal-embedding-api
htop

# Solutions:
# 1. Gi·∫£m batch size n·∫øu out of memory
# 2. Upgrade droplet l√™n 8GB RAM
# 3. Optimize concurrent requests
```

**Issue 5: Container b·ªã crash/restart li√™n t·ª•c**
```bash
# Check logs ƒë·ªÉ t√¨m root cause
docker logs --tail 100 legal-embedding-api

# Common causes:
# 1. OOM (Out of Memory) -> Gi·∫£m MAX_BATCH_SIZE
# 2. Model file corrupted -> Download l·∫°i
# 3. Disk full -> D·ªçn d·∫πp: docker system prune -a

# Check disk usage
df -h
```

---

## B∆∞·ªõc 9: Test v√† Monitor Serving API

### 9.1. Integration Testing v·ªõi Python

**T·∫°o file test script tr√™n m√°y local:**
```python
# test_embedding_api.py
import requests
import time

API_URL = "http://YOUR_DROPLET_IP:5000"  # Thay YOUR_DROPLET_IP

def test_health():
    """Test health endpoint"""
    print("üîç Testing /health endpoint...")
    response = requests.get(f"{API_URL}/health")
    print(f"Status: {response.status_code}")
    print(f"Response: {response.json()}")
    assert response.status_code == 200
    assert response.json()["model_loaded"] == True
    print("‚úÖ Health check passed!\n")

def test_embed_single():
    """Test embedding single text"""
    print("üîç Testing /embed with single text...")
    response = requests.post(
        f"{API_URL}/embed",
        json={"texts": ["Lu·∫≠t D√¢n s·ª± nƒÉm 2015"]}
    )
    print(f"Status: {response.status_code}")
    data = response.json()
    print(f"Embedding dim: {data['embedding_dim']}")
    print(f"Inference time: {data['inference_time']}s")
    assert response.status_code == 200
    assert len(data['embeddings']) == 1
    assert len(data['embeddings'][0]) == 1024
    print("‚úÖ Single text embedding passed!\n")

def test_embed_batch():
    """Test embedding batch of texts"""
    print("üîç Testing /embed with batch texts...")
    texts = [
        "Lu·∫≠t D√¢n s·ª± nƒÉm 2015",
        "B·ªô lu·∫≠t H√¨nh s·ª± nƒÉm 2017",
        "Lu·∫≠t ƒê·∫•t ƒëai nƒÉm 2013"
    ]
    response = requests.post(
        f"{API_URL}/embed",
        json={"texts": texts}
    )
    data = response.json()
    print(f"Status: {response.status_code}")
    print(f"Processed {data['num_texts']} texts")
    print(f"Inference time: {data['inference_time']}s")
    assert response.status_code == 200
    assert len(data['embeddings']) == 3
    print("‚úÖ Batch embedding passed!\n")

def test_similarity():
    """Test similarity calculation"""
    print("üîç Testing /similarity endpoint...")
    response = requests.post(
        f"{API_URL}/similarity",
        json={
            "texts1": ["Quy·ªÅn s·ªü h·ªØu t√†i s·∫£n"],
            "texts2": ["T√†i s·∫£n ri√™ng", "T√†i s·∫£n chung", "Quy·ªÅn k·∫ø th·ª´a"]
        }
    )
    data = response.json()
    print(f"Status: {response.status_code}")
    print(f"Similarities: {data['similarities']}")
    print(f"Inference time: {data['inference_time']}s")
    assert response.status_code == 200
    assert len(data['similarities']) == 1
    assert len(data['similarities'][0]) == 3
    print("‚úÖ Similarity calculation passed!\n")

def benchmark_performance():
    """Benchmark API performance"""
    print("üîç Benchmarking performance...")
    texts = ["Test sentence"] * 10
    
    times = []
    for i in range(5):
        start = time.time()
        response = requests.post(
            f"{API_URL}/embed",
            json={"texts": texts}
        )
        elapsed = time.time() - start
        times.append(elapsed)
        print(f"Request {i+1}: {elapsed:.3f}s")
    
    avg_time = sum(times) / len(times)
    print(f"\nüìä Average time for 10 texts: {avg_time:.3f}s")
    print(f"üìä Throughput: {10/avg_time:.1f} texts/second")
    print("‚úÖ Benchmark completed!\n")

if __name__ == "__main__":
    try:
        test_health()
        test_embed_single()
        test_embed_batch()
        test_similarity()
        benchmark_performance()
        
        print("=" * 60)
        print("üéâ ALL TESTS PASSED!")
        print("=" * 60)
    except Exception as e:
        print(f"\n‚ùå TEST FAILED: {e}")
```

**Ch·∫°y test:**
```bash
# Tr√™n m√°y local
python test_embedding_api.py
```

### 9.2. Monitor API v·ªõi Docker

```bash
# Monitor logs realtime
docker logs -f legal-embedding-api

# Monitor system resources
docker stats legal-embedding-api

# Check container info
docker inspect legal-embedding-api
```

### 9.3. Setup monitoring script trong droplet

```bash
# T·∫°o monitoring script
cat > /root/monitor_api.sh << 'EOF'
#!/bin/bash
LOG_FILE="/root/api_monitor.log"

while true; do
    echo "=== $(date) ===" >> $LOG_FILE
    
    # Check API health
    health=$(curl -s http://localhost:5000/health)
    if echo "$health" | grep -q "healthy"; then
        echo "‚úÖ API is healthy" >> $LOG_FILE
    else
        echo "‚ùå API is DOWN!" >> $LOG_FILE
        # Optional: Restart container
        # docker restart legal-embedding-api
    fi
    
    # Log system stats
    docker stats --no-stream legal-embedding-api >> $LOG_FILE
    
    echo "" >> $LOG_FILE
    sleep 300  # Check every 5 minutes
done
EOF

chmod +x /root/monitor_api.sh

# Run monitor trong background
nohup /root/monitor_api.sh &

# Check monitor log
tail -f /root/api_monitor.log
```

---

## B∆∞·ªõc 10: Best Practices v√† Maintenance

### 10.1. X√≥a GPU Droplet sau khi training xong (Ti·∫øt ki·ªám chi ph√≠)

```bash
# ‚ö†Ô∏è L∆ØU √ù: Ch·ªâ x√≥a GPU droplet SAU KHI ƒë√£ verify model ho·∫°t ƒë·ªông t·ªët

# V√†o Digital Ocean Dashboard:
# 1. Ch·ªçn GPU Droplet
# 2. Click "Destroy"
# 3. Confirm deletion

# üí∞ L√Ω do: GPU Droplet r·∫•t ƒë·∫Øt ($72-144/month)
# V·ªõi chi·∫øn l∆∞·ª£c baseline model, kh√¥ng c·∫ßn training n√™n kh√¥ng c·∫ßn GPU!
```

### 10.2. Update Model khi c·∫ßn thi·∫øt

**Scenario 1: C√≥ baseline model m·ªõi h∆°n t·ª´ Hugging Face**
```bash
# SSH v√†o CPU droplet
ssh root@CPU_DROPLET_IP
cd /root/Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup

# Backup model c≈© (optional)
mv models/bge-m3 models/bge-m3_backup_$(date +%Y%m%d)

# Download model m·ªõi
docker run --rm \
  -v $(pwd)/models:/app/models \
  legal-embedding-serving:latest \
  python download_model_from_spaces.py

# Restart API service
docker restart legal-embedding-api
# Ho·∫∑c v·ªõi docker-compose:
# docker-compose -f docker-compose.serving.yml restart

# Verify
curl http://localhost:5000/health
```

**Scenario 2: Mu·ªën th·ª≠ model kh√°c (v√≠ d·ª•: bge-large)**
```python
# Edit file download_model_from_spaces.py
# Thay ƒë·ªïi MODEL_NAME v√† LOCAL_DIR trong h√†m main():
MODEL_NAME = "BAAI/bge-large-en-v1.5"  # Ho·∫∑c model kh√°c
LOCAL_DIR = "./models/bge-large"

# Sau ƒë√≥ download v√† update MODEL_PATH trong .env.serving
```

### 10.3. Auto-restart v√† High Availability

### 9.4. Monitor system resources
```bash
# Monitor CPU, Memory
watch -n 5 'top -n 1 | head -20'

# Monitor Docker stats
docker stats legal-embedding-api

# Check disk usage
df -h
```

### 9.5. C·∫•u h√¨nh Firewall (B·∫ÆT BU·ªòC cho production)
```bash
# Ki·ªÉm tra firewall hi·ªán t·∫°i
ufw status

# Allow SSH (QUAN TR·ªåNG - ph·∫£i l√†m tr∆∞·ªõc!)
ufw allow OpenSSH
ufw allow 22/tcp

# Allow API port
ufw allow 5000/tcp

# Enable firewall
ufw --force enable

# Verify
ufw status verbose
```

**üîí T√πy ch·ªçn b·∫£o m·∫≠t cao h∆°n:**
```bash
### 10.3. Auto-restart v√† High Availability

**Docker ƒë√£ config auto-restart policy:**
```bash
# V·ªõi docker-compose.serving.yml
restart: unless-stopped

# Container s·∫Ω t·ª± ƒë·ªông restart khi:
# - Droplet reboot
# - Container crash
# - Docker daemon restart
```

**Manual restart khi c·∫ßn:**
```bash
# Restart container
docker restart legal-embedding-api

# Ho·∫∑c v·ªõi docker-compose
docker-compose -f docker-compose.serving.yml restart

# Check status sau restart
docker ps | grep legal-embedding-api
curl http://localhost:5000/health
```

### 10.4. Security Best Practices

**‚úÖ Firewall Configuration:**
```bash
# Ch·ªâ allow t·ª´ backend server c·ª• th·ªÉ (RECOMMENDED cho production)
ufw delete allow 5000/tcp
ufw allow from YOUR_BACKEND_SERVER_IP to any port 5000 proto tcp

# Ho·∫∑c allow t·ª´ m·ªôt subnet
ufw allow from 10.0.0.0/16 to any port 5000 proto tcp

# Rate limiting ƒë·ªÉ ch·ªëng DDoS
ufw limit 5000/tcp comment 'Rate limit API requests'
```

**‚úÖ SSH Security:**
```bash
# Disable password authentication
nano /etc/ssh/sshd_config
# Set: PasswordAuthentication no
systemctl restart sshd

# Ch·ªâ allow SSH t·ª´ IP c·ª• th·ªÉ
ufw delete allow 22/tcp
ufw allow from YOUR_IP to any port 22 proto tcp
```

**‚úÖ Regular Updates:**
```bash
# Setup auto-update cho security patches
apt install unattended-upgrades
dpkg-reconfigure -plow unattended-upgrades

# Manual update ƒë·ªãnh k·ª≥
apt update && apt upgrade -y
apt autoremove -y
```

### 10.5. Performance Optimization

**Monitor resource usage:**
```bash
# Real-time monitoring
htop
docker stats legal-embedding-api

# Check disk usage
df -h
du -sh /root/Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup/models/

# Check memory
free -h
```

**Optimize based on workload:**
```bash
# N·∫øu RAM usage cao -> Gi·∫£m batch size
# Edit .env.serving:
MAX_BATCH_SIZE=16  # Thay v√¨ 32

# Restart API
docker restart legal-embedding-api

# N·∫øu CPU usage cao -> Consider upgrade droplet
# $24/month (2 vCPUs) -> $48/month (4 vCPUs)
```

### 10.6. Backup Strategy

**Backup c·∫•u h√¨nh quan tr·ªçng:**
```bash
# T·∫°o backup script
cat > /root/backup.sh << 'EOF'
#!/bin/bash
BACKUP_DIR="/root/backups"
DATE=$(date +%Y%m%d_%H%M%S)

mkdir -p $BACKUP_DIR

# Backup configs
tar -czf $BACKUP_DIR/configs_$DATE.tar.gz \
  /root/Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup/.env.serving \
  /root/Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup/docker-compose.serving.yml

# Backup logs (last 7 days)
tar -czf $BACKUP_DIR/logs_$DATE.tar.gz \
  /root/Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup/logs/

# Keep only last 7 backups
cd $BACKUP_DIR && ls -t | tail -n +8 | xargs rm -f

echo "Backup completed: $DATE"
EOF

chmod +x /root/backup.sh

# Setup cron job (daily backup at 2 AM)
crontab -e
# Add line:
# 0 2 * * * /root/backup.sh >> /var/log/backup.log 2>&1
```

---

## B∆∞·ªõc 11: Integration v·ªõi Backend c·ªßa b·∫°n

### 11.1. Python Integration Example

```python
# embedding_client.py
import requests
from typing import List, Tuple
import numpy as np

class EmbeddingClient:
    def __init__(self, api_url: str):
        """
        Client ƒë·ªÉ t∆∞∆°ng t√°c v·ªõi Embedding API
        
        Args:
            api_url: URL c·ªßa API (v√≠ d·ª•: "http://YOUR_DROPLET_IP:5000")
        """
        self.api_url = api_url.rstrip('/')
        
    def health_check(self) -> bool:
        """Ki·ªÉm tra API c√≥ s·ªëng kh√¥ng"""
        try:
            response = requests.get(f"{self.api_url}/health", timeout=5)
            return response.json().get("model_loaded", False)
        except:
            return False
    
    def embed(self, texts: List[str]) -> np.ndarray:
        """
        T·∫°o embeddings cho list of texts
        
        Args:
            texts: List of texts c·∫ßn embedding
            
        Returns:
            numpy array shape (len(texts), 1024)
        """
        response = requests.post(
            f"{self.api_url}/embed",
            json={"texts": texts},
            timeout=30
        )
        response.raise_for_status()
        embeddings = response.json()["embeddings"]
        return np.array(embeddings)
    
    def similarity(self, texts1: List[str], texts2: List[str]) -> np.ndarray:
        """
        T√≠nh similarity gi·ªØa 2 lists of texts
        
        Returns:
            numpy array shape (len(texts1), len(texts2))
        """
        response = requests.post(
            f"{self.api_url}/similarity",
            json={"texts1": texts1, "texts2": texts2},
            timeout=30
        )
        response.raise_for_status()
        similarities = response.json()["similarities"]
        return np.array(similarities)
    
    def find_most_similar(self, query: str, candidates: List[str], top_k: int = 5) -> List[Tuple[int, str, float]]:
        """
        T√¨m top-k candidates gi·ªëng query nh·∫•t
        
        Returns:
            List of (index, text, score)
        """
        similarities = self.similarity([query], candidates)[0]
        
        # Get top-k indices
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = [
            (idx, candidates[idx], similarities[idx])
            for idx in top_indices
        ]
        
        return results

# Usage example
if __name__ == "__main__":
    client = EmbeddingClient("http://YOUR_DROPLET_IP:5000")
    
    # Check health
    if not client.health_check():
        print("API is not available!")
        exit(1)
    
    # Example: RAG search
    query = "Quy ƒë·ªãnh v·ªÅ quy·ªÅn s·ªü h·ªØu ƒë·∫•t ƒëai"
    documents = [
        "Lu·∫≠t ƒê·∫•t ƒëai 2013 quy ƒë·ªãnh v·ªÅ quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t",
        "B·ªô lu·∫≠t H√¨nh s·ª± v·ªÅ t·ªôi x√¢m ph·∫°m t√†i s·∫£n",
        "Lu·∫≠t D√¢n s·ª± v·ªÅ quy·ªÅn s·ªü h·ªØu t√†i s·∫£n",
        "Lu·∫≠t Nh√† ·ªü v·ªÅ quy·ªÅn s·ªü h·ªØu nh√†",
    ]
    
    results = client.find_most_similar(query, documents, top_k=3)
    
    print(f"Query: {query}\n")
    print("Top 3 most similar documents:")
    for rank, (idx, text, score) in enumerate(results, 1):
        print(f"{rank}. [{score:.3f}] {text}")
```

### 11.2. Integrate v√†o RAG System

```python
# rag_with_embedding_api.py
from embedding_client import EmbeddingClient
import numpy as np
from typing import List, Dict

class RAGSystem:
    def __init__(self, embedding_api_url: str, corpus: List[Dict[str, str]]):
        """
        RAG System s·ª≠ d·ª•ng external embedding API
        
        Args:
            embedding_api_url: URL c·ªßa embedding API
            corpus: List of documents, m·ªói document c√≥ 'id' v√† 'text'
        """
        self.client = EmbeddingClient(embedding_api_url)
        self.corpus = corpus
        
        # Pre-compute embeddings cho corpus
        print("Computing corpus embeddings...")
        corpus_texts = [doc['text'] for doc in corpus]
        self.corpus_embeddings = self.client.embed(corpus_texts)
        print(f"Embedded {len(corpus)} documents")
    
    def search(self, query: str, top_k: int = 5) -> List[Dict]:
        """
        Search relevant documents cho query
        
        Returns:
            List of documents v·ªõi similarity scores
        """
        # Get query embedding
        query_embedding = self.client.embed([query])[0]
        
        # Calculate similarities
        similarities = np.dot(self.corpus_embeddings, query_embedding)
        
        # Get top-k
        top_indices = np.argsort(similarities)[::-1][:top_k]
        
        results = [
            {
                **self.corpus[idx],
                'score': float(similarities[idx])
            }
            for idx in top_indices
        ]
        
        return results

# Usage
if __name__ == "__main__":
    # Sample corpus
    corpus = [
        {"id": "doc1", "text": "Lu·∫≠t ƒê·∫•t ƒëai 2013 quy ƒë·ªãnh v·ªÅ quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t"},
        {"id": "doc2", "text": "B·ªô lu·∫≠t H√¨nh s·ª± v·ªÅ t·ªôi x√¢m ph·∫°m t√†i s·∫£n"},
        {"id": "doc3", "text": "Lu·∫≠t D√¢n s·ª± v·ªÅ quy·ªÅn s·ªü h·ªØu t√†i s·∫£n"},
    ]
    
    rag = RAGSystem("http://YOUR_DROPLET_IP:5000", corpus)
    
    # Search
    results = rag.search("quy·ªÅn s·ªü h·ªØu ƒë·∫•t ƒëai", top_k=2)
    
    print("\nSearch Results:")
    for result in results:
        print(f"[{result['score']:.3f}] {result['id']}: {result['text']}")
```

---

## üìä T·ªïng k·∫øt Chi ph√≠ v√† Performance

### Chi ph√≠ h√†ng th√°ng

| Service | Configuration | Cost/month | Note |
|---------|--------------|------------|------|
| **GPU Droplet** | V100, 8GB RAM | ~~$72~~ **$0** | ‚ùå Kh√¥ng c·∫ßn! (d√πng baseline model) |
| **CPU Droplet** | 4GB RAM, 2 vCPUs | $24 | ‚úÖ Cho serving c∆° b·∫£n |
| **CPU Droplet** | 8GB RAM, 4 vCPUs | $48 | ‚úÖ RECOMMENDED |
| **Storage** | N/A | $0 | Model ~2.3GB, trong droplet |

**üí∞ Total: $24-48/month** (kh√¥ng c·∫ßn GPU!)

### Performance Metrics (BGE-M3 on CPU)

| Metric | 4GB Droplet | 8GB Droplet |
|--------|-------------|-------------|
| Single text (avg) | 80-100ms | 60-80ms |
| Batch 10 texts | 300-400ms | 250-350ms |
| Batch 32 texts | 800-1000ms | 600-800ms |
| Max throughput | ~25 texts/sec | ~35 texts/sec |
| Embedding dim | 1024 | 1024 |

### So s√°nh Baseline vs Fine-tuned

| Aspect | Baseline BGE-M3 | Fine-tuned Model |
|--------|----------------|------------------|
| **Setup cost** | $0 (no training) | $72-144 (GPU training) |
| **Setup time** | 5 ph√∫t (download) | 2-4 gi·ªù (training) |
| **Model size** | 2.3GB | 2.3GB |
| **Performance** | ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê Excellent | ‚≠ê‚≠ê‚≠ê‚≠ê Good |
| **Serving cost** | $24-48/month | $24-48/month |
| **Maintenance** | ‚úÖ Easy | ‚ö†Ô∏è Complex |

**üéØ K·∫øt lu·∫≠n: Baseline model l√† l·ª±a ch·ªçn t·ªët nh·∫•t!**

---

## üéâ Ho√†n th√†nh!

B·∫°n ƒë√£ setup th√†nh c√¥ng h·ªá th·ªëng Embedding API v·ªõi:
- ‚úÖ **Baseline BGE-M3 model** - Performance t·ªët nh·∫•t
- ‚úÖ **Kh√¥ng c·∫ßn GPU** - Ti·∫øt ki·ªám $72-144/month
- ‚úÖ **CPU Droplet serving 24/7** - Ch·ªâ $24-48/month
- ‚úÖ **Auto-restart** - High availability
- ‚úÖ **Production-ready** - Security, monitoring, backup

**Next steps:**
- Integrate API v√†o backend c·ªßa b·∫°n
- Setup monitoring v√† alerting
- Consider load balancer n·∫øu traffic cao
- Add authentication/API key n·∫øu c·∫ßn

**Support:**
- GitHub Issues: https://github.com/mikeethanh/Vietnamese-Legal-Chatbot-RAG-System/issues
- Documentation: README.md trong repo

chmod +x /root/monitor.sh

# Run in background
nohup /root/monitor.sh > /root/monitor.log 2>&1 &
```

### 10.5. Troubleshooting Common Issues

**API kh√¥ng start:**
```bash
# Check logs
docker logs legal-embedding-api

# Common issues:
# 1. Model kh√¥ng t·ªìn t·∫°i -> Download l·∫°i
# 2. Out of memory -> Reduce MAX_BATCH_SIZE ho·∫∑c upgrade droplet
# 3. Port conflict -> Change API_PORT trong .env.serving
```

**Performance ch·∫≠m:**
```bash
# Check system resources
htop
docker stats

# Solutions:
# 1. Upgrade to bigger droplet (4GB -> 8GB RAM)
# 2. Reduce MAX_BATCH_SIZE
# 3. Optimize model (quantization - advanced)
```

**Model outdated:**
```bash
# Download v√† deploy model m·ªõi b·∫±ng Docker container
cd /root/Vietnamese-Legal-Chatbot-RAG-System/digital_ocean_setup
source .env.serving

docker run --rm \
  -v $(pwd)/models:/app/models \
  --env-file .env.serving \
  legal-embedding-serving:latest \
  python download_model_from_spaces.py

docker-compose -f docker-compose.serving.yml restart
# Ho·∫∑c: docker restart legal-embedding-api
```

---

## üìä Chi ph√≠ d·ª± ki·∫øn

| Service | Size | Cost/month | Usage |
|---------|------|------------|-------|
| **GPU Droplet** | V100, 8GB RAM | $72 | **T·∫°m th·ªùi** (1-2 gi·ªù) ‚âà $0.1 |
| **CPU Droplet** | 4GB RAM, 2 vCPUs | $24 | **L√¢u d√†i** |
| **CPU Droplet** | 8GB RAM, 4 vCPUs | $48 | **Recommended** |
| **Spaces Storage** | 250GB | $5 | Models + Data |

**üí∞ Total Cost: ~$29-53/month** (ch·ªâ tr·∫£ CPU serving + storage)

---

## üéâ Ho√†n th√†nh!

B·∫°n ƒë√£ c√≥:
- ‚úÖ GPU Droplet ƒë·ªÉ training (x√≥a sau khi xong)
- ‚úÖ Model ƒë∆∞·ª£c l∆∞u an to√†n tr√™n Spaces
- ‚úÖ CPU Droplet serving API 24/7
- ‚úÖ Chi ph√≠ t·ªëi ∆∞u (~$29-53/month)

**Next steps:**
- Integrate API v√†o backend c·ªßa b·∫°n
- Setup monitoring & alerting
- Consider load balancer n·∫øu traffic cao

