# üì° Legal Embedding API - H∆∞·ªõng d·∫´n s·ª≠ d·ª•ng

API endpoint ƒë·ªÉ generate embeddings v√† t√≠nh similarity cho vƒÉn b·∫£n ph√°p lu·∫≠t ti·∫øng Vi·ªát.

## üåê Base URL

```
http://YOUR_DROPLET_IP:5000
```

## üìã Endpoints

### 1. Health Check

**Endpoint:** `GET /health`

**M√¥ t·∫£:** Ki·ªÉm tra tr·∫°ng th√°i API v√† model

**Request:**
```bash
curl http://YOUR_DROPLET_IP:5000/health
```

**Response:**
```json
{
  "status": "healthy",
  "model_loaded": true,
  "device": "cpu",
  "embedding_dim": 1024,
  "timestamp": 1761541371.844
}
```

---

### 2. Generate Embeddings

**Endpoint:** `POST /embed`

**M√¥ t·∫£:** Chuy·ªÉn ƒë·ªïi text th√†nh embedding vectors (1024 dimensions)

**Request:**
```bash
curl -X POST http://YOUR_DROPLET_IP:5000/embed \
  -H "Content-Type: application/json" \
  -d '{
    "texts": [
      "Lu·∫≠t D√¢n s·ª± nƒÉm 2015 quy ƒë·ªãnh v·ªÅ quy·ªÅn s·ªü h·ªØu",
      "B·ªô lu·∫≠t H√¨nh s·ª± nƒÉm 2017"
    ]
  }'
```

**Parameters:**
- `texts` (array of strings, required): Danh s√°ch c√°c text c·∫ßn embedding
- `batch_size` (int, optional): Batch size cho processing (default: 32)
- `normalize` (bool, optional): Normalize embeddings (default: true)

**Response:**
```json
{
  "embeddings": [
    [0.123, -0.456, 0.789, ..., 0.321],  // 1024 dimensions
    [0.234, -0.567, 0.890, ..., 0.432]   // 1024 dimensions
  ],
  "processing_time": 0.123,
  "count": 2,
  "embedding_dim": 1024
}
```

**Error Response:**
```json
{
  "error": "texts field is required",
  "status": "error"
}
```

---

### 3. Calculate Similarity

**Endpoint:** `POST /similarity`

**M√¥ t·∫£:** T√≠nh cosine similarity gi·ªØa 2 t·∫≠p text

**Request:**
```bash
curl -X POST http://YOUR_DROPLET_IP:5000/similarity \
  -H "Content-Type: application/json" \
  -d '{
    "texts1": ["Quy·ªÅn s·ªü h·ªØu t√†i s·∫£n trong Lu·∫≠t D√¢n s·ª±"],
    "texts2": [
      "Quy ƒë·ªãnh v·ªÅ t√†i s·∫£n chung",
      "Lu·∫≠t H√¨nh s·ª± v·ªÅ t·ªôi ph·∫°m",
      "Lu·∫≠t ƒê·∫•t ƒëai nƒÉm 2013"
    ]
  }'
```

**Parameters:**
- `texts1` (array of strings, required): T·∫≠p text th·ª© nh·∫•t
- `texts2` (array of strings, required): T·∫≠p text th·ª© hai

**Response:**
```json
{
  "similarities": [
    [0.85, 0.23, 0.67]  // similarity matrix: [len(texts1), len(texts2)]
  ],
  "processing_time": 0.089,
  "shape": [1, 3]
}
```

**Gi·∫£i th√≠ch:**
- `similarities[i][j]` = cosine similarity gi·ªØa `texts1[i]` v√† `texts2[j]`
- Gi√° tr·ªã t·ª´ -1 ƒë·∫øn 1 (c√†ng g·∫ßn 1 c√†ng gi·ªëng nhau)

---

## üíª Code Examples

### Python

**1. Basic Usage:**
```python
import requests

API_URL = "http://YOUR_DROPLET_IP:5000"

# Generate embeddings
response = requests.post(
    f"{API_URL}/embed",
    json={"texts": ["Lu·∫≠t D√¢n s·ª±", "B·ªô lu·∫≠t H√¨nh s·ª±"]}
)
data = response.json()
embeddings = data["embeddings"]
print(f"Got {len(embeddings)} embeddings of dimension {data['embedding_dim']}")
```

**2. Semantic Search:**
```python
import requests
import numpy as np

API_URL = "http://YOUR_DROPLET_IP:5000"

def semantic_search(query: str, documents: list[str], top_k: int = 5):
    """
    T√¨m top-k documents gi·ªëng nh·∫•t v·ªõi query
    """
    # Calculate similarities
    response = requests.post(
        f"{API_URL}/similarity",
        json={"texts1": [query], "texts2": documents}
    )
    
    similarities = np.array(response.json()["similarities"][0])
    
    # Get top-k indices
    top_indices = np.argsort(similarities)[::-1][:top_k]
    
    results = [
        {
            "document": documents[idx],
            "score": float(similarities[idx]),
            "rank": i + 1
        }
        for i, idx in enumerate(top_indices)
    ]
    
    return results

# Example usage
query = "Quy ƒë·ªãnh v·ªÅ quy·ªÅn s·ªü h·ªØu t√†i s·∫£n"
documents = [
    "Lu·∫≠t D√¢n s·ª± 2015 quy ƒë·ªãnh v·ªÅ quy·ªÅn s·ªü h·ªØu",
    "B·ªô lu·∫≠t H√¨nh s·ª± v·ªÅ t·ªôi ph·∫°m t√†i s·∫£n",
    "Lu·∫≠t ƒê·∫•t ƒëai 2013 v·ªÅ quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t",
    "Lu·∫≠t Nh√† ·ªü v·ªÅ quy·ªÅn s·ªü h·ªØu nh√†",
]

results = semantic_search(query, documents, top_k=3)

for result in results:
    print(f"{result['rank']}. {result['document']}")
    print(f"   Score: {result['score']:.3f}\n")
```

**3. Clustering Documents:**
```python
import requests
import numpy as np
from sklearn.cluster import KMeans

API_URL = "http://YOUR_DROPLET_IP:5000"

def cluster_documents(documents: list[str], n_clusters: int = 3):
    """
    Gom nh√≥m documents d·ª±a tr√™n embeddings
    """
    # Get embeddings
    response = requests.post(
        f"{API_URL}/embed",
        json={"texts": documents}
    )
    embeddings = np.array(response.json()["embeddings"])
    
    # Clustering
    kmeans = KMeans(n_clusters=n_clusters, random_state=42)
    labels = kmeans.fit_predict(embeddings)
    
    # Group by cluster
    clusters = {}
    for i, label in enumerate(labels):
        if label not in clusters:
            clusters[label] = []
        clusters[label].append(documents[i])
    
    return clusters

# Example
documents = [
    "Lu·∫≠t D√¢n s·ª± v·ªÅ quy·ªÅn s·ªü h·ªØu",
    "Lu·∫≠t H√¨nh s·ª± v·ªÅ t·ªôi gi·∫øt ng∆∞·ªùi",
    "Lu·∫≠t ƒê·∫•t ƒëai v·ªÅ quy·ªÅn s·ª≠ d·ª•ng ƒë·∫•t",
    "Lu·∫≠t H√¨nh s·ª± v·ªÅ t·ªôi tr·ªôm c·∫Øp",
    "Lu·∫≠t D√¢n s·ª± v·ªÅ h·ª£p ƒë·ªìng",
]

clusters = cluster_documents(documents, n_clusters=2)

for cluster_id, docs in clusters.items():
    print(f"Cluster {cluster_id}:")
    for doc in docs:
        print(f"  - {doc}")
```

### JavaScript/Node.js

```javascript
const axios = require('axios');

const API_URL = 'http://YOUR_DROPLET_IP:5000';

// Generate embeddings
async function getEmbeddings(texts) {
  try {
    const response = await axios.post(`${API_URL}/embed`, {
      texts: texts
    });
    return response.data.embeddings;
  } catch (error) {
    console.error('Error:', error.response?.data || error.message);
    throw error;
  }
}

// Calculate similarity
async function getSimilarity(texts1, texts2) {
  try {
    const response = await axios.post(`${API_URL}/similarity`, {
      texts1: texts1,
      texts2: texts2
    });
    return response.data.similarities;
  } catch (error) {
    console.error('Error:', error.response?.data || error.message);
    throw error;
  }
}

// Example usage
(async () => {
  // Test 1: Embeddings
  const embeddings = await getEmbeddings([
    'Lu·∫≠t D√¢n s·ª± nƒÉm 2015',
    'B·ªô lu·∫≠t H√¨nh s·ª±'
  ]);
  console.log(`Got ${embeddings.length} embeddings`);

  // Test 2: Similarity
  const similarities = await getSimilarity(
    ['Quy·ªÅn s·ªü h·ªØu t√†i s·∫£n'],
    ['T√†i s·∫£n chung', 'T√†i s·∫£n ri√™ng', 'Quy·ªÅn k·∫ø th·ª´a']
  );
  console.log('Similarities:', similarities);
})();
```

### cURL

```bash
# Health check
curl http://YOUR_DROPLET_IP:5000/health

# Embeddings
curl -X POST http://YOUR_DROPLET_IP:5000/embed \
  -H "Content-Type: application/json" \
  -d '{"texts": ["Lu·∫≠t D√¢n s·ª±", "Lu·∫≠t H√¨nh s·ª±"]}'

# Similarity
curl -X POST http://YOUR_DROPLET_IP:5000/similarity \
  -H "Content-Type: application/json" \
  -d '{
    "texts1": ["Quy·ªÅn s·ªü h·ªØu"],
    "texts2": ["T√†i s·∫£n", "Quy·ªÅn k·∫ø th·ª´a"]
  }'
```

---

## ‚ö° Performance Tips

1. **Batch Processing**: G·ª≠i nhi·ªÅu texts c√πng l√∫c thay v√¨ g·ªçi API nhi·ªÅu l·∫ßn
   ```python
   # ‚ùå Slow
   for text in texts:
       embedding = get_embedding([text])
   
   # ‚úÖ Fast
   embeddings = get_embeddings(texts)
   ```

2. **Caching**: Cache embeddings cho documents kh√¥ng thay ƒë·ªïi

3. **Use similarity endpoint**: Nhanh h∆°n l√† t√≠nh embeddings r·ªìi t√≠nh similarity

4. **Adjust batch_size**: TƒÉng n·∫øu server c√≥ nhi·ªÅu RAM

---

## üö® Error Handling

**Common Errors:**

1. **Connection Refused**
   - Ki·ªÉm tra firewall: `ufw status`
   - Ki·ªÉm tra API ƒëang ch·∫°y: `docker ps`

2. **Model Not Loaded**
   - Check logs: `docker logs legal-embedding-api`
   - Restart: `docker restart legal-embedding-api`

3. **Out of Memory**
   - Gi·∫£m batch_size trong request
   - Upgrade droplet RAM

---

## üìä Rate Limits

**Current Configuration:**
- No hard rate limits
- Recommended: < 100 requests/second
- Consider adding rate limiting n·∫øu public API

**To add rate limiting:**
```python
# In serve_model.py
from flask_limiter import Limiter

limiter = Limiter(
    app,
    key_func=lambda: request.remote_addr,
    default_limits=["100 per minute"]
)
```

---

## üîí Security

**Recommendations:**

1. **API Key Authentication:**
```python
# Add to serve_model.py
API_KEY = os.getenv('API_KEY')

@app.before_request
def verify_api_key():
    if request.path == '/health':
        return
    
    key = request.headers.get('X-API-Key')
    if key != API_KEY:
        return jsonify({"error": "Invalid API key"}), 401
```

2. **IP Whitelist:**
```bash
# On droplet
ufw delete allow 5000/tcp
ufw allow from YOUR_BACKEND_IP to any port 5000
```

3. **HTTPS/SSL:**
- Use reverse proxy (nginx) v·ªõi SSL certificate
- Consider using Cloudflare for DDoS protection

---

## üìû Support

Issues? Check:
1. [GPU_CPU_DEPLOYMENT_GUIDE.md](./GPU_CPU_DEPLOYMENT_GUIDE.md)
2. Docker logs: `docker logs legal-embedding-api`
3. API health: `curl http://YOUR_DROPLET_IP:5000/health`
