#!/usr/bin/env python3
"""
Flask API ƒë∆°n gi·∫£n ƒë·ªÉ serving embedding model tr√™n CPU
Endpoints: /health, /embed, /similarity
"""

import os
import logging
import time
from typing import List, Dict, Any
from flask import Flask, request, jsonify
from sentence_transformers import SentenceTransformer
import torch
import numpy as np

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# Initialize Flask app
app = Flask(__name__)

# Global model variable
model = None
model_loaded = False

def load_model():
    """Load embedding model"""
    global model, model_loaded
    
    model_path = os.getenv('MODEL_PATH', './models')
    
    logger.info(f"üì• Loading model from: {model_path}")
    
    try:
        # Force CPU usage
        device = 'cpu'
        logger.info(f"üíª Using device: {device}")
        
        # Load model
        model = SentenceTransformer(model_path, device=device)
        
        # Test model with dummy text
        test_embedding = model.encode(["test"], show_progress_bar=False)
        embedding_dim = test_embedding.shape[1]
        
        model_loaded = True
        logger.info(f"‚úÖ Model loaded successfully!")
        logger.info(f"üìä Embedding dimension: {embedding_dim}")
        
        return True
        
    except Exception as e:
        logger.error(f"‚ùå Failed to load model: {e}")
        model_loaded = False
        return False

@app.route('/health', methods=['GET'])
def health():
    """Health check endpoint"""
    status = {
        "status": "healthy" if model_loaded else "unhealthy",
        "model_loaded": model_loaded,
        "device": "cpu",
        "timestamp": time.time()
    }
    
    if model_loaded and model is not None:
        status["embedding_dim"] = model.get_sentence_embedding_dimension()
    
    return jsonify(status), 200 if model_loaded else 503

@app.route('/embed', methods=['POST'])
def embed():
    """
    Embedding endpoint
    Input: {"texts": ["text1", "text2", ...]}
    Output: {"embeddings": [[...], [...]], "embedding_dim": 1024}
    """
    if not model_loaded:
        return jsonify({"error": "Model not loaded"}), 503
    
    try:
        # Parse request
        data = request.get_json()
        
        if not data or 'texts' not in data:
            return jsonify({"error": "Missing 'texts' field"}), 400
        
        texts = data['texts']
        
        if not isinstance(texts, list) or len(texts) == 0:
            return jsonify({"error": "'texts' must be a non-empty list"}), 400
        
        # Limit batch size
        max_batch_size = int(os.getenv('MAX_BATCH_SIZE', '32'))
        if len(texts) > max_batch_size:
            return jsonify({
                "error": f"Batch size exceeds limit. Max: {max_batch_size}"
            }), 400
        
        # Generate embeddings
        start_time = time.time()
        embeddings = model.encode(
            texts,
            batch_size=16,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        inference_time = time.time() - start_time
        
        # Convert to list for JSON serialization
        embeddings_list = embeddings.tolist()
        
        response = {
            "embeddings": embeddings_list,
            "embedding_dim": embeddings.shape[1],
            "num_texts": len(texts),
            "inference_time": round(inference_time, 3)
        }
        
        logger.info(f"‚úÖ Embedded {len(texts)} texts in {inference_time:.3f}s")
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"‚ùå Embedding error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/similarity', methods=['POST'])
def similarity():
    """
    Similarity endpoint
    Input: {"texts1": ["text1", ...], "texts2": ["text2", ...]}
    Output: {"similarities": [[0.9, 0.8], ...], "shape": [2, 2]}
    """
    if not model_loaded:
        return jsonify({"error": "Model not loaded"}), 503
    
    try:
        # Parse request
        data = request.get_json()
        
        if not data or 'texts1' not in data or 'texts2' not in data:
            return jsonify({"error": "Missing 'texts1' or 'texts2' field"}), 400
        
        texts1 = data['texts1']
        texts2 = data['texts2']
        
        if not isinstance(texts1, list) or not isinstance(texts2, list):
            return jsonify({"error": "'texts1' and 'texts2' must be lists"}), 400
        
        if len(texts1) == 0 or len(texts2) == 0:
            return jsonify({"error": "Input lists cannot be empty"}), 400
        
        # Limit batch size
        max_batch_size = int(os.getenv('MAX_BATCH_SIZE', '32'))
        if len(texts1) > max_batch_size or len(texts2) > max_batch_size:
            return jsonify({
                "error": f"Batch size exceeds limit. Max: {max_batch_size}"
            }), 400
        
        # Generate embeddings
        start_time = time.time()
        
        embeddings1 = model.encode(
            texts1,
            batch_size=16,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        
        embeddings2 = model.encode(
            texts2,
            batch_size=16,
            show_progress_bar=False,
            convert_to_numpy=True
        )
        
        # Calculate cosine similarity
        # Normalize vectors
        embeddings1_norm = embeddings1 / np.linalg.norm(embeddings1, axis=1, keepdims=True)
        embeddings2_norm = embeddings2 / np.linalg.norm(embeddings2, axis=1, keepdims=True)
        
        # Compute similarity matrix
        similarities = np.matmul(embeddings1_norm, embeddings2_norm.T)
        
        inference_time = time.time() - start_time
        
        response = {
            "similarities": similarities.tolist(),
            "shape": list(similarities.shape),
            "inference_time": round(inference_time, 3)
        }
        
        logger.info(f"‚úÖ Computed similarity for {len(texts1)}x{len(texts2)} in {inference_time:.3f}s")
        
        return jsonify(response), 200
        
    except Exception as e:
        logger.error(f"‚ùå Similarity error: {e}")
        return jsonify({"error": str(e)}), 500

@app.route('/', methods=['GET'])
def index():
    """Root endpoint with API info"""
    return jsonify({
        "service": "Vietnamese Legal Embedding API",
        "version": "1.0.0",
        "endpoints": {
            "/health": "Health check",
            "/embed": "Generate embeddings (POST)",
            "/similarity": "Compute similarity (POST)"
        },
        "status": "ready" if model_loaded else "loading"
    }), 200

def main():
    """Main function to start the server"""
    logger.info("üöÄ Starting Vietnamese Legal Embedding Serving API")
    
    # Load model first
    if not load_model():
        logger.error("‚ùå Failed to load model. Exiting.")
        return
    
    # Get config from environment
    host = os.getenv('API_HOST', '0.0.0.0')
    port = int(os.getenv('API_PORT', '5000'))
    debug = os.getenv('DEBUG', 'false').lower() == 'true'
    
    logger.info(f"üåê Starting server on {host}:{port}")
    logger.info(f"üîß Debug mode: {debug}")
    
    # Start Flask server
    app.run(
        host=host,
        port=port,
        debug=debug,
        threaded=True  # Enable multi-threading for better performance
    )

if __name__ == "__main__":
    main()
